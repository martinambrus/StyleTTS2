{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b71b040",
   "metadata": {},
   "source": [
    "\n",
    "# Room and Microphone Stress Evaluation Notebook\n",
    "\n",
    "This notebook probes room acoustics and microphone coloration failure boundaries for a pitch extraction model trained with the JDC-PitchExtractor repository.\n",
    "It loads the latest checkpoint, prepares a clean evaluation cache, and then sweeps:\n",
    "\n",
    "* **Room impulse responses** for small-room, office, and hall environments, selecting examples across a configurable T60 decay-time grid.\n",
    "* **Microphone coloration curves** approximating smartphone, headset, and studio large-diaphragm condenser responses via cascaded biquad filters.\n",
    "\n",
    "For every condition the notebook reports standard melody metrics and visualises metric deltas relative to the clean baseline to highlight robustness boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6492b1",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "Uncomment and execute the next cell if any of the required dependencies are missing in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89262fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld torchcrepe praat-parselmouth pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fc1d2",
   "metadata": {},
   "source": [
    "\n",
    "## Imports and Global Configuration\n",
    "\n",
    "The repository root is added to `sys.path` so we can reuse the dataset and model helpers that ship with the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.functional import equalizer_biquad\n",
    "from Utils.f0_notebook_utils import (\n",
    "    build_notebook_f0_extractor,\n",
    "    compute_f0_for_notebook,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cace23",
   "metadata": {},
   "source": [
    "\n",
    "## User Configuration\n",
    "\n",
    "Update the configuration dictionary with paths that exist on your machine.\n",
    "The notebook will automatically resolve relative paths against the repository root and the training config directory,\n",
    "and will fall back to the most recent checkpoint in `checkpoint_dir` if `checkpoint_path` is left as `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fff0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, Any] = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"eval_list_path\": None,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"rir_library\": {\n",
    "        \"small_room\": [\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"small_room_t60_0.30.wav\", \"t60\": 0.30},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"small_room_t60_0.60.wav\", \"t60\": 0.60},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"small_room_t60_0.90.wav\", \"t60\": 0.90},\n",
    "        ],\n",
    "        \"office\": [\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"office_t60_0.45.wav\", \"t60\": 0.45},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"office_t60_0.80.wav\", \"t60\": 0.80},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"office_t60_1.20.wav\", \"t60\": 1.20},\n",
    "        ],\n",
    "        \"hall\": [\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"hall_t60_0.80.wav\", \"t60\": 0.80},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"hall_t60_1.10.wav\", \"t60\": 1.10},\n",
    "            {\"path\": REPO_ROOT / \"ImpulseResponses\" / \"hall_t60_1.50.wav\", \"t60\": 1.50},\n",
    "        ],\n",
    "    },\n",
    "    \"t60_sweep\": [round(x, 2) for x in np.linspace(0.2, 1.5, 14)],\n",
    "    \"microphone_eq\": {\n",
    "        \"smartphone\": [\n",
    "            {\"freq\": 180.0, \"gain_db\": -6.0, \"Q\": 0.8},\n",
    "            {\"freq\": 3500.0, \"gain_db\": 5.0, \"Q\": 1.2},\n",
    "            {\"freq\": 9000.0, \"gain_db\": 3.0, \"Q\": 1.0},\n",
    "        ],\n",
    "        \"headset\": [\n",
    "            {\"freq\": 120.0, \"gain_db\": -2.0, \"Q\": 0.7},\n",
    "            {\"freq\": 2400.0, \"gain_db\": 3.0, \"Q\": 1.4},\n",
    "            {\"freq\": 6000.0, \"gain_db\": 2.5, \"Q\": 1.1},\n",
    "        ],\n",
    "        \"studio_ldc\": [\n",
    "            {\"freq\": 80.0, \"gain_db\": 2.0, \"Q\": 0.9},\n",
    "            {\"freq\": 4500.0, \"gain_db\": -1.5, \"Q\": 1.3},\n",
    "            {\"freq\": 12000.0, \"gain_db\": 1.5, \"Q\": 0.9},\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75024aa",
   "metadata": {},
   "source": [
    "\n",
    "## Helper Utilities\n",
    "\n",
    "The next cell defines shared helpers for path resolution, checkpoint discovery, audio preprocessing, model inference,\n",
    "and evaluation. These utilities closely mirror the training-time preprocessing performed in `meldataset.MelDataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "def _normalize_mel_params(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"win_len\" in params:\n",
    "        win_len = params.pop(\"win_len\")\n",
    "        params.setdefault(\"win_length\", win_len)\n",
    "    return params\n",
    "\n",
    "\n",
    "def _deep_merge_dict(base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    merged = base.copy()\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, dict):\n",
    "            existing = merged.get(key)\n",
    "            if isinstance(existing, dict):\n",
    "                merged[key] = _deep_merge_dict(existing, value)\n",
    "            else:\n",
    "                merged[key] = value.copy()\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "\n",
    "MEL_PARAMS = _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "FALLBACK_SAMPLE_RATE = int(DEFAULT_MEL_PARAMS.get(\"sample_rate\", 24000))\n",
    "FALLBACK_HOP_LENGTH = int(DEFAULT_MEL_PARAMS.get(\"hop_length\", DEFAULT_MEL_PARAMS.get(\"win_len\", 300)))\n",
    "F0_PARAMS: Dict[str, Any] = {}\n",
    "F0_ZERO_FILL: float = 0.0\n",
    "REFERENCE_F0_BACKENDS_USED: set[str] = set()\n",
    "NOTEBOOK_F0_EXTRACTOR = build_notebook_f0_extractor(\n",
    "    MEL_PARAMS,\n",
    "    F0_PARAMS,\n",
    "    fallback_sr=FALLBACK_SAMPLE_RATE,\n",
    "    fallback_hop=FALLBACK_HOP_LENGTH,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "def _load_training_config():\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "    import yaml\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    global NOTEBOOK_F0_EXTRACTOR, F0_ZERO_FILL\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "    F0_PARAMS.clear()\n",
    "    F0_PARAMS.update(dataset_params.get(\"f0_params\", {}))\n",
    "    NOTEBOOK_F0_EXTRACTOR = build_notebook_f0_extractor(\n",
    "    MEL_PARAMS,\n",
    "    F0_PARAMS,\n",
    "    fallback_sr=FALLBACK_SAMPLE_RATE,\n",
    "    fallback_hop=FALLBACK_HOP_LENGTH,\n",
    "    verbose=True,\n",
    ")\n",
    "    F0_ZERO_FILL = float(F0_PARAMS.get(\"zero_fill_value\", 0.0))\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "    return data\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "def _collect_model_configuration(checkpoint_state: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    model_params: Dict[str, Any] = {}\n",
    "    training_params = TRAINING_CONFIG.get(\"model_params\")\n",
    "    if isinstance(training_params, dict):\n",
    "        model_params = _deep_merge_dict(model_params, training_params)\n",
    "    if isinstance(checkpoint_state, dict):\n",
    "        candidate = checkpoint_state.get(\"model_params\")\n",
    "        if isinstance(candidate, dict):\n",
    "            model_params = _deep_merge_dict(model_params, candidate)\n",
    "        config_section = checkpoint_state.get(\"config\")\n",
    "        if isinstance(config_section, dict):\n",
    "            candidate = config_section.get(\"model_params\")\n",
    "            if isinstance(candidate, dict):\n",
    "                model_params = _deep_merge_dict(model_params, candidate)\n",
    "    sequence_config = model_params.get(\"sequence_model\")\n",
    "    if isinstance(sequence_config, dict):\n",
    "        sequence_config = sequence_config.copy()\n",
    "    else:\n",
    "        sequence_config = {}\n",
    "    top_level = {k: v for k, v in model_params.items() if k != \"sequence_model\"}\n",
    "    top_level.pop(\"num_class\", None)\n",
    "    return top_level, sequence_config\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int] | None:\n",
    "    try:\n",
    "        audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: skipping unreadable file '{path}': {exc}\")\n",
    "        return None\n",
    "    audio = _ensure_mono(audio)\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: skipping empty file '{path}'.\")\n",
    "        return None\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "def compute_reference_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    result = compute_f0_for_notebook(\n",
    "        audio,\n",
    "        sr,\n",
    "        NOTEBOOK_F0_EXTRACTOR,\n",
    "        zero_fill_value=F0_ZERO_FILL,\n",
    "    )\n",
    "    if result.backend_name and result.backend_name not in REFERENCE_F0_BACKENDS_USED:\n",
    "        REFERENCE_F0_BACKENDS_USED.add(result.backend_name)\n",
    "        print(f\"Using F0 backend '{result.backend_name}' for reference computation.\")\n",
    "    return result.f0\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "def hz_to_cents(f0: np.ndarray) -> np.ndarray:\n",
    "    cents = np.zeros_like(f0)\n",
    "    positive = f0 > 0\n",
    "    cents[positive] = 1200.0 * np.log2(f0[positive] / 55.0)\n",
    "    return cents\n",
    "\n",
    "def circular_cents_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    diff = a - b\n",
    "    diff = np.mod(diff + 600.0, 1200.0) - 600.0\n",
    "    return diff\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }\n",
    "\n",
    "DATASET_CACHE: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "def prepare_dataset_cache(force: bool = False) -> List[Dict[str, Any]]:\n",
    "    global DATASET_CACHE\n",
    "    if DATASET_CACHE is not None and not force:\n",
    "        return DATASET_CACHE\n",
    "    eval_list = CONFIG.get(\"eval_list_path\")\n",
    "    if eval_list is None or not Path(eval_list).is_file():\n",
    "        raise FileNotFoundError(f\"Evaluation list not found: {eval_list}\")\n",
    "    entries: List[Path] = []\n",
    "    with open(eval_list, \"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            candidate = Path(line.split(\"|\")[0])\n",
    "            if not candidate.is_absolute():\n",
    "                candidate = (Path(eval_list).parent / candidate).resolve()\n",
    "            entries.append(candidate)\n",
    "    if not entries:\n",
    "        raise RuntimeError(f\"No evaluation files located in {eval_list}\")\n",
    "    cache: List[Dict[str, Any]] = []\n",
    "    for path in tqdm(entries, desc=\"Preparing evaluation cache\"):\n",
    "        result = load_waveform(path)\n",
    "        if result is None:\n",
    "            continue\n",
    "        audio, sr = result\n",
    "        reference_f0 = compute_reference_f0(audio, sr)\n",
    "        cache.append({\n",
    "            \"path\": path,\n",
    "            \"audio\": audio,\n",
    "            \"sample_rate\": sr,\n",
    "            \"reference_f0\": reference_f0,\n",
    "        })\n",
    "    DATASET_CACHE = cache\n",
    "    print(f\"Cached {len(DATASET_CACHE)} evaluation utterances.\")\n",
    "    return DATASET_CACHE\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\"Unexpected checkpoint format\")\n",
    "    model_state = checkpoint.get(\"model\")\n",
    "    if model_state is None:\n",
    "        model_state = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    if not isinstance(model_state, dict):\n",
    "        raise RuntimeError(\"Checkpoint is missing a valid model state\")\n",
    "    model_state = dict(model_state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\")\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(checkpoint.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    model_params, sequence_model_config = _collect_model_configuration(checkpoint)\n",
    "    model_kwargs: Dict[str, Any] = {}\n",
    "    if sequence_model_config:\n",
    "        model_kwargs[\"sequence_model_config\"] = sequence_model_config\n",
    "    leaky_relu_slope = model_params.get(\"leaky_relu_slope\")\n",
    "    if isinstance(leaky_relu_slope, (int, float)):\n",
    "        model_kwargs[\"leaky_relu_slope\"] = float(leaky_relu_slope)\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes, **model_kwargs)\n",
    "    incompatible = model.load_state_dict(model_state, strict=False)\n",
    "    if incompatible.unexpected_keys:\n",
    "        skipped = \", \".join(sorted(incompatible.unexpected_keys))\n",
    "        print(f\"Warning: skipped unexpected parameters: {skipped}\")\n",
    "    if incompatible.missing_keys:\n",
    "        missing = \", \".join(sorted(incompatible.missing_keys))\n",
    "        print(f\"Warning: missing parameters not found in checkpoint: {missing}\")\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n",
    "def _load_rir_waveform(path: Path) -> np.ndarray | None:\n",
    "    result = load_waveform(path, TARGET_SAMPLE_RATE)\n",
    "    if result is None:\n",
    "        return None\n",
    "    audio, _ = result\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: impulse response at '{path}' is empty; skipping.\")\n",
    "        return None\n",
    "    audio = audio / (np.max(np.abs(audio)) + 1e-6)\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "def _infer_t60_from_name(path: Path) -> Optional[float]:\n",
    "    match = re.search(r\"t60[_=]?([0-9]+(?:\\.[0-9]+)?)\", path.stem, re.IGNORECASE)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    match = re.search(r\"([0-9]+(?:\\.[0-9]+)?)s\", path.stem, re.IGNORECASE)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "RIR_CACHE: Dict[str, List[Dict[str, Any]]] = {}\n",
    "\n",
    "def resolve_rir_library(force: bool = False) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    global RIR_CACHE\n",
    "    if RIR_CACHE and not force:\n",
    "        return RIR_CACHE\n",
    "    resolved: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    base_config = Path(CONFIG.get(\"config_path\", REPO_ROOT))\n",
    "    for room, entries in CONFIG.get(\"rir_library\", {}).items():\n",
    "        room_entries: List[Dict[str, Any]] = []\n",
    "        for entry in entries:\n",
    "            if isinstance(entry, dict):\n",
    "                candidate = entry.get(\"path\")\n",
    "                t60 = entry.get(\"t60\")\n",
    "            else:\n",
    "                candidate = entry\n",
    "                t60 = None\n",
    "            if candidate is None:\n",
    "                continue\n",
    "            path = _resolve_relative_path(base_config, candidate)\n",
    "            if not path.is_file():\n",
    "                print(f\"Warning: RIR file not found ({path})\")\n",
    "                continue\n",
    "            rir_audio = _load_rir_waveform(path)\n",
    "            if rir_audio is None:\n",
    "                continue\n",
    "            if t60 is None:\n",
    "                t60 = _infer_t60_from_name(path)\n",
    "            room_entries.append({\n",
    "                \"path\": path,\n",
    "                \"t60\": t60,\n",
    "                \"audio\": rir_audio,\n",
    "            })\n",
    "        if room_entries:\n",
    "            resolved[room] = room_entries\n",
    "        else:\n",
    "            print(f\"Warning: no valid RIRs configured for room '{room}'.\")\n",
    "    RIR_CACHE = resolved\n",
    "    return RIR_CACHE\n",
    "\n",
    "def _select_rir(room: str, target_t60: float) -> Optional[Dict[str, Any]]:\n",
    "    candidates = resolve_rir_library().get(room, [])\n",
    "    valid = [c for c in candidates if c.get(\"t60\") is not None]\n",
    "    if not valid:\n",
    "        return None\n",
    "    return min(valid, key=lambda item: abs(float(item[\"t60\"]) - target_t60))\n",
    "\n",
    "def apply_rir(audio: np.ndarray, rir: np.ndarray) -> np.ndarray:\n",
    "    audio_tensor = torch.from_numpy(audio).view(1, 1, -1)\n",
    "    rir_tensor = torch.from_numpy(rir).view(1, 1, -1)\n",
    "    padding = rir_tensor.shape[-1] - 1\n",
    "    with torch.no_grad():\n",
    "        convolved = torch.nn.functional.conv1d(\n",
    "            torch.nn.functional.pad(audio_tensor, (padding, 0)),\n",
    "            rir_tensor.flip(-1),\n",
    "        )\n",
    "    result = convolved.squeeze().cpu().numpy()\n",
    "    if result.shape[0] >= audio.shape[0]:\n",
    "        result = result[: audio.shape[0]]\n",
    "    else:\n",
    "        result = np.pad(result, (0, audio.shape[0] - result.shape[0]))\n",
    "    max_val = np.max(np.abs(result))\n",
    "    if max_val > 0.99:\n",
    "        result = result / (max_val + 1e-6)\n",
    "    return result.astype(np.float32)\n",
    "\n",
    "def apply_microphone_eq(audio: np.ndarray, sample_rate: int, curve: Iterable[Dict[str, float]]) -> np.ndarray:\n",
    "    tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "    result = tensor\n",
    "    for stage in curve:\n",
    "        freq = float(stage.get(\"freq\", 1000.0))\n",
    "        gain = float(stage.get(\"gain_db\", 0.0))\n",
    "        q = float(stage.get(\"Q\", 0.707))\n",
    "        result = equalizer_biquad(result, sample_rate, freq, gain, q)\n",
    "    return result.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "def evaluate_condition(model: JDCNet, transform_fn, label: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for entry in DATASET_CACHE or []:\n",
    "        processed = transform_fn(entry)\n",
    "        prediction = predict_f0(model, processed)\n",
    "        metrics = compute_metrics(entry[\"reference_f0\"], prediction)\n",
    "        record = {\n",
    "            **label,\n",
    "            \"path\": str(entry[\"path\"]),\n",
    "            **metrics,\n",
    "        }\n",
    "        results.append(record)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee65870",
   "metadata": {},
   "source": [
    "\n",
    "## Load Model and Evaluation Set\n",
    "\n",
    "Run the next cell after updating the configuration to load the trained checkpoint and prepare the clean evaluation cache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "DATASET_CACHE = prepare_dataset_cache(force=True)\n",
    "len(DATASET_CACHE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde3a91",
   "metadata": {},
   "source": [
    "\n",
    "## Baseline (Clean) Evaluation\n",
    "\n",
    "Compute clean-mix metrics to use as the reference point for subsequent delta calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d184c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_records: List[Dict[str, Any]] = []\n",
    "for entry in tqdm(DATASET_CACHE, desc=\"Evaluating clean baseline\"):\n",
    "    prediction = predict_f0(model, entry[\"audio\"])\n",
    "    metrics = compute_metrics(entry[\"reference_f0\"], prediction)\n",
    "    baseline_records.append({\"path\": str(entry[\"path\"]), **metrics})\n",
    "baseline_df = pd.DataFrame(baseline_records)\n",
    "baseline_summary = baseline_df[[\"RPA\", \"RCA\", \"VUV\", \"OctaveError\"]].mean()\n",
    "display(baseline_summary.to_frame(name=\"Baseline\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a1b62",
   "metadata": {},
   "source": [
    "\n",
    "## Room Impulse Response Sweep\n",
    "\n",
    "For each room category the notebook selects the configured impulse response whose measured T60 is closest to each target in `CONFIG['t60_sweep']`.\n",
    "Metrics are averaged per condition and plotted against the T60 grid to expose decay-time failure thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbec0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rir_results: List[Dict[str, Any]] = []\n",
    "t60_targets = CONFIG.get(\"t60_sweep\", [])\n",
    "for room in CONFIG.get(\"rir_library\", {}):\n",
    "    for target_t60 in t60_targets:\n",
    "        rir_entry = _select_rir(room, float(target_t60))\n",
    "        if rir_entry is None:\n",
    "            continue\n",
    "        rir_audio = rir_entry[\"audio\"]\n",
    "        actual_t60 = rir_entry.get(\"t60\")\n",
    "        def _transform(sample, rir_waveform=rir_audio):\n",
    "            return apply_rir(sample[\"audio\"], rir_waveform)\n",
    "        records = evaluate_condition(model, _transform, {\n",
    "            \"room\": room,\n",
    "            \"target_t60\": float(target_t60),\n",
    "            \"rir_t60\": float(actual_t60) if actual_t60 is not None else np.nan,\n",
    "            \"rir_path\": str(rir_entry.get(\"path\")),\n",
    "        })\n",
    "        rir_results.extend(records)\n",
    "rir_df = pd.DataFrame(rir_results)\n",
    "if not rir_df.empty:\n",
    "    rir_summary = (\n",
    "        rir_df\n",
    "        .groupby([\"room\", \"target_t60\", \"rir_t60\"])\n",
    "        [[\"RPA\", \"RCA\", \"VUV\", \"OctaveError\"]]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    display(rir_summary.head())\n",
    "else:\n",
    "    print(\"No RIR results computed. Check CONFIG['rir_library'].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f4945",
   "metadata": {},
   "source": [
    "\n",
    "### Plot: Metric vs T60\n",
    "\n",
    "The plot below shows averaged RPA and RCA across the T60 sweep for each room category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rir_summary' in globals() and not rir_summary.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharey=True)\n",
    "    metrics_to_plot = ['RPA', 'RCA']\n",
    "    for ax, metric in zip(axes, metrics_to_plot):\n",
    "        for room, group in rir_summary.groupby('room'):\n",
    "            ax.plot(group['target_t60'], group[metric], marker='o', label=room)\n",
    "        ax.set_xlabel('Target T60 (s)')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} vs T60')\n",
    "        ax.set_xlim(min(CONFIG['t60_sweep']), max(CONFIG['t60_sweep']))\n",
    "        ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No RIR summary available to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48c999",
   "metadata": {},
   "source": [
    "\n",
    "## Microphone Coloration Sweep\n",
    "\n",
    "Each microphone profile is implemented as a sequence of biquad peaking/shelving filters.\n",
    "We report deltas relative to the clean baseline for RPA and RCA to emphasise degradation due to coloration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_results: List[Dict[str, Any]] = []\n",
    "for mic_name, curve in CONFIG.get(\"microphone_eq\", {}).items():\n",
    "    def _transform(sample, eq_curve=curve):\n",
    "        return apply_microphone_eq(sample[\"audio\"], sample[\"sample_rate\"], eq_curve)\n",
    "    records = evaluate_condition(model, _transform, {\n",
    "        \"microphone\": mic_name,\n",
    "    })\n",
    "    mic_results.extend(records)\n",
    "mic_df = pd.DataFrame(mic_results)\n",
    "if not mic_df.empty:\n",
    "    mic_summary = (\n",
    "        mic_df\n",
    "        .groupby('microphone')\n",
    "        [[\"RPA\", \"RCA\", \"VUV\", \"OctaveError\"]]\n",
    "        .mean()\n",
    "    )\n",
    "    rpa_delta = mic_summary['RPA'] - baseline_summary['RPA']\n",
    "    rca_delta = mic_summary['RCA'] - baseline_summary['RCA']\n",
    "    delta_df = pd.DataFrame({\n",
    "        \"RPA_delta\": rpa_delta,\n",
    "        \"RCA_delta\": rca_delta,\n",
    "    })\n",
    "    display(delta_df)\n",
    "else:\n",
    "    print(\"No microphone results computed. Check CONFIG['microphone_eq'].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056170d",
   "metadata": {},
   "source": [
    "\n",
    "### Plot: Microphone-Induced Metric Delta\n",
    "\n",
    "Bar plots showing deviation from the clean baseline for RPA and RCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'delta_df' in globals() and not delta_df.empty:\n",
    "    ax = delta_df[['RPA_delta', 'RCA_delta']].plot(kind='bar', figsize=(10, 5))\n",
    "    ax.set_ylabel('Delta relative to clean')\n",
    "    ax.set_title('Microphone coloration impact on RPA / RCA')\n",
    "    plt.axhline(0.0, color='black', linewidth=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No microphone delta data available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73441cbc",
   "metadata": {},
   "source": [
    "\n",
    "## Saving Results\n",
    "\n",
    "Optional cell to persist the aggregated DataFrames for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = CONFIG[\"output_dir\"]\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if 'rir_summary' in globals() and not rir_summary.empty:\n",
    "    rir_summary.to_csv(OUTPUT_DIR / \"rir_sweep_metrics.csv\", index=False)\n",
    "if 'delta_df' in globals() and not delta_df.empty:\n",
    "    delta_df.to_csv(OUTPUT_DIR / \"microphone_delta_metrics.csv\")\n",
    "print(f\"Artifacts written to {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
