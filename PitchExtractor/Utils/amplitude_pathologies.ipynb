{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd730c",
   "metadata": {},
   "source": [
    "# Amplitude Pathology Sweep\n",
    "\n",
    "This notebook evaluates how a trained pitch extraction model handles amplitude-related pathologies. We synthesise clean reference tones, introduce clipping and AGC pumping across severity levels, and measure performance degradation via voiced/unvoiced (V/UV) flips and raw chroma accuracy (RCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c280d0",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Uncomment and run the cell below if you need to install the required dependencies inside your runtime environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld torchcrepe praat-parselmouth pandas tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9e924",
   "metadata": {},
   "source": [
    "## Imports and Global Configuration\n",
    "\n",
    "The repository root is added to `sys.path` so that we can reuse the training utilities when running the evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "from Utils.dynamic_pitch_tools import (\n",
    "    synthesize_from_f0_curve,\n",
    "    sample_reference_f0,\n",
    "    hz_to_cents,\n",
    "    circular_cents_distance,\n",
    ")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9dbb34",
   "metadata": {},
   "source": [
    "## User Configuration\n",
    "\n",
    "Update the configuration below to point at your training config, checkpoint directory, and evaluation preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\" / \"amplitude_pathologies\",\n",
    "    \"stimulus\": {\n",
    "        \"frequencies_hz\": [110.0, 220.0, 440.0],\n",
    "        \"duration_seconds\": 3.0,\n",
    "        \"amplitude\": 0.85,\n",
    "    },\n",
    "    \"clipping\": {\n",
    "        \"levels_percent\": [0, 2, 4, 6, 8, 10],\n",
    "    },\n",
    "    \"agc\": {\n",
    "        \"levels_db\": [0, 2, 4, 6, 8, 10],\n",
    "        \"target_rms\": 0.15,\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a253777",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions for loading checkpoints, running inference, and computing melody extraction metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3afcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "\n",
    "def _normalize_mel_params(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"win_len\" in params:\n",
    "        win_len = params.pop(\"win_len\")\n",
    "        params.setdefault(\"win_length\", win_len)\n",
    "    return params\n",
    "\n",
    "def _deep_merge_dict(base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    merged = base.copy()\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, dict):\n",
    "            existing = merged.get(key)\n",
    "            if isinstance(existing, dict):\n",
    "                merged[key] = _deep_merge_dict(existing, value)\n",
    "            else:\n",
    "                merged[key] = value.copy()\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "\n",
    "MEL_PARAMS = _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "def _collect_model_configuration(checkpoint_state: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    model_params: Dict[str, Any] = {}\n",
    "    training_params = TRAINING_CONFIG.get(\"model_params\")\n",
    "    if isinstance(training_params, dict):\n",
    "        model_params = _deep_merge_dict(model_params, training_params)\n",
    "    if isinstance(checkpoint_state, dict):\n",
    "        candidate = checkpoint_state.get(\"model_params\")\n",
    "        if isinstance(candidate, dict):\n",
    "            model_params = _deep_merge_dict(model_params, candidate)\n",
    "        config_section = checkpoint_state.get(\"config\")\n",
    "        if isinstance(config_section, dict):\n",
    "            candidate = config_section.get(\"model_params\")\n",
    "            if isinstance(candidate, dict):\n",
    "                model_params = _deep_merge_dict(model_params, candidate)\n",
    "    sequence_config = model_params.get(\"sequence_model\")\n",
    "    if isinstance(sequence_config, dict):\n",
    "        sequence_config = sequence_config.copy()\n",
    "    else:\n",
    "        sequence_config = {}\n",
    "    top_level = {k: v for k, v in model_params.items() if k != \"sequence_model\"}\n",
    "    top_level.pop(\"num_class\", None)\n",
    "    return top_level, sequence_config\n",
    "\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int] | None:\n",
    "    try:\n",
    "        audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: skipping unreadable file '{path}': {exc}\")\n",
    "        return None\n",
    "    audio = _ensure_mono(audio)\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: skipping empty file '{path}'.\")\n",
    "        return None\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\"Unexpected checkpoint format\")\n",
    "    model_state = checkpoint.get(\"model\")\n",
    "    if model_state is None:\n",
    "        model_state = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    if not isinstance(model_state, dict):\n",
    "        raise RuntimeError(\"Checkpoint is missing a valid model state\")\n",
    "    model_state = dict(model_state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\")\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(checkpoint.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    model_params, sequence_model_config = _collect_model_configuration(checkpoint)\n",
    "    model_kwargs: Dict[str, Any] = {}\n",
    "    if sequence_model_config:\n",
    "        model_kwargs[\"sequence_model_config\"] = sequence_model_config\n",
    "    leaky_relu_slope = model_params.get(\"leaky_relu_slope\")\n",
    "    if isinstance(leaky_relu_slope, (int, float)):\n",
    "        model_kwargs[\"leaky_relu_slope\"] = float(leaky_relu_slope)\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes, **model_kwargs)\n",
    "    incompatible = model.load_state_dict(model_state, strict=False)\n",
    "    if incompatible.unexpected_keys:\n",
    "        skipped = \", \".join(sorted(incompatible.unexpected_keys))\n",
    "        print(f\"Warning: skipped unexpected parameters: {skipped}\")\n",
    "    if incompatible.missing_keys:\n",
    "        missing = \", \".join(sorted(incompatible.missing_keys))\n",
    "        print(f\"Warning: missing parameters not found in checkpoint: {missing}\")\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850abe3",
   "metadata": {},
   "source": [
    "## Stimulus Generation and Perturbations\n",
    "\n",
    "Functions for synthesising reference tones and applying amplitude pathologies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98304d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reference_stimuli() -> List[Dict[str, Any]]:\n",
    "    sr = TARGET_SAMPLE_RATE\n",
    "    duration = float(CONFIG[\"stimulus\"][\"duration_seconds\"])\n",
    "    amplitude = float(CONFIG[\"stimulus\"][\"amplitude\"])\n",
    "    num_samples = int(duration * sr)\n",
    "    time_axis = np.arange(num_samples, dtype=np.float32) / float(sr)\n",
    "    stimuli: List[Dict[str, Any]] = []\n",
    "    for freq in CONFIG[\"stimulus\"][\"frequencies_hz\"]:\n",
    "        f0_curve = np.full((num_samples,), float(freq), dtype=np.float32)\n",
    "        audio = synthesize_from_f0_curve(f0_curve, sr, amplitude=amplitude)\n",
    "        stimuli.append(\n",
    "            {\n",
    "                \"id\": f\"tone_{int(freq)}Hz\",\n",
    "                \"audio\": audio.astype(np.float32, copy=False),\n",
    "                \"sr\": sr,\n",
    "                \"time_axis\": time_axis.copy(),\n",
    "                \"f0_curve\": f0_curve,\n",
    "            }\n",
    "        )\n",
    "    return stimuli\n",
    "\n",
    "\n",
    "def apply_sample_clipping(audio: np.ndarray, percent: float, sr: int, **_: Any) -> np.ndarray:\n",
    "    percent = float(percent)\n",
    "    if percent <= 0:\n",
    "        return audio.astype(np.float32, copy=True)\n",
    "    threshold = np.quantile(np.abs(audio), max(0.0, 1.0 - percent / 100.0))\n",
    "    if threshold <= 0:\n",
    "        return audio.astype(np.float32, copy=True)\n",
    "    clipped = np.clip(audio, -threshold, threshold)\n",
    "    return clipped.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def apply_agc_pumping(audio: np.ndarray, level_db: float, sr: int, target_rms: float, **_: Any) -> np.ndarray:\n",
    "    level_db = float(level_db)\n",
    "    if level_db <= 0:\n",
    "        return audio.astype(np.float32, copy=True)\n",
    "    attack = 0.01\n",
    "    release = np.interp(level_db, [0.0, 10.0], [0.05, 0.4])\n",
    "    depth_db = np.interp(level_db, [0.0, 10.0], [3.0, 18.0])\n",
    "    attack_coeff = np.exp(-1.0 / (attack * sr))\n",
    "    release_coeff = np.exp(-1.0 / (release * sr))\n",
    "    env = 0.0\n",
    "    gains = np.zeros_like(audio, dtype=np.float32)\n",
    "    for i, sample in enumerate(audio):\n",
    "        rectified = abs(float(sample))\n",
    "        if rectified > env:\n",
    "            env = attack_coeff * env + (1.0 - attack_coeff) * rectified\n",
    "        else:\n",
    "            env = release_coeff * env + (1.0 - release_coeff) * rectified\n",
    "        desired = target_rms / (env + 1e-6)\n",
    "        max_gain = 10 ** (depth_db / 20.0)\n",
    "        gains[i] = np.clip(desired, 1.0 / max_gain, max_gain)\n",
    "    smoothing = int(sr * np.interp(level_db, [0.0, 10.0], [0.01, 0.12]))\n",
    "    if smoothing > 1:\n",
    "        kernel = np.ones(smoothing, dtype=np.float32) / smoothing\n",
    "        gains = np.convolve(gains, kernel, mode=\"same\")\n",
    "    pumped = audio * gains\n",
    "    pumped = np.clip(pumped, -1.0, 1.0)\n",
    "    return pumped.astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def evaluate_pathology(\n",
    "    model: JDCNet,\n",
    "    stimuli: List[Dict[str, Any]],\n",
    "    levels: List[float],\n",
    "    transform_fn,\n",
    "    pathology_name: str,\n",
    "    transform_kwargs: Optional[Dict[str, Any]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    transform_kwargs = transform_kwargs or {}\n",
    "    levels = list(levels)\n",
    "    if 0 not in levels:\n",
    "        levels = [0] + levels\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    baseline_cache: Dict[str, Dict[str, Any]] = {}\n",
    "    for stimulus in stimuli:\n",
    "        prediction = predict_f0(model, stimulus[\"audio\"])\n",
    "        reference = sample_reference_f0(stimulus[\"time_axis\"], stimulus[\"f0_curve\"], prediction.shape[0])\n",
    "        metrics = compute_metrics(reference, prediction)\n",
    "        voicing = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "        baseline_cache[stimulus[\"id\"]] = {\n",
    "            \"prediction\": prediction,\n",
    "            \"voicing\": voicing,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "        results.append(\n",
    "            {\n",
    "                \"pathology\": pathology_name,\n",
    "                \"level\": 0.0,\n",
    "                \"stimulus\": stimulus[\"id\"],\n",
    "                \"RCA\": metrics[\"RCA\"],\n",
    "                \"VUV_flips\": 0.0,\n",
    "                \"VUV_accuracy\": metrics[\"VUV\"],\n",
    "            }\n",
    "        )\n",
    "    for level in levels:\n",
    "        if level == 0:\n",
    "            continue\n",
    "        for stimulus in stimuli:\n",
    "            kwargs = dict(transform_kwargs)\n",
    "            perturbed = transform_fn(stimulus[\"audio\"], level, stimulus[\"sr\"], **kwargs)\n",
    "            prediction = predict_f0(model, perturbed)\n",
    "            reference = sample_reference_f0(stimulus[\"time_axis\"], stimulus[\"f0_curve\"], prediction.shape[0])\n",
    "            metrics = compute_metrics(reference, prediction)\n",
    "            baseline = baseline_cache[stimulus[\"id\"]]\n",
    "            baseline_voicing = baseline[\"voicing\"]\n",
    "            candidate_voicing = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "            length = min(baseline_voicing.shape[0], candidate_voicing.shape[0])\n",
    "            if length == 0:\n",
    "                flip_rate = float(\"nan\")\n",
    "            else:\n",
    "                flip_rate = float(np.count_nonzero(baseline_voicing[:length] != candidate_voicing[:length]) / length)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"pathology\": pathology_name,\n",
    "                    \"level\": float(level),\n",
    "                    \"stimulus\": stimulus[\"id\"],\n",
    "                    \"RCA\": metrics[\"RCA\"],\n",
    "                    \"VUV_flips\": flip_rate,\n",
    "                    \"VUV_accuracy\": metrics[\"VUV\"],\n",
    "                }\n",
    "            )\n",
    "    df = pd.DataFrame(results)\n",
    "    return df.sort_values([\"pathology\", \"level\", \"stimulus\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000accc0",
   "metadata": {},
   "source": [
    "## Load Model and Generate Stimuli\n",
    "\n",
    "Instantiate the trained model and synthesise the clean reference tones used throughout the sweeps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "stimuli = generate_reference_stimuli()\n",
    "print(f\"Prepared {len(stimuli)} stimuli at {TARGET_SAMPLE_RATE} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1712a7",
   "metadata": {},
   "source": [
    "## Run Amplitude Pathology Sweeps\n",
    "\n",
    "Evaluate clipping and AGC pumping severity sweeps, collecting RCA and V/UV flip statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ef7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping_levels = CONFIG[\"clipping\"][\"levels_percent\"]\n",
    "agc_levels = CONFIG[\"agc\"][\"levels_db\"]\n",
    "\n",
    "df_clipping = evaluate_pathology(\n",
    "    model,\n",
    "    stimuli,\n",
    "    clipping_levels,\n",
    "    apply_sample_clipping,\n",
    "    pathology_name=\"Clipping\",\n",
    ")\n",
    "\n",
    "df_agc = evaluate_pathology(\n",
    "    model,\n",
    "    stimuli,\n",
    "    agc_levels,\n",
    "    apply_agc_pumping,\n",
    "    pathology_name=\"AGC pumping\",\n",
    "    transform_kwargs={\"target_rms\": CONFIG[\"agc\"][\"target_rms\"]},\n",
    ")\n",
    "\n",
    "results_df = pd.concat([df_clipping, df_agc], ignore_index=True)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074fbef",
   "metadata": {},
   "source": [
    "## Aggregate Metrics\n",
    "\n",
    "Summarise average RCA and V/UV flip rates across stimuli for each severity level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    results_df\n",
    "    .groupby([\"pathology\", \"level\"], as_index=False)\n",
    "    .agg(\n",
    "        RCA_mean=(\"RCA\", \"mean\"),\n",
    "        RCA_std=(\"RCA\", \"std\"),\n",
    "        VUV_flip_mean=(\"VUV_flips\", \"mean\"),\n",
    "        VUV_flip_std=(\"VUV_flips\", \"std\"),\n",
    "        VUV_accuracy_mean=(\"VUV_accuracy\", \"mean\"),\n",
    "        VUV_accuracy_std=(\"VUV_accuracy\", \"std\"),\n",
    "    )\n",
    "    .sort_values([\"pathology\", \"level\"])\n",
    ")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12a455",
   "metadata": {},
   "source": [
    "## Plot Metric vs Condition Level\n",
    "\n",
    "Visualise how RCA and V/UV flip rates evolve with increasing amplitude-pathology severity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ae528",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pathology, group in summary.groupby(\"pathology\"):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharex=True)\n",
    "    levels = group[\"level\"].to_numpy()\n",
    "    rca_mean = group[\"RCA_mean\"].to_numpy()\n",
    "    rca_std = group[\"RCA_std\"].fillna(0.0).to_numpy()\n",
    "    vuv_mean = group[\"VUV_flip_mean\"].to_numpy()\n",
    "    vuv_std = group[\"VUV_flip_std\"].fillna(0.0).to_numpy()\n",
    "\n",
    "    axes[0].errorbar(levels, rca_mean, yerr=rca_std, fmt=\"-o\", capsize=4)\n",
    "    axes[0].set_title(f\"{pathology} — RCA\")\n",
    "    axes[0].set_xlabel(\"Condition level\")\n",
    "    axes[0].set_ylabel(\"Raw Chroma Accuracy\")\n",
    "    axes[0].set_ylim(0.0, 1.05)\n",
    "\n",
    "    axes[1].errorbar(levels, vuv_mean, yerr=vuv_std, fmt=\"-o\", color=\"tab:red\", capsize=4)\n",
    "    axes[1].set_title(f\"{pathology} — V/UV flip rate\")\n",
    "    axes[1].set_xlabel(\"Condition level\")\n",
    "    axes[1].set_ylabel(\"Fraction of frames with voicing flips\")\n",
    "    axes[1].set_ylim(0.0, 1.0)\n",
    "\n",
    "    if pathology.lower().startswith(\"clipping\"):\n",
    "        axes[0].set_xlabel(\"Clipped samples (%)\")\n",
    "        axes[1].set_xlabel(\"Clipped samples (%)\")\n",
    "    else:\n",
    "        axes[0].set_xlabel(\"AGC severity (dB)\")\n",
    "        axes[1].set_xlabel(\"AGC severity (dB)\")\n",
    "\n",
    "    fig.suptitle(f\"{pathology} severity sweep\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
