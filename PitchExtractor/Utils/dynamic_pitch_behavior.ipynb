{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8534ac9",
   "metadata": {},
   "source": [
    "# Dynamic Pitch Behavior Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225f49e",
   "metadata": {},
   "source": [
    "This notebook evaluates how the latest pitch extraction model responds to dynamic pitch behaviors such as vibrato and portamento/glide transitions. Synthetic stimuli are generated with controlled parameters so that we can quantify accuracy, latency, and overshoot across a range of condition levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld torchcrepe praat-parselmouth pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613eb2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "from Utils.dynamic_pitch_tools import (\n",
    "    generate_vibrato_waveform,\n",
    "    generate_glide_waveform,\n",
    "    sample_reference_f0,\n",
    "    hz_to_cents,\n",
    "    circular_cents_distance,\n",
    "    rms_cents_error,\n",
    "    estimate_tracking_delay_ms,\n",
    "    compute_overshoot_cents,\n",
    ")\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599372",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"vibrato\": {\n",
    "        \"base_frequency_hz\": 220.0,\n",
    "        \"duration_seconds\": 3.0,\n",
    "        \"rates_hz\": [4.0, 6.0, 8.0],\n",
    "        \"depth_cents\": [20, 60, 120, 200],\n",
    "    },\n",
    "    \"glide\": {\n",
    "        \"start_hz\": 60.0,\n",
    "        \"end_hz\": 500.0,\n",
    "        \"durations_seconds\": [0.4, 0.8, 1.6, 3.2],\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "def _normalize_mel_params(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"win_len\" in params:\n",
    "        win_len = params.pop(\"win_len\")\n",
    "        params.setdefault(\"win_length\", win_len)\n",
    "    return params\n",
    "\n",
    "\n",
    "def _deep_merge_dict(base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    merged = base.copy()\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, dict):\n",
    "            existing = merged.get(key)\n",
    "            if isinstance(existing, dict):\n",
    "                merged[key] = _deep_merge_dict(existing, value)\n",
    "            else:\n",
    "                merged[key] = value.copy()\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "\n",
    "MEL_PARAMS = _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "def _collect_model_configuration(checkpoint_state: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    model_params: Dict[str, Any] = {}\n",
    "    training_params = TRAINING_CONFIG.get(\"model_params\")\n",
    "    if isinstance(training_params, dict):\n",
    "        model_params = _deep_merge_dict(model_params, training_params)\n",
    "    if isinstance(checkpoint_state, dict):\n",
    "        candidate = checkpoint_state.get(\"model_params\")\n",
    "        if isinstance(candidate, dict):\n",
    "            model_params = _deep_merge_dict(model_params, candidate)\n",
    "        config_section = checkpoint_state.get(\"config\")\n",
    "        if isinstance(config_section, dict):\n",
    "            candidate = config_section.get(\"model_params\")\n",
    "            if isinstance(candidate, dict):\n",
    "                model_params = _deep_merge_dict(model_params, candidate)\n",
    "    sequence_config = model_params.get(\"sequence_model\")\n",
    "    if isinstance(sequence_config, dict):\n",
    "        sequence_config = sequence_config.copy()\n",
    "    else:\n",
    "        sequence_config = {}\n",
    "    top_level = {k: v for k, v in model_params.items() if k != \"sequence_model\"}\n",
    "    top_level.pop(\"num_class\", None)\n",
    "    return top_level, sequence_config\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int] | None:\n",
    "    try:\n",
    "        audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: skipping unreadable file '{path}': {exc}\")\n",
    "        return None\n",
    "    audio = _ensure_mono(audio)\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: skipping empty file '{path}'.\")\n",
    "        return None\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\"Unexpected checkpoint format\")\n",
    "    model_state = checkpoint.get(\"model\")\n",
    "    if model_state is None:\n",
    "        model_state = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    if not isinstance(model_state, dict):\n",
    "        raise RuntimeError(\"Checkpoint is missing a valid model state\")\n",
    "    model_state = dict(model_state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\")\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(checkpoint.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    model_params, sequence_model_config = _collect_model_configuration(checkpoint)\n",
    "    model_kwargs: Dict[str, Any] = {}\n",
    "    if sequence_model_config:\n",
    "        model_kwargs[\"sequence_model_config\"] = sequence_model_config\n",
    "    leaky_relu_slope = model_params.get(\"leaky_relu_slope\")\n",
    "    if isinstance(leaky_relu_slope, (int, float)):\n",
    "        model_kwargs[\"leaky_relu_slope\"] = float(leaky_relu_slope)\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes, **model_kwargs)\n",
    "    incompatible = model.load_state_dict(model_state, strict=False)\n",
    "    if incompatible.unexpected_keys:\n",
    "        skipped = \", \".join(sorted(incompatible.unexpected_keys))\n",
    "        print(f\"Warning: skipped unexpected parameters: {skipped}\")\n",
    "    if incompatible.missing_keys:\n",
    "        missing = \", \".join(sorted(incompatible.missing_keys))\n",
    "        print(f\"Warning: missing parameters not found in checkpoint: {missing}\")\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vibrato_cfg = CONFIG[\"vibrato\"]\n",
    "base_freq = float(vibrato_cfg[\"base_frequency_hz\"])\n",
    "duration = float(vibrato_cfg[\"duration_seconds\"])\n",
    "rates = [float(r) for r in vibrato_cfg[\"rates_hz\"]]\n",
    "depths = [float(d) for d in vibrato_cfg[\"depth_cents\"]]\n",
    "\n",
    "VIBRATO_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for rate in rates:\n",
    "    for depth in depths:\n",
    "        audio, t, f0_curve = generate_vibrato_waveform(rate, depth, base_freq, duration, TARGET_SAMPLE_RATE)\n",
    "        prediction = predict_f0(model, audio)\n",
    "        reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "        metrics = compute_metrics(reference, prediction)\n",
    "        rmse = rms_cents_error(reference, prediction)\n",
    "        VIBRATO_RESULTS.append({\n",
    "            \"rate_hz\": rate,\n",
    "            \"depth_cents\": depth,\n",
    "            \"RPA\": metrics[\"RPA\"],\n",
    "            \"RCA\": metrics[\"RCA\"],\n",
    "            \"VUV\": metrics[\"VUV\"],\n",
    "            \"OctaveError\": metrics[\"OctaveError\"],\n",
    "            \"RMSE_cents\": rmse,\n",
    "        })\n",
    "\n",
    "vibrato_df = pd.DataFrame(VIBRATO_RESULTS)\n",
    "vibrato_df.sort_values([\"rate_hz\", \"depth_cents\"], inplace=True)\n",
    "vibrato_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "\n",
    "for rate in rates:\n",
    "    subset = vibrato_df[vibrato_df[\"rate_hz\"] == rate]\n",
    "    axes[0].plot(subset[\"depth_cents\"], subset[\"RPA\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "    axes[1].plot(subset[\"depth_cents\"], subset[\"RMSE_cents\"], marker=\"o\", label=f\"{rate:.1f} Hz\")\n",
    "\n",
    "axes[0].set_title(\"RPA vs. Vibrato Depth\")\n",
    "axes[0].set_ylabel(\"RPA\")\n",
    "axes[1].set_title(\"RMSE (cents) vs. Vibrato Depth\")\n",
    "axes[1].set_ylabel(\"RMSE (cents)\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Vibrato depth (cents)\")\n",
    "    ax.set_xticks(depths)\n",
    "    ax.grid(True)\n",
    "axes[-1].legend(title=\"Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glide_cfg = CONFIG[\"glide\"]\n",
    "start_hz = float(glide_cfg[\"start_hz\"])\n",
    "end_hz = float(glide_cfg[\"end_hz\"])\n",
    "durations = [float(d) for d in glide_cfg[\"durations_seconds\"]]\n",
    "\n",
    "GLIDE_RESULTS: List[Dict[str, float]] = []\n",
    "\n",
    "for duration in durations:\n",
    "    audio, t, f0_curve = generate_glide_waveform(duration, start_hz, end_hz, TARGET_SAMPLE_RATE)\n",
    "    prediction = predict_f0(model, audio)\n",
    "    reference = sample_reference_f0(t, f0_curve, prediction.shape[0])\n",
    "    metrics = compute_metrics(reference, prediction)\n",
    "    rmse = rms_cents_error(reference, prediction)\n",
    "    lag_ms = estimate_tracking_delay_ms(reference, prediction, FRAME_PERIOD_MS)\n",
    "    overshoot = compute_overshoot_cents(reference, prediction)\n",
    "    final_error = float(1200.0 * np.log2(max(prediction[-1], 1e-5) / max(reference[-1], 1e-5))) if prediction.size and reference[-1] > 0 else float(\"nan\")\n",
    "    GLIDE_RESULTS.append({\n",
    "        \"duration_s\": duration,\n",
    "        \"RPA\": metrics[\"RPA\"],\n",
    "        \"RCA\": metrics[\"RCA\"],\n",
    "        \"VUV\": metrics[\"VUV\"],\n",
    "        \"OctaveError\": metrics[\"OctaveError\"],\n",
    "        \"RMSE_cents\": rmse,\n",
    "        \"Lag_ms\": lag_ms,\n",
    "        \"Overshoot_cents\": overshoot,\n",
    "        \"Final_error_cents\": final_error,\n",
    "    })\n",
    "\n",
    "glide_df = pd.DataFrame(GLIDE_RESULTS)\n",
    "glide_df.sort_values(\"duration_s\", inplace=True)\n",
    "glide_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(glide_df[\"duration_s\"], glide_df[\"Lag_ms\"], marker=\"o\")\n",
    "axes[0].set_title(\"Tracking Delay vs. Glide Duration\")\n",
    "axes[0].set_xlabel(\"Duration (s)\")\n",
    "axes[0].set_ylabel(\"Lag (ms)\")\n",
    "\n",
    "axes[1].plot(glide_df[\"duration_s\"], glide_df[\"Overshoot_cents\"], marker=\"o\")\n",
    "axes[1].set_title(\"Overshoot vs. Glide Duration\")\n",
    "axes[1].set_xlabel(\"Duration (s)\")\n",
    "axes[1].set_ylabel(\"Overshoot (cents)\")\n",
    "\n",
    "axes[2].plot(glide_df[\"duration_s\"], glide_df[\"RMSE_cents\"], marker=\"o\")\n",
    "axes[2].set_title(\"RMSE vs. Glide Duration\")\n",
    "axes[2].set_xlabel(\"Duration (s)\")\n",
    "axes[2].set_ylabel(\"RMSE (cents)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0add441",
   "metadata": {},
   "outputs": [],
   "source": [
    "vibrato_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_vibrato_metrics.csv\"\n",
    "vibrato_df.to_csv(vibrato_path, index=False)\n",
    "print(f\"Saved vibrato metrics to {vibrato_path.resolve()}\")\n",
    "\n",
    "\n",
    "glide_path = CONFIG[\"output_dir\"] / \"dynamic_pitch_glide_metrics.csv\"\n",
    "glide_df.to_csv(glide_path, index=False)\n",
    "print(f\"Saved glide metrics to {glide_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
