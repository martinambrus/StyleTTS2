{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d137cfd7",
   "metadata": {},
   "source": [
    "# Codec and Bandwidth Torture Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0e693",
   "metadata": {},
   "source": [
    "\n",
    "This notebook evaluates how the latest pitch extraction model handles aggressive resampling and lossy compression.We sweep over a range of sample rates and codec/bitrate combinations, compare the resulting pitch metrics against thereference (clean) audio, and visualize where performance begins to degrade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d29ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pyworld torchcrepe praat-parselmouth pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from Utils.f0_notebook_utils import (\n",
    "    build_notebook_f0_extractor,\n",
    "    compute_f0_for_notebook,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bfb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"eval_list_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"resample_rates_hz\": [8000, 16000, 22050, 24000, 44100],\n",
    "    \"codecs\": {\n",
    "        \"opus\": {\"ffmpeg_codec\": \"libopus\", \"extension\": \".opus\", \"bitrates_kbps\": [16, 32, 64, 128]},\n",
    "        \"mp3\": {\"ffmpeg_codec\": \"libmp3lame\", \"extension\": \".mp3\", \"bitrates_kbps\": [16, 32, 64, 128]},\n",
    "        \"aac\": {\"ffmpeg_codec\": \"aac\", \"extension\": \".m4a\", \"bitrates_kbps\": [16, 32, 64, 128]},\n",
    "    },\n",
    "}\n",
    "\n",
    "CONFIG[\"output_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "def _normalize_mel_params(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"win_len\" in params:\n",
    "        win_len = params.pop(\"win_len\")\n",
    "        params.setdefault(\"win_length\", win_len)\n",
    "    return params\n",
    "\n",
    "\n",
    "def _deep_merge_dict(base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    merged = base.copy()\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, dict):\n",
    "            existing = merged.get(key)\n",
    "            if isinstance(existing, dict):\n",
    "                merged[key] = _deep_merge_dict(existing, value)\n",
    "            else:\n",
    "                merged[key] = value.copy()\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "\n",
    "MEL_PARAMS = _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "FALLBACK_SAMPLE_RATE = int(DEFAULT_MEL_PARAMS.get(\"sample_rate\", 24000))\n",
    "FALLBACK_HOP_LENGTH = int(DEFAULT_MEL_PARAMS.get(\"hop_length\", DEFAULT_MEL_PARAMS.get(\"win_len\", 300)))\n",
    "F0_PARAMS: Dict[str, Any] = {}\n",
    "F0_ZERO_FILL: float = 0.0\n",
    "REFERENCE_F0_BACKENDS_USED: set[str] = set()\n",
    "NOTEBOOK_F0_EXTRACTOR = build_notebook_f0_extractor(\n",
    "    MEL_PARAMS,\n",
    "    F0_PARAMS,\n",
    "    fallback_sr=FALLBACK_SAMPLE_RATE,\n",
    "    fallback_hop=FALLBACK_HOP_LENGTH,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "AUDIO_EXTENSIONS = {\".wav\", \".flac\", \".ogg\", \".mp3\", \".m4a\"}\n",
    "\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    global NOTEBOOK_F0_EXTRACTOR, F0_ZERO_FILL\n",
    "\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "    F0_PARAMS.clear()\n",
    "    F0_PARAMS.update(dataset_params.get(\"f0_params\", {}))\n",
    "    NOTEBOOK_F0_EXTRACTOR = build_notebook_f0_extractor(\n",
    "        MEL_PARAMS,\n",
    "        F0_PARAMS,\n",
    "        fallback_sr=FALLBACK_SAMPLE_RATE,\n",
    "        fallback_hop=FALLBACK_HOP_LENGTH,\n",
    "        verbose=True,\n",
    "    )\n",
    "    F0_ZERO_FILL = float(F0_PARAMS.get(\"zero_fill_value\", 0.0))\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = int(MEL_PARAMS[\"sample_rate\"])\n",
    "HOP_LENGTH = int(MEL_PARAMS[\"hop_length\"])\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _collect_model_configuration(checkpoint_state: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    model_params: Dict[str, Any] = {}\n",
    "    training_params = TRAINING_CONFIG.get(\"model_params\")\n",
    "    if isinstance(training_params, dict):\n",
    "        model_params = _deep_merge_dict(model_params, training_params)\n",
    "    if isinstance(checkpoint_state, dict):\n",
    "        candidate = checkpoint_state.get(\"model_params\")\n",
    "        if isinstance(candidate, dict):\n",
    "            model_params = _deep_merge_dict(model_params, candidate)\n",
    "        config_section = checkpoint_state.get(\"config\")\n",
    "        if isinstance(config_section, dict):\n",
    "            candidate = config_section.get(\"model_params\")\n",
    "            if isinstance(candidate, dict):\n",
    "                model_params = _deep_merge_dict(model_params, candidate)\n",
    "    sequence_config = model_params.get(\"sequence_model\")\n",
    "    if isinstance(sequence_config, dict):\n",
    "        sequence_config = sequence_config.copy()\n",
    "    else:\n",
    "        sequence_config = {}\n",
    "    top_level = {k: v for k, v in model_params.items() if k != \"sequence_model\"}\n",
    "    top_level.pop(\"num_class\", None)\n",
    "    return top_level, sequence_config\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int] | None:\n",
    "    try:\n",
    "        audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: skipping unreadable file '{path}': {exc}\")\n",
    "        return None\n",
    "    audio = _ensure_mono(audio)\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: skipping empty file '{path}'.\")\n",
    "        return None\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "def compute_reference_f0(audio: np.ndarray, sr: int) -> np.ndarray:\n",
    "    result = compute_f0_for_notebook(\n",
    "        audio,\n",
    "        sr,\n",
    "        NOTEBOOK_F0_EXTRACTOR,\n",
    "        zero_fill_value=F0_ZERO_FILL,\n",
    "    )\n",
    "    if result.backend_name and result.backend_name not in REFERENCE_F0_BACKENDS_USED:\n",
    "        REFERENCE_F0_BACKENDS_USED.add(result.backend_name)\n",
    "        print(f\"Using F0 backend '{result.backend_name}' for reference computation.\")\n",
    "    return result.f0\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\"Unexpected checkpoint format\")\n",
    "    model_state = checkpoint.get(\"model\")\n",
    "    if model_state is None:\n",
    "        model_state = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    if not isinstance(model_state, dict):\n",
    "        raise RuntimeError(\"Checkpoint is missing a valid model state\")\n",
    "    model_state = dict(model_state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\")\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(checkpoint.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    model_params, sequence_model_config = _collect_model_configuration(checkpoint)\n",
    "    model_kwargs: Dict[str, Any] = {}\n",
    "    if sequence_model_config:\n",
    "        model_kwargs[\"sequence_model_config\"] = sequence_model_config\n",
    "    leaky_relu_slope = model_params.get(\"leaky_relu_slope\")\n",
    "    if isinstance(leaky_relu_slope, (int, float)):\n",
    "        model_kwargs[\"leaky_relu_slope\"] = float(leaky_relu_slope)\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes, **model_kwargs)\n",
    "    incompatible = model.load_state_dict(model_state, strict=False)\n",
    "    if incompatible.unexpected_keys:\n",
    "        skipped = \", \".join(sorted(incompatible.unexpected_keys))\n",
    "        print(f\"Warning: skipped unexpected parameters: {skipped}\")\n",
    "    if incompatible.missing_keys:\n",
    "        missing = \", \".join(sorted(incompatible.missing_keys))\n",
    "        print(f\"Warning: missing parameters not found in checkpoint: {missing}\")\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hz_to_cents(f0: np.ndarray) -> np.ndarray:\n",
    "    cents = np.zeros_like(f0)\n",
    "    positive = f0 > 0\n",
    "    cents[positive] = 1200.0 * np.log2(f0[positive] / 55.0)\n",
    "    return cents\n",
    "\n",
    "\n",
    "def circular_cents_distance(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    diff = a - b\n",
    "    diff = np.mod(diff + 600.0, 1200.0) - 600.0\n",
    "    return diff\n",
    "\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "\n",
    "    total_frames = length\n",
    "    voiced_frames = np.count_nonzero(ref_voiced)\n",
    "\n",
    "    vuv_accuracy = np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1)\n",
    "\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (\n",
    "        np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"RPA\": np.count_nonzero(rpa_hits) / voiced_frames,\n",
    "        \"RCA\": np.count_nonzero(rca_hits) / voiced_frames,\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": np.count_nonzero(octave_errors) / voiced_frames,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_CACHE: List[Dict[str, Any]] | None = None\n",
    "\n",
    "\n",
    "def prepare_dataset_cache(force: bool = False) -> List[Dict[str, Any]]:\n",
    "    global DATASET_CACHE\n",
    "    if DATASET_CACHE is not None and not force:\n",
    "        return DATASET_CACHE\n",
    "\n",
    "    eval_list = CONFIG.get(\"eval_list_path\")\n",
    "    if eval_list is None or not Path(eval_list).is_file():\n",
    "        raise FileNotFoundError(f\"Evaluation list not found: {eval_list}\")\n",
    "\n",
    "    entries: List[Path] = []\n",
    "    with open(eval_list, \"r\", encoding=\"utf-8\") as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            relative_path = line.split(\"|\")[0]\n",
    "            candidate = Path(relative_path)\n",
    "            if not candidate.is_absolute():\n",
    "                candidate = (Path(eval_list).parent / candidate).resolve()\n",
    "            entries.append(candidate)\n",
    "\n",
    "    if not entries:\n",
    "        raise RuntimeError(f\"No evaluation files listed in {eval_list}\")\n",
    "\n",
    "    cache: List[Dict[str, Any]] = []\n",
    "    for path in tqdm(entries, desc=\"Preparing evaluation cache\"):\n",
    "        result = load_waveform(path, TARGET_SAMPLE_RATE)\n",
    "        if result is None:\n",
    "            continue\n",
    "        audio, sr = result\n",
    "        reference_f0 = compute_reference_f0(audio, sr)\n",
    "        cache.append(\n",
    "            {\n",
    "                \"path\": path,\n",
    "                \"audio\": audio,\n",
    "                \"sample_rate\": sr,\n",
    "                \"reference_f0\": reference_f0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    DATASET_CACHE = cache\n",
    "    print(f\"Cached {len(DATASET_CACHE)} evaluation utterances.\")\n",
    "    return DATASET_CACHE\n",
    "\n",
    "\n",
    "def evaluate_condition(\n",
    "    model: JDCNet,\n",
    "    dataset: List[Dict[str, Any]],\n",
    "    transform_fn,\n",
    "    label: Dict[str, Any],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for entry in dataset:\n",
    "        processed = transform_fn(entry)\n",
    "        prediction = predict_f0(model, processed)\n",
    "        metrics = compute_metrics(entry[\"reference_f0\"], prediction)\n",
    "        record = {\"path\": str(entry[\"path\"]), **label, **metrics}\n",
    "        results.append(record)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FFMPEG_PATH = shutil.which(\"ffmpeg\")\n",
    "if FFMPEG_PATH is None:\n",
    "    raise EnvironmentError(\"ffmpeg executable not found. Install ffmpeg to run codec evaluations.\")\n",
    "\n",
    "\n",
    "TORCHAUDIO_RESAMPLER_CACHE: Dict[tuple[int, int], torchaudio.transforms.Resample] = {}\n",
    "\n",
    "\n",
    "def _resample_audio(audio: np.ndarray, src_sr: int, dst_sr: int) -> np.ndarray:\n",
    "    if src_sr == dst_sr:\n",
    "        return audio.astype(np.float32, copy=True)\n",
    "    key = (src_sr, dst_sr)\n",
    "    if key not in TORCHAUDIO_RESAMPLER_CACHE:\n",
    "        TORCHAUDIO_RESAMPLER_CACHE[key] = torchaudio.transforms.Resample(src_sr, dst_sr)\n",
    "    transform = TORCHAUDIO_RESAMPLER_CACHE[key]\n",
    "    tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        resampled = transform(tensor)\n",
    "    return resampled.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_resample_condition(entry: Dict[str, Any], target_rate: int) -> np.ndarray:\n",
    "    degraded = _resample_audio(entry[\"audio\"], entry[\"sample_rate\"], target_rate)\n",
    "    if target_rate != TARGET_SAMPLE_RATE:\n",
    "        degraded = _resample_audio(degraded, target_rate, TARGET_SAMPLE_RATE)\n",
    "    return degraded.astype(np.float32)\n",
    "\n",
    "\n",
    "def _ffmpeg_encode_decode(\n",
    "    audio: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    codec_key: str,\n",
    "    ffmpeg_codec: str,\n",
    "    extension: str,\n",
    "    bitrate_kbps: int,\n",
    ") -> np.ndarray:\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tmpdir_path = Path(tmpdir)\n",
    "        input_path = tmpdir_path / \"input.wav\"\n",
    "        encoded_path = tmpdir_path / f\"encoded{extension}\"\n",
    "        output_path = tmpdir_path / \"decoded.wav\"\n",
    "\n",
    "        sf.write(str(input_path), audio, sample_rate)\n",
    "\n",
    "        bitrate_arg = f\"{int(bitrate_kbps)}k\"\n",
    "        encode_cmd = [\n",
    "            FFMPEG_PATH,\n",
    "            \"-y\",\n",
    "            \"-loglevel\",\n",
    "            \"error\",\n",
    "            \"-i\",\n",
    "            str(input_path),\n",
    "            \"-c:a\",\n",
    "            ffmpeg_codec,\n",
    "            \"-b:a\",\n",
    "            bitrate_arg,\n",
    "            str(encoded_path),\n",
    "        ]\n",
    "        result = subprocess.run(encode_cmd, capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"ffmpeg encoding failed for {codec_key} @ {bitrate_kbps} kbps:\\n{result.stderr.decode()}\"\n",
    "            )\n",
    "\n",
    "        decode_cmd = [\n",
    "            FFMPEG_PATH,\n",
    "            \"-y\",\n",
    "            \"-loglevel\",\n",
    "            \"error\",\n",
    "            \"-i\",\n",
    "            str(encoded_path),\n",
    "            \"-ar\",\n",
    "            str(TARGET_SAMPLE_RATE),\n",
    "            str(output_path),\n",
    "        ]\n",
    "        result = subprocess.run(decode_cmd, capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"ffmpeg decoding failed for {codec_key} @ {bitrate_kbps} kbps:\\n{result.stderr.decode()}\"\n",
    "            )\n",
    "\n",
    "        degraded, sr = sf.read(str(output_path), dtype=\"float32\")\n",
    "        degraded = _ensure_mono(degraded)\n",
    "        if sr != TARGET_SAMPLE_RATE:\n",
    "            degraded = _resample_audio(degraded, sr, TARGET_SAMPLE_RATE)\n",
    "        return degraded.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_codec_condition(\n",
    "    entry: Dict[str, Any],\n",
    "    codec_key: str,\n",
    "    codec_config: Dict[str, Any],\n",
    "    bitrate_kbps: int,\n",
    ") -> np.ndarray:\n",
    "    return _ffmpeg_encode_decode(\n",
    "        entry[\"audio\"],\n",
    "        entry[\"sample_rate\"],\n",
    "        codec_key,\n",
    "        codec_config[\"ffmpeg_codec\"],\n",
    "        codec_config[\"extension\"],\n",
    "        bitrate_kbps,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392fa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model()\n",
    "dataset_cache = prepare_dataset_cache()\n",
    "print(f\"Loaded dataset cache with {len(dataset_cache)} entries at {TARGET_SAMPLE_RATE} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "METRICS = [\"RPA\", \"RCA\", \"VUV\", \"OctaveError\"]\n",
    "METRIC_DIRECTIONS = {\"RPA\": \"higher\", \"RCA\": \"higher\", \"VUV\": \"higher\", \"OctaveError\": \"lower\"}\n",
    "\n",
    "baseline_records = evaluate_condition(\n",
    "    model,\n",
    "    dataset_cache,\n",
    "    lambda entry: entry[\"audio\"],\n",
    "    {\"condition_type\": \"baseline\"},\n",
    ")\n",
    "baseline_df = pd.DataFrame(baseline_records)\n",
    "baseline_summary = baseline_df[METRICS].agg([\"mean\", \"std\"])\n",
    "baseline_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e80e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resample_records: List[Dict[str, Any]] = []\n",
    "for rate in CONFIG[\"resample_rates_hz\"]:\n",
    "    resample_records.extend(\n",
    "        evaluate_condition(\n",
    "            model,\n",
    "            dataset_cache,\n",
    "            lambda entry, rate=rate: apply_resample_condition(entry, rate),\n",
    "            {\"condition_type\": \"resample\", \"resample_hz\": rate},\n",
    "        )\n",
    "    )\n",
    "\n",
    "resample_df = pd.DataFrame(resample_records)\n",
    "resample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49035f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _summarize_with_drop(df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:\n",
    "    grouped = df.groupby(group_cols)\n",
    "    summary = grouped[METRICS].agg([\"mean\", \"std\"])\n",
    "    summary.columns = [f\"{metric}_{stat}\" for metric, stat in summary.columns]\n",
    "    summary = summary.reset_index()\n",
    "    for metric in METRICS:\n",
    "        orientation = METRIC_DIRECTIONS[metric]\n",
    "        baseline_value = baseline_summary.loc[\"mean\", metric]\n",
    "        if orientation == \"higher\":\n",
    "            drop = baseline_value - summary[f\"{metric}_mean\"]\n",
    "        else:\n",
    "            drop = summary[f\"{metric}_mean\"] - baseline_value\n",
    "        summary[f\"{metric}_drop\"] = drop\n",
    "    return summary\n",
    "\n",
    "\n",
    "resample_summary = _summarize_with_drop(resample_df, [\"resample_hz\"])\n",
    "resample_summary.sort_values(\"resample_hz\", inplace=True)\n",
    "resample_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f399e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "fig, axes = plt.subplots(1, len(METRICS), figsize=(4 * len(METRICS), 4), sharex=False)\n",
    "for metric, ax in zip(METRICS, axes):\n",
    "    ax.plot(\n",
    "        resample_summary[\"resample_hz\"],\n",
    "        resample_summary[f\"{metric}_mean\"],\n",
    "        marker=\"o\",\n",
    "        label=\"Resampled\",\n",
    "    )\n",
    "    ax.axhline(baseline_summary.loc[\"mean\", metric], color=\"k\", linestyle=\"--\", label=\"Baseline\")\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"Sample rate (Hz)\")\n",
    "    if metric == \"OctaveError\":\n",
    "        ax.set_ylabel(\"Error rate\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Score\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks(CONFIG[\"resample_rates_hz\"])\n",
    "    ax.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "axes[-1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, len(METRICS), figsize=(4 * len(METRICS), 4))\n",
    "for metric, ax in zip(METRICS, axes):\n",
    "    ax.plot(\n",
    "        resample_summary[\"resample_hz\"],\n",
    "        resample_summary[f\"{metric}_drop\"],\n",
    "        marker=\"o\",\n",
    "        label=\"Drop\",\n",
    "    )\n",
    "    ax.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "    ax.set_title(f\"{metric} drop vs. baseline\")\n",
    "    ax.set_xlabel(\"Sample rate (Hz)\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks(CONFIG[\"resample_rates_hz\"])\n",
    "    ax.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "    ax.set_ylabel(\"Drop\" if metric != \"OctaveError\" else \"Increase\")\n",
    "axes[-1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8324d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codec_records: List[Dict[str, Any]] = []\n",
    "for codec_key, codec_config in CONFIG[\"codecs\"].items():\n",
    "    for bitrate in codec_config[\"bitrates_kbps\"]:\n",
    "        codec_records.extend(\n",
    "            evaluate_condition(\n",
    "                model,\n",
    "                dataset_cache,\n",
    "                lambda entry, ck=codec_key, cfg=codec_config, br=bitrate: apply_codec_condition(\n",
    "                    entry, ck, cfg, br\n",
    "                ),\n",
    "                {\n",
    "                    \"condition_type\": \"codec\",\n",
    "                    \"codec\": codec_key,\n",
    "                    \"bitrate_kbps\": bitrate,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "codec_df = pd.DataFrame(codec_records)\n",
    "codec_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c196822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codec_summary = _summarize_with_drop(codec_df, [\"codec\", \"bitrate_kbps\"])\n",
    "codec_summary.sort_values([\"codec\", \"bitrate_kbps\"], inplace=True)\n",
    "codec_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b478a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(len(METRICS), 1, figsize=(6, 3 * len(METRICS)), sharex=True)\n",
    "for metric, ax in zip(METRICS, axes):\n",
    "    for codec_key in codec_summary[\"codec\"].unique():\n",
    "        subset = codec_summary[codec_summary[\"codec\"] == codec_key]\n",
    "        ax.plot(\n",
    "            subset[\"bitrate_kbps\"],\n",
    "            subset[f\"{metric}_mean\"],\n",
    "            marker=\"o\",\n",
    "            label=codec_key.upper(),\n",
    "        )\n",
    "    ax.axhline(baseline_summary.loc[\"mean\", metric], color=\"k\", linestyle=\"--\", label=\"Baseline\")\n",
    "    ax.set_title(metric)\n",
    "    ax.set_ylabel(\"Score\" if metric != \"OctaveError\" else \"Error rate\")\n",
    "    ax.set_xlabel(\"Bitrate (kbps)\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1.0), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(len(METRICS), 1, figsize=(6, 3 * len(METRICS)), sharex=True)\n",
    "for metric, ax in zip(METRICS, axes):\n",
    "    for codec_key in codec_summary[\"codec\"].unique():\n",
    "        subset = codec_summary[codec_summary[\"codec\"] == codec_key]\n",
    "        ax.plot(\n",
    "            subset[\"bitrate_kbps\"],\n",
    "            subset[f\"{metric}_drop\"],\n",
    "            marker=\"o\",\n",
    "            label=codec_key.upper(),\n",
    "        )\n",
    "    ax.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "    ax.set_title(f\"{metric} drop vs. baseline\")\n",
    "    ax.set_ylabel(\"Drop\" if metric != \"OctaveError\" else \"Increase\")\n",
    "    ax.set_xlabel(\"Bitrate (kbps)\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1.0), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_base = CONFIG[\"output_dir\"]\n",
    "output_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "baseline_path = output_base / \"codec_bandwidth_baseline_metrics.csv\"\n",
    "resample_path = output_base / \"codec_bandwidth_resample_metrics.csv\"\n",
    "codec_path = output_base / \"codec_bandwidth_codec_metrics.csv\"\n",
    "resample_summary_path = output_base / \"codec_bandwidth_resample_summary.csv\"\n",
    "codec_summary_path = output_base / \"codec_bandwidth_codec_summary.csv\"\n",
    "\n",
    "baseline_df.to_csv(baseline_path, index=False)\n",
    "resample_df.to_csv(resample_path, index=False)\n",
    "codec_df.to_csv(codec_path, index=False)\n",
    "resample_summary.to_csv(resample_summary_path, index=False)\n",
    "codec_summary.to_csv(codec_summary_path, index=False)\n",
    "\n",
    "print(f\"Saved detailed metrics to {baseline_path}\")\n",
    "print(f\"Saved resample sweep metrics to {resample_path}\")\n",
    "print(f\"Saved codec sweep metrics to {codec_path}\")\n",
    "print(f\"Saved resample summary to {resample_summary_path}\")\n",
    "print(f\"Saved codec summary to {codec_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5df89",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "* Inspect the CSV artifacts for deeper analysis or downstream reporting.\n",
    "* Adjust `CONFIG[\"resample_rates_hz\"]` and `CONFIG[\"codecs\"]` to explore additional stress conditions.\n",
    "* Consider combining these degradations (e.g., resampling **and** compressing) to probe compounded failure modes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
