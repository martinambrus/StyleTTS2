{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45636446",
   "metadata": {},
   "source": [
    "# Pitch Range & Timbre Coverage Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb5b0b",
   "metadata": {},
   "source": [
    "This notebook sweeps through defined vocal register ranges and synthetic timbre profiles to evaluate the latest pitch extraction model. It reports per-range metrics, highlights octave confusions at the extreme ends of each range, and visualizes how accuracy varies with pitch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soundfile torchaudio torch pyyaml matplotlib librosa pandas tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "from meldataset import DEFAULT_MEL_PARAMS\n",
    "from model import JDCNet\n",
    "from Utils.dynamic_pitch_tools import sample_reference_f0, hz_to_cents, circular_cents_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3190821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 4), \"axes.grid\": True})\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, Any] = {\n",
    "    \"config_path\": REPO_ROOT / \"Configs\" / \"config.yml\",\n",
    "    \"checkpoint_dir\": REPO_ROOT / \"Checkpoint\",\n",
    "    \"checkpoint_path\": None,\n",
    "    \"chunk_size\": 192,\n",
    "    \"chunk_overlap\": 48,\n",
    "    \"mel_mean\": -4.0,\n",
    "    \"mel_std\": 4.0,\n",
    "    \"voicing_threshold_hz\": 10.0,\n",
    "    \"output_dir\": REPO_ROOT / \"notebooks\" / \"artifacts\",\n",
    "    \"stimulus\": {\n",
    "        \"duration_seconds\": 2.5,\n",
    "        \"frequencies_per_range\": 15,\n",
    "        \"edge_band_fraction\": 0.15,\n",
    "        \"random_seed\": 1337,\n",
    "    },\n",
    "    \"ranges\": [\n",
    "        {\"name\": \"Bass\", \"min_hz\": 70.0, \"max_hz\": 120.0},\n",
    "        {\"name\": \"Baritone/Tenor\", \"min_hz\": 120.0, \"max_hz\": 220.0},\n",
    "        {\"name\": \"Alto\", \"min_hz\": 220.0, \"max_hz\": 350.0},\n",
    "        {\"name\": \"Child/Falsetto\", \"min_hz\": 350.0, \"max_hz\": 1000.0},\n",
    "    ],\n",
    "    \"timbre_profiles\": {\n",
    "        \"Pure Sine\": {\"partials\": {1: 1.0}},\n",
    "        \"Warm Vocal\": {\"partials\": {1: 1.0, 2: 0.45, 3: 0.2}},\n",
    "        \"Bright Belt\": {\"partials\": {1: 1.0, 2: 0.9, 3: 0.75, 4: 0.5, 5: 0.35}},\n",
    "        \"Breathy Head\": {\"partials\": {1: 1.0, 2: 0.5, 3: 0.35}, \"snr_db\": 25.0},\n",
    "    },\n",
    "}\n",
    "\n",
    "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629579ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEL_PARAMS = DEFAULT_MEL_PARAMS.copy()\n",
    "\n",
    "def _normalize_mel_params(params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if \"win_len\" in params:\n",
    "        win_len = params.pop(\"win_len\")\n",
    "        params.setdefault(\"win_length\", win_len)\n",
    "    return params\n",
    "\n",
    "\n",
    "def _deep_merge_dict(base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    merged = base.copy()\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, dict):\n",
    "            existing = merged.get(key)\n",
    "            if isinstance(existing, dict):\n",
    "                merged[key] = _deep_merge_dict(existing, value)\n",
    "            else:\n",
    "                merged[key] = value.copy()\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n",
    "\n",
    "MEL_PARAMS = _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "def _resolve_relative_path(base: Path, candidate: str | Path) -> Path:\n",
    "    base_dir = base if base.is_dir() else base.parent\n",
    "    candidate_path = Path(candidate)\n",
    "    if candidate_path.is_absolute():\n",
    "        return candidate_path\n",
    "    repo_candidate = (REPO_ROOT / candidate_path).resolve()\n",
    "    config_candidate = (base_dir / candidate_path).resolve()\n",
    "    if repo_candidate.exists():\n",
    "        return repo_candidate\n",
    "    if config_candidate.exists():\n",
    "        return config_candidate\n",
    "    return repo_candidate\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def _latest_checkpoint(path: Path) -> Optional[Path]:\n",
    "    if not path.is_dir():\n",
    "        return None\n",
    "\n",
    "    def _sort_key(p: Path) -> Tuple[int, float]:\n",
    "        numbers = [int(match) for match in re.findall(r\"\\d+\", p.stem)]\n",
    "        last = numbers[-1] if numbers else -1\n",
    "        return last, p.stat().st_mtime\n",
    "\n",
    "    candidates = sorted(path.glob(\"*.pth\"), key=_sort_key)\n",
    "    return candidates[-1] if candidates else None\n",
    "\n",
    "\n",
    "TRAINING_CONFIG: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "def _load_training_config() -> Dict[str, Any]:\n",
    "    config_path = CONFIG.get(\"config_path\")\n",
    "    if not config_path or not Path(config_path).is_file():\n",
    "        print(\"Warning: config file not found; using DEFAULT_MEL_PARAMS.\")\n",
    "        return {}\n",
    "\n",
    "    import yaml\n",
    "\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        data = yaml.safe_load(handle) or {}\n",
    "\n",
    "    dataset_params = data.get(\"dataset_params\", {})\n",
    "    MEL_PARAMS.update(dataset_params.get(\"mel_params\", {}))\n",
    "    _normalize_mel_params(MEL_PARAMS)\n",
    "\n",
    "    dataset_sr = dataset_params.get(\"sr\")\n",
    "    if dataset_sr is not None:\n",
    "        MEL_PARAMS[\"sample_rate\"] = dataset_sr\n",
    "\n",
    "    val_list = data.get(\"val_data\")\n",
    "    if val_list and CONFIG.get(\"eval_list_path\") is None:\n",
    "        CONFIG[\"eval_list_path\"] = _resolve_relative_path(config_path, val_list)\n",
    "        if not CONFIG[\"eval_list_path\"].is_file():\n",
    "            print(f\"Warning: evaluation list not found at {CONFIG['eval_list_path']}\")\n",
    "\n",
    "    log_dir = data.get(\"log_dir\")\n",
    "    if log_dir and (not CONFIG.get(\"checkpoint_dir\") or not Path(CONFIG[\"checkpoint_dir\"]).is_dir()):\n",
    "        CONFIG[\"checkpoint_dir\"] = _resolve_relative_path(config_path, log_dir)\n",
    "\n",
    "    if CONFIG.get(\"checkpoint_path\") is None:\n",
    "        latest = _latest_checkpoint(Path(CONFIG[\"checkpoint_dir\"]))\n",
    "        if latest is not None:\n",
    "            CONFIG[\"checkpoint_path\"] = latest\n",
    "        else:\n",
    "            print(\"Warning: no checkpoints found; set CONFIG['checkpoint_path'] manually.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "TRAINING_CONFIG = _load_training_config()\n",
    "TARGET_SAMPLE_RATE = MEL_PARAMS[\"sample_rate\"]\n",
    "HOP_LENGTH = MEL_PARAMS[\"hop_length\"]\n",
    "FRAME_PERIOD_MS = HOP_LENGTH * 1000.0 / TARGET_SAMPLE_RATE\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(**MEL_PARAMS).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(CONFIG[\"stimulus\"][\"random_seed\"])\n",
    "\n",
    "\n",
    "def _apply_fade(audio: np.ndarray, sr: int, fade_time: float = 0.02) -> np.ndarray:\n",
    "    fade_samples = int(max(fade_time * sr, 0))\n",
    "    if fade_samples <= 0:\n",
    "        return audio.astype(np.float32, copy=False)\n",
    "    window = np.ones_like(audio, dtype=np.float64)\n",
    "    ramp = 0.5 - 0.5 * np.cos(np.linspace(0.0, math.pi, fade_samples, dtype=np.float64))\n",
    "    window[:fade_samples] = ramp\n",
    "    window[-fade_samples:] = ramp[::-1]\n",
    "    return (audio * window).astype(np.float32)\n",
    "\n",
    "\n",
    "def _normalize(audio: np.ndarray) -> np.ndarray:\n",
    "    peak = float(np.max(np.abs(audio)))\n",
    "    if peak > 0.99:\n",
    "        audio = audio / (peak + 1e-6)\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "\n",
    "def synthesize_timbre_waveform(frequency: float, sr: int, duration: float, profile: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    total_samples = int(duration * sr)\n",
    "    t = np.linspace(0.0, duration, total_samples, endpoint=False, dtype=np.float64)\n",
    "    partials = profile.get(\"partials\", {1: 1.0})\n",
    "    waveform = np.zeros_like(t, dtype=np.float64)\n",
    "    for harmonic, amplitude in partials.items():\n",
    "        if amplitude == 0:\n",
    "            continue\n",
    "        waveform += amplitude * np.sin(2.0 * math.pi * frequency * float(harmonic) * t)\n",
    "\n",
    "    envelope = profile.get(\"envelope\")\n",
    "    if callable(envelope):\n",
    "        waveform *= envelope(t)\n",
    "\n",
    "    waveform = _apply_fade(waveform.astype(np.float32), sr)\n",
    "    signal_rms = float(np.sqrt(np.mean(waveform**2))) if waveform.size else 0.0\n",
    "\n",
    "    snr_db = profile.get(\"snr_db\")\n",
    "    if snr_db is not None and signal_rms > 0:\n",
    "        noise = rng.standard_normal(waveform.shape).astype(np.float32)\n",
    "        noise_rms = float(np.sqrt(np.mean(noise**2)))\n",
    "        if noise_rms > 0:\n",
    "            target_noise_rms = signal_rms / (10.0 ** (snr_db / 20.0))\n",
    "            noise *= target_noise_rms / noise_rms\n",
    "            waveform = waveform + noise\n",
    "\n",
    "    return _normalize(waveform), t.astype(np.float32)\n",
    "\n",
    "\n",
    "def _ensure_mono(audio: np.ndarray) -> np.ndarray:\n",
    "    if audio.ndim == 1:\n",
    "        return audio\n",
    "    return audio.mean(axis=1)\n",
    "\n",
    "\n",
    "def load_waveform(path: Path, target_sr: int = TARGET_SAMPLE_RATE) -> tuple[np.ndarray, int] | None:\n",
    "    try:\n",
    "        audio, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: skipping unreadable file '{path}': {exc}\")\n",
    "        return None\n",
    "    audio = _ensure_mono(audio)\n",
    "    if audio.size == 0:\n",
    "        print(f\"Warning: skipping empty file '{path}'.\")\n",
    "        return None\n",
    "    if sr != target_sr:\n",
    "        tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "        resampled = torchaudio.functional.resample(tensor, sr, target_sr)\n",
    "        audio = resampled.squeeze(0).cpu().numpy()\n",
    "        sr = target_sr\n",
    "    return audio.astype(np.float32), sr\n",
    "\n",
    "def waveform_to_mel(audio: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.from_numpy(audio).float().unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mel = mel_transform(tensor)\n",
    "    mel = torch.log(mel + 1e-5)\n",
    "    mel = (mel - CONFIG[\"mel_mean\"]) / CONFIG[\"mel_std\"]\n",
    "    return mel.squeeze(0)\n",
    "\n",
    "\n",
    "def predict_f0(model: JDCNet, audio: np.ndarray) -> np.ndarray:\n",
    "    mel = waveform_to_mel(audio)\n",
    "    total_frames = mel.shape[-1]\n",
    "    chunk_size = CONFIG[\"chunk_size\"]\n",
    "    overlap = CONFIG[\"chunk_overlap\"]\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    preds: List[np.ndarray] = []\n",
    "    for start in range(0, total_frames, step):\n",
    "        end = min(start + chunk_size, total_frames)\n",
    "        mel_chunk = mel[:, start:end]\n",
    "        pad = chunk_size - mel_chunk.shape[-1]\n",
    "        if pad > 0:\n",
    "            mel_chunk = torch.nn.functional.pad(mel_chunk, (0, pad))\n",
    "        mel_chunk = mel_chunk.unsqueeze(0).unsqueeze(0).transpose(-1, -2)\n",
    "        with torch.no_grad():\n",
    "            f0_chunk, _ = model(mel_chunk)\n",
    "        f0_chunk = f0_chunk.squeeze().detach().cpu().numpy()\n",
    "        preds.append(f0_chunk[: end - start])\n",
    "    if preds:\n",
    "        return np.concatenate(preds)\n",
    "    return np.zeros((0,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_metrics(reference: np.ndarray, prediction: np.ndarray) -> Dict[str, float]:\n",
    "    length = min(reference.shape[0], prediction.shape[0])\n",
    "    reference = reference[:length]\n",
    "    prediction = prediction[:length]\n",
    "    ref_voiced = reference > 0\n",
    "    pred_voiced = prediction > CONFIG[\"voicing_threshold_hz\"]\n",
    "    total_frames = length\n",
    "    voiced_frames = int(np.count_nonzero(ref_voiced))\n",
    "    vuv_accuracy = float(np.count_nonzero(ref_voiced == pred_voiced) / max(total_frames, 1))\n",
    "    if voiced_frames == 0:\n",
    "        return {\n",
    "            \"RPA\": float(\"nan\"),\n",
    "            \"RCA\": float(\"nan\"),\n",
    "            \"VUV\": vuv_accuracy,\n",
    "            \"OctaveError\": float(\"nan\"),\n",
    "        }\n",
    "    ref_cents = hz_to_cents(reference[ref_voiced])\n",
    "    pred_cents = hz_to_cents(np.clip(prediction[ref_voiced], a_min=1e-5, a_max=None))\n",
    "    cents_diff = pred_cents - ref_cents\n",
    "    rpa_hits = np.abs(cents_diff) <= 50.0\n",
    "    chroma_diff = circular_cents_distance(pred_cents, ref_cents)\n",
    "    rca_hits = np.abs(chroma_diff) <= 50.0\n",
    "    octave_candidates = np.abs(cents_diff) > 50.0\n",
    "    octave_numbers = np.round(cents_diff / 1200.0)\n",
    "    octave_errors = octave_candidates & (octave_numbers != 0) & (np.abs(cents_diff - octave_numbers * 1200.0) <= 50.0)\n",
    "    return {\n",
    "        \"RPA\": float(np.count_nonzero(rpa_hits) / voiced_frames),\n",
    "        \"RCA\": float(np.count_nonzero(rca_hits) / voiced_frames),\n",
    "        \"VUV\": vuv_accuracy,\n",
    "        \"OctaveError\": float(np.count_nonzero(octave_errors) / voiced_frames),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _collect_model_configuration(checkpoint_state: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    model_params: Dict[str, Any] = {}\n",
    "    training_params = TRAINING_CONFIG.get(\"model_params\")\n",
    "    if isinstance(training_params, dict):\n",
    "        model_params = _deep_merge_dict(model_params, training_params)\n",
    "    if isinstance(checkpoint_state, dict):\n",
    "        candidate = checkpoint_state.get(\"model_params\")\n",
    "        if isinstance(candidate, dict):\n",
    "            model_params = _deep_merge_dict(model_params, candidate)\n",
    "        config_section = checkpoint_state.get(\"config\")\n",
    "        if isinstance(config_section, dict):\n",
    "            candidate = config_section.get(\"model_params\")\n",
    "            if isinstance(candidate, dict):\n",
    "                model_params = _deep_merge_dict(model_params, candidate)\n",
    "    sequence_config = model_params.get(\"sequence_model\")\n",
    "    if isinstance(sequence_config, dict):\n",
    "        sequence_config = sequence_config.copy()\n",
    "    else:\n",
    "        sequence_config = {}\n",
    "    top_level = {k: v for k, v in model_params.items() if k != \"sequence_model\"}\n",
    "    top_level.pop(\"num_class\", None)\n",
    "    return top_level, sequence_config\n",
    "\n",
    "\n",
    "def load_model(checkpoint_path: Optional[Path] = None) -> JDCNet:\n",
    "    checkpoint_path = Path(checkpoint_path or CONFIG.get(\"checkpoint_path\"))\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    if not isinstance(checkpoint, dict):\n",
    "        raise RuntimeError(\"Unexpected checkpoint format\")\n",
    "    model_state = checkpoint.get(\"model\")\n",
    "    if model_state is None:\n",
    "        model_state = checkpoint.get(\"state_dict\", checkpoint)\n",
    "    if not isinstance(model_state, dict):\n",
    "        raise RuntimeError(\"Checkpoint is missing a valid model state\")\n",
    "    model_state = dict(model_state)\n",
    "    classifier_weight = model_state.get(\"classifier.weight\")\n",
    "    if classifier_weight is not None:\n",
    "        inferred_classes = int(classifier_weight.shape[0])\n",
    "    else:\n",
    "        inferred_classes = int(checkpoint.get(\"num_class\", CONFIG.get(\"num_class\", 722)))\n",
    "    if inferred_classes <= 0:\n",
    "        inferred_classes = 722\n",
    "    model_params, sequence_model_config = _collect_model_configuration(checkpoint)\n",
    "    model_kwargs: Dict[str, Any] = {}\n",
    "    if sequence_model_config:\n",
    "        model_kwargs[\"sequence_model_config\"] = sequence_model_config\n",
    "    leaky_relu_slope = model_params.get(\"leaky_relu_slope\")\n",
    "    if isinstance(leaky_relu_slope, (int, float)):\n",
    "        model_kwargs[\"leaky_relu_slope\"] = float(leaky_relu_slope)\n",
    "    print(f\"Instantiating JDCNet with {inferred_classes} classes\")\n",
    "    model = JDCNet(num_class=inferred_classes, **model_kwargs)\n",
    "    incompatible = model.load_state_dict(model_state, strict=False)\n",
    "    if incompatible.unexpected_keys:\n",
    "        skipped = \", \".join(sorted(incompatible.unexpected_keys))\n",
    "        print(f\"Warning: skipped unexpected parameters: {skipped}\")\n",
    "    if incompatible.missing_keys:\n",
    "        missing = \", \".join(sorted(incompatible.missing_keys))\n",
    "        print(f\"Warning: missing parameters not found in checkpoint: {missing}\")\n",
    "    model.to(DEVICE).eval()\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23179dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _frequency_grid(min_hz: float, max_hz: float, count: int) -> np.ndarray:\n",
    "    if count <= 1:\n",
    "        return np.array([(min_hz + max_hz) / 2.0], dtype=np.float64)\n",
    "    return np.linspace(min_hz, max_hz, count, dtype=np.float64)\n",
    "\n",
    "\n",
    "RESULTS: List[Dict[str, Any]] = []\n",
    "\n",
    "for range_info in tqdm(CONFIG[\"ranges\"], desc=\"Ranges\"):\n",
    "    name = range_info[\"name\"]\n",
    "    min_hz = float(range_info[\"min_hz\"])\n",
    "    max_hz = float(range_info[\"max_hz\"])\n",
    "    frequencies = _frequency_grid(min_hz, max_hz, CONFIG[\"stimulus\"][\"frequencies_per_range\"])\n",
    "    low_cut = min_hz + (max_hz - min_hz) * CONFIG[\"stimulus\"][\"edge_band_fraction\"]\n",
    "    high_cut = max_hz - (max_hz - min_hz) * CONFIG[\"stimulus\"][\"edge_band_fraction\"]\n",
    "\n",
    "    for frequency in tqdm(frequencies, desc=f\"Frequencies: {name}\", leave=False):\n",
    "        for timbre_name, profile in CONFIG[\"timbre_profiles\"].items():\n",
    "            audio, time_axis = synthesize_timbre_waveform(float(frequency), TARGET_SAMPLE_RATE, CONFIG[\"stimulus\"][\"duration_seconds\"], profile)\n",
    "            prediction = predict_f0(model, audio)\n",
    "            reference_curve = np.full(time_axis.shape[0], float(frequency), dtype=np.float32)\n",
    "            reference = sample_reference_f0(time_axis, reference_curve, prediction.shape[0])\n",
    "            metrics = compute_metrics(reference, prediction)\n",
    "            if frequency <= low_cut:\n",
    "                edge = \"low\"\n",
    "            elif frequency >= high_cut:\n",
    "                edge = \"high\"\n",
    "            else:\n",
    "                edge = \"mid\"\n",
    "            RESULTS.append({\n",
    "                \"range\": name,\n",
    "                \"frequency_hz\": float(frequency),\n",
    "                \"timbre\": timbre_name,\n",
    "                \"edge_region\": edge,\n",
    "                **metrics,\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(RESULTS)\n",
    "results_df.sort_values([\"range\", \"frequency_hz\", \"timbre\"], inplace=True)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ca14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "for (range_name, timbre_name), group in results_df.groupby([\"range\", \"timbre\"]):\n",
    "    axes[0].plot(group[\"frequency_hz\"], group[\"RPA\"], marker=\"o\", label=f\"{range_name} - {timbre_name}\")\n",
    "    axes[1].plot(group[\"frequency_hz\"], group[\"OctaveError\"], marker=\"o\", label=f\"{range_name} - {timbre_name}\")\n",
    "\n",
    "axes[0].set_ylabel(\"RPA\")\n",
    "axes[0].set_title(\"Raw Pitch Accuracy vs. Frequency\")\n",
    "axes[0].set_ylim(0.0, 1.05)\n",
    "\n",
    "axes[1].set_ylabel(\"Octave Error Rate\")\n",
    "axes[1].set_xlabel(\"Fundamental Frequency (Hz)\")\n",
    "axes[1].set_title(\"Octave Confusions vs. Frequency\")\n",
    "axes[1].set_ylim(0.0, 1.0)\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=2, bbox_to_anchor=(0.5, 1.02))\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.98))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_range_metrics = (\n",
    "    results_df.groupby([\"range\"])\n",
    "    .agg({\"RPA\": \"mean\", \"RCA\": \"mean\", \"VUV\": \"mean\", \"OctaveError\": \"mean\"})\n",
    "    .rename(columns={\"RPA\": \"RPA_mean\", \"RCA\": \"RCA_mean\", \"VUV\": \"VUV_mean\", \"OctaveError\": \"OctaveError_mean\"})\n",
    ")\n",
    "per_range_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00786051",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_summary = (\n",
    "    results_df[results_df[\"edge_region\"].isin([\"low\", \"high\"])]\n",
    "    .groupby([\"range\", \"edge_region\"])\n",
    "    .agg({\"OctaveError\": [\"mean\", \"count\"], \"RPA\": \"mean\"})\n",
    ")\n",
    "extreme_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(CONFIG[\"output_dir\"]) / \"pitch_range_timbre_metrics.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved detailed metrics to {output_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}