{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3174d70a",
   "metadata": {},
   "source": [
    "## Multi-task auxiliary objectives update\n",
    "This notebook has been updated to work with the extended multi-task ASR head. You can enable or disable CTC, seq2seq, frame-level phoneme classifiers, speaker embeddings, and pronunciation error classifiers individually through `Configs/config.yml` under the `multi_task` section. Adjust the corresponding `loss_weights` entries when experimenting with these objectives.\n",
    "\n",
    "> **Entropy regularisation**: Use the new `regularization.entropy` section in `Configs/config.yml` to enable or disable per-head entropy penalties. You can independently attach the regulariser to the CTC and seq2seq objectives without breaking joint training.\n",
    "\n",
    "> **Lazy phoneme dictionaries**: Toggle `phoneme_dictionary.lazy_loading` and `phoneme_dictionary.shared_cache` in `Configs/config.yml` to control when the phoneme map is parsed and whether dataloader workers reuse the cached mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and optionally warm the metadata cache for notebook experiments\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    os.chdir('..')\n",
    "\n",
    "_nb_root_dir = os.getcwd()\n",
    "if _nb_root_dir not in sys.path:\n",
    "    sys.path.insert(0, _nb_root_dir)\n",
    "\n",
    "import yaml\n",
    "from train import prepare_data_list\n",
    "from utils import get_data_path_list\n",
    "\n",
    "with open('Configs/config.yml', 'r', encoding='utf-8') as _cfg_file:\n",
    "    _nb_root_config = yaml.safe_load(_cfg_file)\n",
    "\n",
    "_metadata_cache_cfg = (_nb_root_config.get('metadata_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"metadata cache enabled --> {bool(_metadata_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"metadata cache directory --> {_metadata_cache_cfg.get('directory', 'Data/cache')}\")\n",
    "_dataset_toggles = (_metadata_cache_cfg.get('datasets') or {}) if isinstance(_metadata_cache_cfg, dict) else {}\n",
    "_normalised_dataset_toggles = {str(k).lower(): bool(v) for k, v in _dataset_toggles.items()} if isinstance(_dataset_toggles, dict) else {}\n",
    "\n",
    "def _cache_enabled_for(name: str) -> bool:\n",
    "    if not bool(_metadata_cache_cfg.get('enabled', False)):\n",
    "        return False\n",
    "    if not _normalised_dataset_toggles:\n",
    "        return True\n",
    "    return bool(_normalised_dataset_toggles.get(name.lower(), True))\n",
    "\n",
    "(\n",
    "    _train_raw,\n",
    "    _val_raw,\n",
    "    _train_meta_path,\n",
    "    _val_meta_path,\n",
    ") = get_data_path_list(\n",
    "    _nb_root_config.get('train_data'),\n",
    "    _nb_root_config.get('val_data'),\n",
    "    return_paths=True,\n",
    ")\n",
    "\n",
    "if _cache_enabled_for('train'):\n",
    "    _train_entries, _train_durations = prepare_data_list(\n",
    "        _train_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_train_meta_path,\n",
    "        dataset_name='train',\n",
    "    )\n",
    "    print(f\"[cache] training entries available: {len(_train_entries)}\")\n",
    "\n",
    "if _cache_enabled_for('val'):\n",
    "    _val_entries, _val_durations = prepare_data_list(\n",
    "        _val_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_val_meta_path,\n",
    "        dataset_name='val',\n",
    "    )\n",
    "    print(f\"[cache] validation entries available: {len(_val_entries)}\")\n",
    "\n",
    "_ood_metadata_path = _nb_root_config.get('ood_data')\n",
    "if _ood_metadata_path and _cache_enabled_for('ood'):\n",
    "    try:\n",
    "        with open(_ood_metadata_path, 'r', encoding='utf-8') as _ood_file:\n",
    "            _ood_raw = _ood_file.readlines()\n",
    "        _ood_entries, _ood_durations = prepare_data_list(\n",
    "            _ood_raw,\n",
    "            root_path='',\n",
    "            cache_config=_metadata_cache_cfg,\n",
    "            metadata_path=_ood_metadata_path,\n",
    "            dataset_name='ood',\n",
    "        )\n",
    "        print(f\"[cache] OOD entries available: {len(_ood_entries)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[cache] OOD metadata not found at {_ood_metadata_path!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect mel cache configuration for notebook experiments\n",
    "_mel_cache_cfg = (_nb_root_config.get('mel_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"mel cache enabled --> {bool(_mel_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"mel cache directory --> {_mel_cache_cfg.get('directory', 'Data/mel_cache')}\")\n",
    "print(f\"mel cache dtype --> {_mel_cache_cfg.get('dtype', 'float32')}\")\n",
    "_mel_cache_datasets = (_mel_cache_cfg.get('datasets') or {}) if isinstance(_mel_cache_cfg, dict) else {}\n",
    "if isinstance(_mel_cache_datasets, dict) and _mel_cache_datasets:\n",
    "    for _name, _flag in sorted(_mel_cache_datasets.items()):\n",
    "        print(f\"[mel cache] {_name}: {bool(_flag)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188e205",
   "metadata": {},
   "source": [
    "## Memory optimization settings\n",
    "Lazy creation of decoder masks can now be toggled through the new `memory_optimizations.lazy_masks` section in `Configs/config.yml`. When enabled (the default), the trainer skips preallocating the `future_mask` and `text_mask` tensors unless an experiment explicitly needs them. Set the corresponding flags to `false` to restore the previous eager allocation behaviour.\n",
    "\n",
    "Gradient checkpointing for the deeper encoder blocks is configurable through `memory_optimizations.gradient_checkpointing`. Setting `enabled: true` activates `torch.utils.checkpoint.checkpoint_sequential` on the selected layer range so activations are recomputed during backpropagation instead of stored. Tune `start_layer`/`end_layer` to target specific encoder depths, `chunk_size` to limit how many stages are bundled into a checkpoint, and `segments` when you need finer control over how each chunk is split. The helper flags `min_sequence_length` and `use_checkpoint_sequential` let you skip short utterances or fall back to the basic checkpoint API if required.\n",
    "\n",
    "### CTC/seq2seq head sharing\n",
    "Enable `multi_task.head_sharing.ctc_seq2seq.enabled` in `Configs/config.yml` to reuse the intermediate projection computed for the CTC logits when running the seq2seq decoder. With the feature turned on the encoder exposes the shared tensor via `model_outputs[\"ctc_seq2seq_shared_states\"]` and feeds an adapter into the decoder so both heads reuse the same computation.\n",
    "\n",
    "Set the flag back to `false` to restore the previous behaviour. The optional `detach_for_seq2seq` switch stops decoder gradients from flowing through the shared branch if you want to isolate the two objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245669c",
   "metadata": {},
   "source": [
    "# AuxiliaryASR PER Evaluation\n",
    "\n",
    "This notebook loads a trained AuxiliaryASR model, prepares the validation dataset, and computes the phoneme error rate (PER) using greedy CTC decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724345b",
   "metadata": {},
   "source": [
    "## Augmentation configuration\n",
    "This notebook now honours the extended SpecAugment policies, waveform perturbations, mixup, and phoneme-level dropout toggles defined in `Configs/config.yml`. Adjust those settings in the config file before running these evaluation cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available CUDA devices\n",
    "import torch\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        devices.append({\n",
    "            'type': 'CUDA',\n",
    "            'available': True,\n",
    "            'name': device_name,\n",
    "            'index': i\n",
    "        })\n",
    "else:\n",
    "    devices.append({'type': 'CUDA', 'available': False, 'name': 'N/A'})\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a5f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change folder into the root of the ASR project\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    %cd ../\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78689fb8",
   "metadata": {},
   "source": [
    "## Imports and helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages, define helper utilities\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from models import ASRCNN\n",
    "from meldataset import build_dataloader\n",
    "from utils import BatchSizeScheduler\n",
    "from token_map import build_token_map_from_data\n",
    "from text_utils import TextCleaner\n",
    "from utils import build_beam_search_decoder\n",
    "\n",
    "def cfg_get_nested(cfg: dict, path, default=None, sep='.'):\n",
    "    \"\"\"Get a nested value from a dict using a list of keys or a dot-separated string.\"\"\"\n",
    "    if isinstance(path, str):\n",
    "        keys = path.split(sep) if path else []\n",
    "    else:\n",
    "        keys = path\n",
    "\n",
    "    cur = cfg\n",
    "    for k in keys:\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "def load_token_map_from_config(config):\n",
    "    token_src = config.get('phoneme_maps_path')\n",
    "    if not token_src:\n",
    "        return build_token_map_from_data(\n",
    "            config.get('train_data'),\n",
    "            config.get('val_data'),\n",
    "            config.get('ood_data'),\n",
    "            apply_asr_tokenizer=True,\n",
    "        )\n",
    "    if isinstance(token_src, dict):\n",
    "        return token_src\n",
    "    csv = pd.read_csv(token_src, header=None).values\n",
    "    return {word: index for word, index in csv}\n",
    "\n",
    "def load_asr_model(model_path, config_path, device):\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    token_map = load_token_map_from_config(config)\n",
    "\n",
    "    model_params = cfg_get_nested(config, 'model_params', {\n",
    "        'input_dim': 80,\n",
    "        'hidden_dim': 256,\n",
    "        'n_token': len(token_map),\n",
    "        'token_embedding_dim': 512,\n",
    "        'n_layers': 5,\n",
    "        'location_kernel_size': 31,\n",
    "    })\n",
    "    if 'n_token' not in model_params:\n",
    "        model_params['n_token'] = len(token_map)\n",
    "\n",
    "    model_params.setdefault('stabilization_config', cfg_get_nested(config, 'stabilization', {}))\n",
    "\n",
    "    model = ASRCNN(**model_params)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "    state_dict = checkpoint.get('model', checkpoint)\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError:\n",
    "        sanitized_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(sanitized_state)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, config, token_map\n",
    "\n",
    "def build_dev_dataloader(config, device, batch_size=None, num_workers=None):\n",
    "    val_csv_path = config.get('val_data')\n",
    "    if val_csv_path is None:\n",
    "        raise ValueError(\"Validation CSV path ('val_data') not found in the config.\")\n",
    "\n",
    "    with open(val_csv_path, 'r', encoding='utf-8') as f:\n",
    "        raw_lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    path_list = []\n",
    "    for raw in raw_lines:\n",
    "        if not raw.strip():\n",
    "            continue\n",
    "        parts = raw.split('|')\n",
    "        if len(parts) == 1:\n",
    "            continue\n",
    "        path = parts[0]\n",
    "        if len(parts) == 2:\n",
    "            text = parts[1]\n",
    "            speaker = ''\n",
    "        else:\n",
    "            text = '|'.join(parts[1:-1])\n",
    "            speaker = parts[-1]\n",
    "        path_list.append([path, text, speaker])\n",
    "\n",
    "    if batch_size is None:\n",
    "        base_batch_size = int(cfg_get_nested(config, 'eval_params.batch_size', cfg_get_nested(config, 'batch_size', 4)))\n",
    "        curriculum_cfg = cfg_get_nested(config, 'training_curriculum.batch_size_schedule', {}) or {}\n",
    "        scheduler = BatchSizeScheduler(curriculum_cfg, default_batch_size=base_batch_size, total_epochs=int(cfg_get_nested(config, 'epochs', 1)))\n",
    "        if scheduler.enabled:\n",
    "            eval_epoch = cfg_get_nested(curriculum_cfg, 'evaluation_epoch', None)\n",
    "            if eval_epoch is None:\n",
    "                eval_epoch = scheduler.evaluation_epoch if scheduler.evaluation_epoch is not None else scheduler.total_epochs\n",
    "            eval_epoch = max(1, min(int(eval_epoch), scheduler.total_epochs))\n",
    "            batch_size = scheduler.batch_size_for_epoch(eval_epoch)\n",
    "        else:\n",
    "            batch_size = base_batch_size\n",
    "    if num_workers is None:\n",
    "        num_workers = int(cfg_get_nested(config, 'dataloader_params.val_num_workers', 2))\n",
    "\n",
    "    dataset_params = resolve_dataset_params(config)\n",
    "\n",
    "    collate_config = {'return_wave': False}\n",
    "    device_flag = device.type if isinstance(device, torch.device) else str(device)\n",
    "    loader = build_dataloader(\n",
    "        path_list=path_list,\n",
    "        validation=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        device=device_flag,\n",
    "        collate_config=collate_config,\n",
    "        dataset_config=dataset_params,\n",
    "        dataset_name=\"val\",\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dataset_params(config, base_overrides=None):\n",
    "    dataset_params = {\n",
    "        'dict_path': cfg_get_nested(config, 'phoneme_maps_path', 'Data/word_index_dict.txt'),\n",
    "        'sr': cfg_get_nested(config, 'preprocess_params.sr', cfg_get_nested(config, 'preprocess_parasm.sr', 24000)),\n",
    "        'spect_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.spect_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.spect_params', {'n_fft': 1024, 'win_length': 1024, 'hop_length': 300}),\n",
    "        ),\n",
    "        'mel_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.mel_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.mel_params', {'n_mels': 80}),\n",
    "        ),\n",
    "    }\n",
    "    dataset_params['mel_cache'] = cfg_get_nested(config, 'mel_cache', {}) or {}\n",
    "    dataset_params['phoneme_dictionary_config'] = cfg_get_nested(config, 'phoneme_dictionary', {}) or {}\n",
    "    dataset_overrides = cfg_get_nested(config, 'dataset_params', {})\n",
    "    if isinstance(dataset_overrides, dict):\n",
    "        for key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config'):\n",
    "            if key in dataset_overrides:\n",
    "                dataset_params[key] = dataset_overrides[key]\n",
    "        if 'spec_augment' in dataset_overrides:\n",
    "            dataset_params['spec_augment_params'] = dataset_overrides['spec_augment']\n",
    "        for key, value in dataset_overrides.items():\n",
    "            if key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config', 'spec_augment'):\n",
    "                continue\n",
    "            dataset_params[key] = value\n",
    "    if base_overrides:\n",
    "        dataset_params.update(base_overrides)\n",
    "    return dataset_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d09a2b",
   "metadata": {},
   "source": [
    "## Load model, configuration, and validation loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'Checkpoint'\n",
    "config_path = 'Checkpoint/config.yml'\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    raise FileNotFoundError(f\"Checkpoint directory '{checkpoint_dir}' not found.\")\n",
    "\n",
    "ckpt_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(f\"No checkpoint files found in '{checkpoint_dir}'.\")\n",
    "\n",
    "ckpt_files = sorted(ckpt_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "model_path = os.path.join(checkpoint_dir, ckpt_files[-1])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, config, token_map = load_asr_model(model_path, config_path, device)\n",
    "\n",
    "print(f'model --> {model_path}')\n",
    "print(f'config --> {config_path}')\n",
    "phoneme_source = config.get('phoneme_maps_path', 'built from dataset')\n",
    "print(f'dictionary --> {phoneme_source}')\n",
    "\n",
    "dev_loader = build_dev_dataloader(config, device)\n",
    "print(f'Validation dataset size: {len(dev_loader.dataset)} samples')\n",
    "\n",
    "vocab = token_map\n",
    "if ' ' not in vocab:\n",
    "    raise KeyError(\"The vocabulary does not contain the blank symbol ' '.\")\n",
    "BLANK_ID = vocab[' ']\n",
    "ID2PH = {idx: symbol for symbol, idx in vocab.items()}\n",
    "print(f'Blank token id: {BLANK_ID}')\n",
    "\n",
    "\n",
    "decoder = build_beam_search_decoder(config, vocab_size=len(vocab))\n",
    "if decoder is None:\n",
    "    print('Decoding strategy: CTC greedy')\n",
    "else:\n",
    "    fusion_flags = []\n",
    "    if decoder.shallow_fusion_lm is not None:\n",
    "        fusion_flags.append('shallow fusion LM')\n",
    "    if decoder.cold_fusion_lm is not None:\n",
    "        fusion_flags.append('cold fusion LM')\n",
    "    fusion_desc = ' with ' + ' and '.join(fusion_flags) if fusion_flags else ''\n",
    "    print(f'Decoding strategy: Beam search (beam={decoder.beam_width}){fusion_desc}')\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3377325",
   "metadata": {},
   "source": [
    "## Greedy CTC decoding and PER computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9781d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding utilities and PER evaluation\n",
    "from typing import List, Sequence, Optional\n",
    "\n",
    "\n",
    "def ctc_decode(logits: torch.Tensor, lens: torch.Tensor, decoder: Optional[object] = None) -> List[List[int]]:\n",
    "    \"\"\"Decode logits either with beam search or greedy collapse.\"\"\"\n",
    "    if logits.dim() != 3:\n",
    "        raise ValueError(f\"Expected logits of shape (B, T, V), got {tuple(logits.shape)}\")\n",
    "\n",
    "    if decoder is None:\n",
    "        pred_ids = logits.argmax(-1)\n",
    "        hyps: List[List[int]] = []\n",
    "        for b in range(pred_ids.size(0)):\n",
    "            prev = BLANK_ID\n",
    "            out: List[int] = []\n",
    "            T = int(lens[b])\n",
    "            for t in range(T):\n",
    "                p = int(pred_ids[b, t])\n",
    "                if p != BLANK_ID and p != prev:\n",
    "                    out.append(p)\n",
    "                prev = p\n",
    "            hyps.append(out)\n",
    "        return hyps\n",
    "\n",
    "    return decoder.decode(logits, lens)\n",
    "\n",
    "\n",
    "def edit_distance(a: Sequence[int], b: Sequence[int]) -> int:\n",
    "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i in range(len(a) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b) + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, len(a) + 1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,\n",
    "                dp[i][j - 1] + 1,\n",
    "                dp[i - 1][j - 1] + (a[i - 1] != b[j - 1]),\n",
    "            )\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_per(model: torch.nn.Module, dev_loader, device=None, max_examples: int = 5, decoder: Optional[object] = None):\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    tot_err, tot_len = 0, 0\n",
    "    phoneme_freq: Counter = Counter()\n",
    "    examples = []\n",
    "    downsample_factor = 2 ** getattr(model, 'n_down', 1)\n",
    "\n",
    "    for batch in dev_loader:\n",
    "        texts, text_lens, mels, mel_lens = batch[:4]\n",
    "        mels = mels.to(device)\n",
    "        text_lens = text_lens.to(torch.long)\n",
    "        mel_lens = mel_lens.to(torch.long)\n",
    "\n",
    "        outputs = model(mels)\n",
    "        if isinstance(outputs, dict):\n",
    "            logits = outputs.get('logits_ctc')\n",
    "            if logits is None:\n",
    "                logits = outputs.get('ctc_logits')\n",
    "            if logits is None:\n",
    "                logits = outputs.get('primary_logits')\n",
    "            if logits is None:\n",
    "                raise KeyError(\"Model output dict does not contain CTC logits.\")\n",
    "        elif isinstance(outputs, (tuple, list)):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        if logits.dim() != 3:\n",
    "            raise ValueError(f\"Unexpected logits shape: {tuple(logits.shape)}\")\n",
    "\n",
    "        logit_lens = torch.clamp(mel_lens // downsample_factor, min=1, max=logits.size(1))\n",
    "        hyps = ctc_decode(logits.cpu(), logit_lens.cpu(), decoder)\n",
    "\n",
    "        for hyp, tgt, tgt_len in zip(hyps, texts, text_lens):\n",
    "            effective_len = int(tgt_len)\n",
    "            tgt_ids = tgt[:effective_len].tolist()\n",
    "            tgt_ids = [idx for idx in tgt_ids if idx != BLANK_ID]\n",
    "\n",
    "            phoneme_freq.update(tgt_ids)\n",
    "            tot_err += edit_distance(hyp, tgt_ids)\n",
    "            tot_len += len(tgt_ids)\n",
    "\n",
    "            if len(examples) < max_examples:\n",
    "                examples.append({\n",
    "                    'prediction': hyp,\n",
    "                    'reference': tgt_ids,\n",
    "                })\n",
    "\n",
    "    per = tot_err / max(1, tot_len)\n",
    "    stats = {\n",
    "        'total_errors': tot_err,\n",
    "        'total_phonemes': tot_len,\n",
    "        'phoneme_frequency': phoneme_freq,\n",
    "        'examples': examples,\n",
    "    }\n",
    "    return per, stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c8b16",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f81677",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_mode = 'beam search' if decoder is not None else 'CTC greedy'\n",
    "per, per_stats = eval_per(model, dev_loader, device=device, max_examples=5, decoder=decoder)\n",
    "print(f'Dev PER ({decode_mode}): {per:.3f}')\n",
    "print(f'Total phonemes evaluated: {per_stats[\"total_phonemes\"]}')\n",
    "print(f'Total edit distance: {per_stats[\"total_errors\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4d90d",
   "metadata": {},
   "source": [
    "## Inspect sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33411e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_symbols(ids):\n",
    "    return ' '.join(ID2PH.get(i, f\"<unk:{i}>\") for i in ids)\n",
    "\n",
    "for idx, example in enumerate(per_stats[\"examples\"], 1):\n",
    "    print(f'Sample {idx}')\n",
    "    print('Reference :', ids_to_symbols(example['reference']))\n",
    "    print('Prediction:', ids_to_symbols(example['prediction']))\n",
    "    print('-' * 60)\n",
    "    if idx >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most common phonemes in the validation references:')\n",
    "for phoneme_id, count in per_stats[\"phoneme_frequency\"].most_common(10):\n",
    "    symbol = ID2PH.get(phoneme_id, phoneme_id)\n",
    "    print(f'{symbol}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cb10a",
   "metadata": {},
   "source": [
    "## Self-Conditioned CTC Support\n",
    "\n",
    "This notebook has been updated to work with the optional self-conditioned CTC feature. To experiment with it, enable the feature in `Configs/config.yml` by setting:\n",
    "\n",
    "```yaml\n",
    "stabilization:\n",
    "  self_conditioned_ctc:\n",
    "    enabled: true\n",
    "    layers:\n",
    "      - index: 2\n",
    "      - index: 4\n",
    "    conditioning_strategy: add  # or concat\n",
    "    detach_conditioning: true\n",
    "loss_weights:\n",
    "  self_conditioned_ctc: 0.2\n",
    "```\n",
    "\n",
    "With the feature enabled the trainer will expose `self_conditioned_ctc_logits` and `self_conditioned_ctc_log_probs` in the model outputs so they can be visualised or scored inside the notebook just like the existing CTC signals.\n",
    "\n",
    "```yaml\n",
    "regularization:\n",
    "  entropy:\n",
    "    enabled: false\n",
    "    mode: minimize  # set to \"maximize\" to encourage smoother distributions\n",
    "    eps: 1.0e-6\n",
    "    targets:\n",
    "      ctc:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "      s2s:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def _resolve_mixed_precision_from_config(_cfg):\n",
    "    if 'cfg_get_nested' in globals():\n",
    "        return cfg_get_nested(_cfg, 'precision.mixed_precision', {})\n",
    "    precision_cfg = _cfg.get('precision', {}) if isinstance(_cfg, dict) else {}\n",
    "    return precision_cfg.get('mixed_precision', {}) if isinstance(precision_cfg, dict) else {}\n",
    "\n",
    "if 'config' in globals():\n",
    "    _mp_root_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_root_config = yaml.safe_load(_f)\n",
    "\n",
    "mixed_precision_cfg = _resolve_mixed_precision_from_config(_mp_root_config) or {}\n",
    "grad_scaler_cfg = mixed_precision_cfg.get('grad_scaler', {}) if isinstance(mixed_precision_cfg, dict) else {}\n",
    "print(f\"mixed precision enabled --> {bool(mixed_precision_cfg.get('enabled', False))}\")\n",
    "print(f\"mixed precision dtype --> {mixed_precision_cfg.get('dtype', 'float16')}\")\n",
    "print(f\"grad scaler enabled --> {bool(grad_scaler_cfg.get('enabled', True))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
