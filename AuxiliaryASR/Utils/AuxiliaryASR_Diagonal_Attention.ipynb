{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e477161c",
   "metadata": {},
   "source": [
    "## Multi-task auxiliary objectives update\n",
    "This notebook has been updated to work with the extended multi-task ASR head. You can enable or disable CTC, seq2seq, frame-level phoneme classifiers, speaker embeddings, and pronunciation error classifiers individually through `Configs/config.yml` under the `multi_task` section. Adjust the corresponding `loss_weights` entries when experimenting with these objectives.\n",
    "\n",
    "> **Entropy regularisation**: Use the new `regularization.entropy` section in `Configs/config.yml` to enable or disable per-head entropy penalties. You can independently attach the regulariser to the CTC and seq2seq objectives without breaking joint training.\n",
    "\n",
    "> **Lazy phoneme dictionaries**: Toggle `phoneme_dictionary.lazy_loading` and `phoneme_dictionary.shared_cache` in `Configs/config.yml` to control when the phoneme map is parsed and whether dataloader workers reuse the cached mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511037db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and optionally warm the metadata cache for notebook experiments\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    os.chdir('..')\n",
    "\n",
    "_nb_root_dir = os.getcwd()\n",
    "if _nb_root_dir not in sys.path:\n",
    "    sys.path.insert(0, _nb_root_dir)\n",
    "\n",
    "import yaml\n",
    "from train import prepare_data_list\n",
    "from models import ASRCNN\n",
    "from utils import get_data_path_list, load_asr_model_from_config, build_dev_dataloader_from_config\n",
    "\n",
    "with open('Configs/config.yml', 'r', encoding='utf-8') as _cfg_file:\n",
    "    _nb_root_config = yaml.safe_load(_cfg_file)\n",
    "\n",
    "_metadata_cache_cfg = (_nb_root_config.get('metadata_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"metadata cache enabled --> {bool(_metadata_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"metadata cache directory --> {_metadata_cache_cfg.get('directory', 'Data/cache')}\")\n",
    "_dataset_toggles = (_metadata_cache_cfg.get('datasets') or {}) if isinstance(_metadata_cache_cfg, dict) else {}\n",
    "_normalised_dataset_toggles = {str(k).lower(): bool(v) for k, v in _dataset_toggles.items()} if isinstance(_dataset_toggles, dict) else {}\n",
    "\n",
    "def _cache_enabled_for(name: str) -> bool:\n",
    "    if not bool(_metadata_cache_cfg.get('enabled', False)):\n",
    "        return False\n",
    "    if not _normalised_dataset_toggles:\n",
    "        return True\n",
    "    return bool(_normalised_dataset_toggles.get(name.lower(), True))\n",
    "\n",
    "(\n",
    "    _train_raw,\n",
    "    _val_raw,\n",
    "    _train_meta_path,\n",
    "    _val_meta_path,\n",
    ") = get_data_path_list(\n",
    "    _nb_root_config.get('train_data'),\n",
    "    _nb_root_config.get('val_data'),\n",
    "    return_paths=True,\n",
    ")\n",
    "\n",
    "if _cache_enabled_for('train'):\n",
    "    _train_entries, _train_durations = prepare_data_list(\n",
    "        _train_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_train_meta_path,\n",
    "        dataset_name='train',\n",
    "    )\n",
    "    print(f\"[cache] training entries available: {len(_train_entries)}\")\n",
    "\n",
    "if _cache_enabled_for('val'):\n",
    "    _val_entries, _val_durations = prepare_data_list(\n",
    "        _val_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_val_meta_path,\n",
    "        dataset_name='val',\n",
    "    )\n",
    "    print(f\"[cache] validation entries available: {len(_val_entries)}\")\n",
    "\n",
    "_ood_metadata_path = _nb_root_config.get('ood_data')\n",
    "if _ood_metadata_path and _cache_enabled_for('ood'):\n",
    "    try:\n",
    "        with open(_ood_metadata_path, 'r', encoding='utf-8') as _ood_file:\n",
    "            _ood_raw = _ood_file.readlines()\n",
    "        _ood_entries, _ood_durations = prepare_data_list(\n",
    "            _ood_raw,\n",
    "            root_path='',\n",
    "            cache_config=_metadata_cache_cfg,\n",
    "            metadata_path=_ood_metadata_path,\n",
    "            dataset_name='ood',\n",
    "        )\n",
    "        print(f\"[cache] OOD entries available: {len(_ood_entries)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[cache] OOD metadata not found at {_ood_metadata_path!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9280aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect mel cache configuration for notebook experiments\n",
    "_mel_cache_cfg = (_nb_root_config.get('mel_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"mel cache enabled --> {bool(_mel_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"mel cache directory --> {_mel_cache_cfg.get('directory', 'Data/mel_cache')}\")\n",
    "print(f\"mel cache dtype --> {_mel_cache_cfg.get('dtype', 'float32')}\")\n",
    "_mel_cache_datasets = (_mel_cache_cfg.get('datasets') or {}) if isinstance(_mel_cache_cfg, dict) else {}\n",
    "if isinstance(_mel_cache_datasets, dict) and _mel_cache_datasets:\n",
    "    for _name, _flag in sorted(_mel_cache_datasets.items()):\n",
    "        print(f\"[mel cache] {_name}: {bool(_flag)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158ac40",
   "metadata": {},
   "source": [
    "## Memory optimization settings\n",
    "Lazy creation of decoder masks can now be toggled through the new `memory_optimizations.lazy_masks` section in `Configs/config.yml`. When enabled (the default), the trainer skips preallocating the `future_mask` and `text_mask` tensors unless an experiment explicitly needs them. Set the corresponding flags to `false` to restore the previous eager allocation behaviour.\n",
    "\n",
    "Gradient checkpointing for the deeper encoder blocks is configurable through `memory_optimizations.gradient_checkpointing`. Setting `enabled: true` activates `torch.utils.checkpoint.checkpoint_sequential` on the selected layer range so activations are recomputed during backpropagation instead of stored. Tune `start_layer`/`end_layer` to target specific encoder depths, `chunk_size` to limit how many stages are bundled into a checkpoint, and `segments` when you need finer control over how each chunk is split. The helper flags `min_sequence_length` and `use_checkpoint_sequential` let you skip short utterances or fall back to the basic checkpoint API if required.\n",
    "\n",
    "### CTC/seq2seq head sharing\n",
    "Enable `multi_task.head_sharing.ctc_seq2seq.enabled` in `Configs/config.yml` to reuse the intermediate projection computed for the CTC logits when running the seq2seq decoder. With the feature turned on the encoder exposes the shared tensor via `model_outputs[\"ctc_seq2seq_shared_states\"]` and feeds an adapter into the decoder so both heads reuse the same computation.\n",
    "\n",
    "Set the flag back to `false` to restore the previous behaviour. The optional `detach_for_seq2seq` switch stops decoder gradients from flowing through the shared branch if you want to isolate the two objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5b82",
   "metadata": {},
   "source": [
    "# Auxiliary ASR Diagonal Attention Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5bd8a",
   "metadata": {},
   "source": [
    "## Augmentation configuration\n",
    "This notebook now honours the extended SpecAugment policies, waveform perturbations, mixup, and phoneme-level dropout toggles defined in `Configs/config.yml`. Adjust those settings in the config file before running these evaluation cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check available CUDA devices\n",
    "import torch\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        devices.append({\n",
    "            'type': 'CUDA',\n",
    "            'available': True,\n",
    "            'name': device_name,\n",
    "            'index': i\n",
    "        })\n",
    "else:\n",
    "    devices.append({'type': 'CUDA', 'available': False, 'name': 'N/A'})\n",
    "devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change folder into the root of the ASR project\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    %cd ../\n",
    "\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23302d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dataset_params(config, base_overrides=None):\n",
    "    dataset_params = {\n",
    "        'dict_path': cfg_get_nested(config, 'phoneme_maps_path', 'Data/word_index_dict.txt'),\n",
    "        'sr': cfg_get_nested(config, 'preprocess_params.sr', cfg_get_nested(config, 'preprocess_parasm.sr', 24000)),\n",
    "        'spect_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.spect_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.spect_params', {'n_fft': 1024, 'win_length': 1024, 'hop_length': 300}),\n",
    "        ),\n",
    "        'mel_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.mel_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.mel_params', {'n_mels': 80}),\n",
    "        ),\n",
    "    }\n",
    "    dataset_params['mel_cache'] = cfg_get_nested(config, 'mel_cache', {}) or {}\n",
    "    dataset_params['phoneme_dictionary_config'] = cfg_get_nested(config, 'phoneme_dictionary', {}) or {}\n",
    "    dataset_overrides = cfg_get_nested(config, 'dataset_params', {})\n",
    "    if isinstance(dataset_overrides, dict):\n",
    "        for key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config'):\n",
    "            if key in dataset_overrides:\n",
    "                dataset_params[key] = dataset_overrides[key]\n",
    "        if 'spec_augment' in dataset_overrides:\n",
    "            dataset_params['spec_augment_params'] = dataset_overrides['spec_augment']\n",
    "        for key, value in dataset_overrides.items():\n",
    "            if key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config', 'spec_augment'):\n",
    "                continue\n",
    "            dataset_params[key] = value\n",
    "    if base_overrides:\n",
    "        dataset_params.update(base_overrides)\n",
    "    return dataset_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b023338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, config, and validation dataloader\n",
    "checkpoint_dir = 'Checkpoint'\n",
    "config_path = 'Checkpoint/config.yml'\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    raise FileNotFoundError(f\"Checkpoint directory '{checkpoint_dir}' not found.\")\n",
    "\n",
    "ckpt_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(f\"No checkpoint files found in '{checkpoint_dir}'.\")\n",
    "\n",
    "ckpt_files = sorted(ckpt_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "model_path = os.path.join(checkpoint_dir, ckpt_files[-1])\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f'model --> {model_path}')\n",
    "print(f'config --> {config_path}')\n",
    "phoneme_source = config.get('phoneme_maps_path', 'built from dataset')\n",
    "print(f'dictionary --> {phoneme_source}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, token_map = load_asr_model_from_config(config, model_path, device)\n",
    "\n",
    "dev_loader, val_entries = build_dev_dataloader_from_config(config, device)\n",
    "print(f'Validation dataset size: {len(val_entries)} samples')\n",
    "print(f'Batch size --> {dev_loader.batch_size}')\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention alignment diagnostics\n",
    "import numpy as np\n",
    "from typing import List, Optional\n",
    "\n",
    "@torch.no_grad()\n",
    "def diagonal_attention_score(model: ASRCNN, dev_loader, device: Optional[torch.device] = None, band: float = 0.1, max_batches: Optional[int] = None):\n",
    "    \"\"\"Compute the average diagonal attention concentration score.\n\n",
    "    Args:\n",
    "        model: ASR model returning alignment matrices when teacher-forced.\n",
    "        dev_loader: validation dataloader yielding (texts, text_lens, mels, mel_lens).\n",
    "        device: device for computation; defaults to model parameters' device.\n",
    "        band: allowable deviation from the diagonal (0-1 range).\n",
    "        max_batches: limit the number of batches to evaluate (useful for quick checks).\n\n",
    "    Returns:\n",
    "        mean_score: average ratio of attention mass inside the diagonal band.\n",
    "        scores: list of per-utterance scores.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n\n",
    "    diag_scores: List[float] = []\n",
    "    downsample = 2 ** getattr(model, 'n_down', 1)\n\n",
    "    for batch_idx, batch in enumerate(dev_loader):\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n\n",
    "        texts, text_lens, mels, mel_lens = batch[:4]\n",
    "        mels = mels.to(device)\n",
    "        texts = texts.to(device)\n",
    "        text_lens = text_lens.to(torch.long)\n",
    "        mel_lens = mel_lens.to(device=device, dtype=torch.long)\n\n",
    "        reduced_mel_lens = torch.clamp(mel_lens // downsample, min=1)\n",
    "        mel_mask = model.length_to_mask(reduced_mel_lens)\n\n",
    "        outputs = model(mels, src_key_padding_mask=mel_mask, text_input=texts)\n",
    "        attn = None\n",
    "        if isinstance(outputs, dict):\n",
    "            for key in ('s2s_attn', 'attn', 'attention', 'alignments'):\n",
    "                tensor = outputs.get(key)\n",
    "                if tensor is not None:\n",
    "                    attn = tensor\n",
    "                    break\n",
    "            if attn is None:\n",
    "                available = ', '.join(outputs.keys())\n",
    "                raise KeyError(f'Model output dictionary does not contain attention matrices. Available keys: {available}')\n",
    "        elif isinstance(outputs, (tuple, list)):\n",
    "            if len(outputs) < 3:\n",
    "                raise ValueError('Model forward output does not include attention tensors.')\n",
    "            attn = outputs[2]\n",
    "        else:\n",
    "            raise TypeError('Unsupported model output type for attention extraction.')\n\n",
    "        attn = attn.detach()\n",
    "        time_axis = attn.size(-1)\n",
    "        output_axis = attn.size(1)\n\n",
    "        text_lens_list = text_lens.tolist()\n",
    "        mel_lens_list = reduced_mel_lens.tolist()\n\n",
    "        for b in range(attn.size(0)):\n",
    "            To = min(int(text_lens_list[b]), output_axis)\n",
    "            Te = min(int(mel_lens_list[b]), time_axis)\n",
    "            if To <= 1 or Te <= 1:\n",
    "                continue\n\n",
    "            a = attn[b, :To, :Te]\n",
    "            total_mass = a.sum()\n",
    "            if torch.isclose(total_mass, torch.tensor(0.0, device=a.device)):\n",
    "                continue\n\n",
    "            t = torch.arange(To, device=a.device, dtype=torch.float32).unsqueeze(1)\n",
    "            e = torch.arange(Te, device=a.device, dtype=torch.float32).unsqueeze(0)\n",
    "            t = t / (To - 1) if To > 1 else torch.zeros_like(t)\n",
    "            e = e / (Te - 1) if Te > 1 else torch.zeros_like(e)\n",
    "            diag = t - e\n",
    "            mask = (diag.abs() <= band).to(a.dtype)\n\n",
    "            score = (a * mask).sum() / total_mass.clamp_min(1e-8)\n",
    "            diag_scores.append(float(score))\n\n",
    "    mean_score = float(np.mean(diag_scores)) if diag_scores else float('nan')\n",
    "    return mean_score, diag_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844e43c-4d2f-4e1e-bae0-f11dd1a1e6a2",
   "metadata": {},
   "source": [
    "### Diagonal Attention Score\n",
    "- scores > 0.6-0.7 are usually good; trending higher across training is a healthy sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a91005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the diagonal attention score\n",
    "mean_score, scores = diagonal_attention_score(model, dev_loader, device=device, band=0.1, max_batches=None)\n",
    "print(f'Diagonal attention score: {mean_score:.4f}')\n",
    "print(f'Evaluated {len(scores)} alignments')\n",
    "\n",
    "if scores:\n",
    "    scores_array = np.array(scores)\n",
    "    print('Score statistics:')\n",
    "    print(f'  min:  {scores_array.min():.4f}')\n",
    "    print(f'  25%:  {np.percentile(scores_array, 25):.4f}')\n",
    "    print(f'  median: {np.median(scores_array):.4f}')\n",
    "    print(f'  75%:  {np.percentile(scores_array, 75):.4f}')\n",
    "    print(f'  max:  {scores_array.max():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae23f0",
   "metadata": {},
   "source": [
    "## Self-Conditioned CTC Support\n",
    "\n",
    "This notebook has been updated to work with the optional self-conditioned CTC feature. To experiment with it, enable the feature in `Configs/config.yml` by setting:\n",
    "\n",
    "```yaml\n",
    "stabilization:\n",
    "  self_conditioned_ctc:\n",
    "    enabled: true\n",
    "    layers:\n",
    "      - index: 2\n",
    "      - index: 4\n",
    "    conditioning_strategy: add  # or concat\n",
    "    detach_conditioning: true\n",
    "loss_weights:\n",
    "  self_conditioned_ctc: 0.2\n",
    "```\n",
    "\n",
    "With the feature enabled the trainer will expose `self_conditioned_ctc_logits` and `self_conditioned_ctc_log_probs` in the model outputs so they can be visualised or scored inside the notebook just like the existing CTC signals.\n",
    "\n",
    "```yaml\n",
    "regularization:\n",
    "  entropy:\n",
    "    enabled: false\n",
    "    mode: minimize  # set to \"maximize\" to encourage smoother distributions\n",
    "    eps: 1.0e-6\n",
    "    targets:\n",
    "      ctc:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "      s2s:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def _resolve_mixed_precision_from_config(_cfg):\n",
    "    if 'cfg_get_nested' in globals():\n",
    "        return cfg_get_nested(_cfg, 'precision.mixed_precision', {})\n",
    "    precision_cfg = _cfg.get('precision', {}) if isinstance(_cfg, dict) else {}\n",
    "    return precision_cfg.get('mixed_precision', {}) if isinstance(precision_cfg, dict) else {}\n",
    "\n",
    "if 'config' in globals():\n",
    "    _mp_root_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_root_config = yaml.safe_load(_f)\n",
    "\n",
    "mixed_precision_cfg = _resolve_mixed_precision_from_config(_mp_root_config) or {}\n",
    "grad_scaler_cfg = mixed_precision_cfg.get('grad_scaler', {}) if isinstance(mixed_precision_cfg, dict) else {}\n",
    "print(f\"mixed precision enabled --> {bool(mixed_precision_cfg.get('enabled', False))}\")\n",
    "print(f\"mixed precision dtype --> {mixed_precision_cfg.get('dtype', 'float16')}\")\n",
    "print(f\"grad scaler enabled --> {bool(grad_scaler_cfg.get('enabled', True))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
