{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e96737e",
   "metadata": {},
   "source": [
    "## Multi-task auxiliary objectives update\n",
    "This notebook has been updated to work with the extended multi-task ASR head. You can enable or disable CTC, seq2seq, frame-level phoneme classifiers, speaker embeddings, and pronunciation error classifiers individually through `Configs/config.yml` under the `multi_task` section. Adjust the corresponding `loss_weights` entries when experimenting with these objectives.\n",
    "\n",
    "> **Entropy regularisation**: Use the new `regularization.entropy` section in `Configs/config.yml` to enable or disable per-head entropy penalties. You can independently attach the regulariser to the CTC and seq2seq objectives without breaking joint training.\n",
    "\n",
    "> **Lazy phoneme dictionaries**: Toggle `phoneme_dictionary.lazy_loading` and `phoneme_dictionary.shared_cache` in `Configs/config.yml` to control when the phoneme map is parsed and whether dataloader workers reuse the cached mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030157ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and optionally warm the metadata cache for notebook experiments\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    os.chdir('..')\n",
    "\n",
    "_nb_root_dir = os.getcwd()\n",
    "if _nb_root_dir not in sys.path:\n",
    "    sys.path.insert(0, _nb_root_dir)\n",
    "\n",
    "import yaml\n",
    "from train import prepare_data_list\n",
    "from utils import get_data_path_list\n",
    "\n",
    "with open('Configs/config.yml', 'r', encoding='utf-8') as _cfg_file:\n",
    "    _nb_root_config = yaml.safe_load(_cfg_file)\n",
    "\n",
    "_metadata_cache_cfg = (_nb_root_config.get('metadata_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"metadata cache enabled --> {bool(_metadata_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"metadata cache directory --> {_metadata_cache_cfg.get('directory', 'Data/cache')}\")\n",
    "_dataset_toggles = (_metadata_cache_cfg.get('datasets') or {}) if isinstance(_metadata_cache_cfg, dict) else {}\n",
    "_normalised_dataset_toggles = {str(k).lower(): bool(v) for k, v in _dataset_toggles.items()} if isinstance(_dataset_toggles, dict) else {}\n",
    "\n",
    "def _cache_enabled_for(name: str) -> bool:\n",
    "    if not bool(_metadata_cache_cfg.get('enabled', False)):\n",
    "        return False\n",
    "    if not _normalised_dataset_toggles:\n",
    "        return True\n",
    "    return bool(_normalised_dataset_toggles.get(name.lower(), True))\n",
    "\n",
    "(\n",
    "    _train_raw,\n",
    "    _val_raw,\n",
    "    _train_meta_path,\n",
    "    _val_meta_path,\n",
    ") = get_data_path_list(\n",
    "    _nb_root_config.get('train_data'),\n",
    "    _nb_root_config.get('val_data'),\n",
    "    return_paths=True,\n",
    ")\n",
    "\n",
    "if _cache_enabled_for('train'):\n",
    "    _train_entries, _train_durations = prepare_data_list(\n",
    "        _train_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_train_meta_path,\n",
    "        dataset_name='train',\n",
    "    )\n",
    "    print(f\"[cache] training entries available: {len(_train_entries)}\")\n",
    "\n",
    "if _cache_enabled_for('val'):\n",
    "    _val_entries, _val_durations = prepare_data_list(\n",
    "        _val_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_val_meta_path,\n",
    "        dataset_name='val',\n",
    "    )\n",
    "    print(f\"[cache] validation entries available: {len(_val_entries)}\")\n",
    "\n",
    "_ood_metadata_path = _nb_root_config.get('ood_data')\n",
    "if _ood_metadata_path and _cache_enabled_for('ood'):\n",
    "    try:\n",
    "        with open(_ood_metadata_path, 'r', encoding='utf-8') as _ood_file:\n",
    "            _ood_raw = _ood_file.readlines()\n",
    "        _ood_entries, _ood_durations = prepare_data_list(\n",
    "            _ood_raw,\n",
    "            root_path='',\n",
    "            cache_config=_metadata_cache_cfg,\n",
    "            metadata_path=_ood_metadata_path,\n",
    "            dataset_name='ood',\n",
    "        )\n",
    "        print(f\"[cache] OOD entries available: {len(_ood_entries)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[cache] OOD metadata not found at {_ood_metadata_path!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect mel cache configuration for notebook experiments\n",
    "_mel_cache_cfg = (_nb_root_config.get('mel_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"mel cache enabled --> {bool(_mel_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"mel cache directory --> {_mel_cache_cfg.get('directory', 'Data/mel_cache')}\")\n",
    "print(f\"mel cache dtype --> {_mel_cache_cfg.get('dtype', 'float32')}\")\n",
    "_mel_cache_datasets = (_mel_cache_cfg.get('datasets') or {}) if isinstance(_mel_cache_cfg, dict) else {}\n",
    "if isinstance(_mel_cache_datasets, dict) and _mel_cache_datasets:\n",
    "    for _name, _flag in sorted(_mel_cache_datasets.items()):\n",
    "        print(f\"[mel cache] {_name}: {bool(_flag)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be174965",
   "metadata": {},
   "source": [
    "## Memory optimization settings\n",
    "Lazy creation of decoder masks can now be toggled through the new `memory_optimizations.lazy_masks` section in `Configs/config.yml`. When enabled (the default), the trainer skips preallocating the `future_mask` and `text_mask` tensors unless an experiment explicitly needs them. Set the corresponding flags to `false` to restore the previous eager allocation behaviour.\n",
    "\n",
    "Gradient checkpointing for the deeper encoder blocks is configurable through `memory_optimizations.gradient_checkpointing`. Setting `enabled: true` activates `torch.utils.checkpoint.checkpoint_sequential` on the selected layer range so activations are recomputed during backpropagation instead of stored. Tune `start_layer`/`end_layer` to target specific encoder depths, `chunk_size` to limit how many stages are bundled into a checkpoint, and `segments` when you need finer control over how each chunk is split. The helper flags `min_sequence_length` and `use_checkpoint_sequential` let you skip short utterances or fall back to the basic checkpoint API if required.\n",
    "\n",
    "### CTC/seq2seq head sharing\n",
    "Enable `multi_task.head_sharing.ctc_seq2seq.enabled` in `Configs/config.yml` to reuse the intermediate projection computed for the CTC logits when running the seq2seq decoder. With the feature turned on the encoder exposes the shared tensor via `model_outputs[\"ctc_seq2seq_shared_states\"]` and feeds an adapter into the decoder so both heads reuse the same computation.\n",
    "\n",
    "Set the flag back to `false` to restore the previous behaviour. The optional `detach_for_seq2seq` switch stops decoder gradients from flowing through the shared branch if you want to isolate the two objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b812a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available CUDA devices\n",
    "import torch\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        devices.append({\n",
    "            \"type\": \"CUDA\",\n",
    "            \"available\": True,\n",
    "            \"name\": device_name,\n",
    "            \"index\": i\n",
    "        })\n",
    "else:\n",
    "    devices.append({\"type\": \"CUDA\", \"available\": False, \"name\": \"N/A\"})\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6488924b",
   "metadata": {},
   "source": [
    "## Augmentation configuration\n",
    "This notebook now honours the extended SpecAugment policies, waveform perturbations, mixup, and phoneme-level dropout toggles defined in `Configs/config.yml`. Adjust those settings in the config file before running these evaluation cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change folder into the root of the ASR project\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"Configs\"):\n",
    "    %cd ../\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and define helper utilities\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from models import ASRCNN\n",
    "from meldataset import build_dataloader\n",
    "from utils import BatchSizeScheduler\n",
    "from token_map import build_token_map_from_data\n",
    "\n",
    "\n",
    "def cfg_get_nested(cfg: dict, path, default=None, sep='.'):\n",
    "    \"\"\"Get a nested value from a dict using a list of keys or a dot-separated string.\"\"\"\n",
    "    if isinstance(path, str):\n",
    "        keys = path.split(sep) if path else []\n",
    "    else:\n",
    "        keys = path\n",
    "\n",
    "    cur = cfg\n",
    "    for k in keys:\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "\n",
    "\n",
    "def load_token_map_from_config(config):\n",
    "    token_src = config.get('phoneme_maps_path')\n",
    "    if not token_src:\n",
    "        return build_token_map_from_data(\n",
    "            config.get('train_data'),\n",
    "            config.get('val_data'),\n",
    "            config.get('ood_data'),\n",
    "            apply_asr_tokenizer=True,\n",
    "        )\n",
    "    if isinstance(token_src, dict):\n",
    "        return token_src\n",
    "    csv = pd.read_csv(token_src, header=None).values\n",
    "    return {word: index for word, index in csv}\n",
    "\n",
    "\n",
    "\n",
    "def load_asr_model(model_path, config_path, device):\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    token_map = load_token_map_from_config(config)\n",
    "\n",
    "    model_params = cfg_get_nested(config, 'model_params', {\n",
    "        'input_dim': 80,\n",
    "        'hidden_dim': 256,\n",
    "        'n_token': len(token_map),\n",
    "        'token_embedding_dim': 512,\n",
    "        'n_layers': 5,\n",
    "        'location_kernel_size': 31,\n",
    "    })\n",
    "    if 'n_token' not in model_params:\n",
    "        model_params['n_token'] = len(token_map)\n",
    "\n",
    "    model_params.setdefault('stabilization_config', cfg_get_nested(config, 'stabilization', {}))\n",
    "\n",
    "    model = ASRCNN(**model_params)\n",
    "    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "    state_dict = checkpoint.get('model', checkpoint)\n",
    "    try:\n",
    "        model.load_state_dict(state_dict)\n",
    "    except RuntimeError:\n",
    "        sanitized_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(sanitized_state)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, config, token_map\n",
    "\n",
    "\n",
    "\n",
    "def build_dev_dataloader(config, device, batch_size=None, num_workers=None):\n",
    "    val_csv_path = config.get('val_data')\n",
    "    if val_csv_path is None:\n",
    "        raise ValueError(\"Validation CSV path ('val_data') not found in the config.\")\n",
    "\n",
    "    with open(val_csv_path, 'r', encoding='utf-8') as f:\n",
    "        raw_lines = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    path_list = []\n",
    "    for raw in raw_lines:\n",
    "        if not raw.strip():\n",
    "            continue\n",
    "        parts = raw.split('|')\n",
    "        if len(parts) == 1:\n",
    "            continue\n",
    "        path = parts[0]\n",
    "        if len(parts) == 2:\n",
    "            text = parts[1]\n",
    "            speaker = ''\n",
    "        else:\n",
    "            text = '|'.join(parts[1:-1])\n",
    "            speaker = parts[-1]\n",
    "        path_list.append([path, text, speaker])\n",
    "\n",
    "    if batch_size is None:\n",
    "        base_batch_size = int(cfg_get_nested(config, 'eval_params.batch_size', cfg_get_nested(config, 'batch_size', 4)))\n",
    "        curriculum_cfg = cfg_get_nested(config, 'training_curriculum.batch_size_schedule', {}) or {}\n",
    "        scheduler = BatchSizeScheduler(curriculum_cfg, default_batch_size=base_batch_size, total_epochs=int(cfg_get_nested(config, 'epochs', 1)))\n",
    "        if scheduler.enabled:\n",
    "            eval_epoch = cfg_get_nested(curriculum_cfg, 'evaluation_epoch', None)\n",
    "            if eval_epoch is None:\n",
    "                eval_epoch = scheduler.evaluation_epoch if scheduler.evaluation_epoch is not None else scheduler.total_epochs\n",
    "            eval_epoch = max(1, min(int(eval_epoch), scheduler.total_epochs))\n",
    "            batch_size = scheduler.batch_size_for_epoch(eval_epoch)\n",
    "        else:\n",
    "            batch_size = base_batch_size\n",
    "    if num_workers is None:\n",
    "        num_workers = int(cfg_get_nested(config, 'dataloader_params.val_num_workers', 2))\n",
    "\n",
    "    dataset_params = resolve_dataset_params(config)\n",
    "\n",
    "    collate_config = {'return_wave': False}\n",
    "    device_flag = device.type if isinstance(device, torch.device) else str(device)\n",
    "    loader = build_dataloader(\n",
    "        path_list=path_list,\n",
    "        validation=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        device=device_flag,\n",
    "        collate_config=collate_config,\n",
    "        dataset_config=dataset_params,\n",
    "        dataset_name=\"val\",\n",
    "    )\n",
    "    return loader\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e98fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dataset_params(config, base_overrides=None):\n",
    "    dataset_params = {\n",
    "        'dict_path': cfg_get_nested(config, 'phoneme_maps_path', 'Data/word_index_dict.txt'),\n",
    "        'sr': cfg_get_nested(config, 'preprocess_params.sr', cfg_get_nested(config, 'preprocess_parasm.sr', 24000)),\n",
    "        'spect_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.spect_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.spect_params', {'n_fft': 1024, 'win_length': 1024, 'hop_length': 300}),\n",
    "        ),\n",
    "        'mel_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.mel_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.mel_params', {'n_mels': 80}),\n",
    "        ),\n",
    "    }\n",
    "    dataset_params['mel_cache'] = cfg_get_nested(config, 'mel_cache', {}) or {}\n",
    "    dataset_params['phoneme_dictionary_config'] = cfg_get_nested(config, 'phoneme_dictionary', {}) or {}\n",
    "    dataset_overrides = cfg_get_nested(config, 'dataset_params', {})\n",
    "    if isinstance(dataset_overrides, dict):\n",
    "        for key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config'):\n",
    "            if key in dataset_overrides:\n",
    "                dataset_params[key] = dataset_overrides[key]\n",
    "        if 'spec_augment' in dataset_overrides:\n",
    "            dataset_params['spec_augment_params'] = dataset_overrides['spec_augment']\n",
    "        for key, value in dataset_overrides.items():\n",
    "            if key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config', 'spec_augment'):\n",
    "                continue\n",
    "            dataset_params[key] = value\n",
    "    if base_overrides:\n",
    "        dataset_params.update(base_overrides)\n",
    "    return dataset_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd38910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the latest ASR checkpoint and validation dataloader\n",
    "checkpoint_dir = 'Checkpoint'\n",
    "config_path = 'Checkpoint/config.yml'\n",
    "\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    raise FileNotFoundError(f\"Checkpoint directory '{checkpoint_dir}' not found.\")\n",
    "\n",
    "ckpt_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "if not ckpt_files:\n",
    "    raise FileNotFoundError(f\"No checkpoint files found in '{checkpoint_dir}'.\")\n",
    "\n",
    "ckpt_files = sorted(ckpt_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "model_path = os.path.join(checkpoint_dir, ckpt_files[-1])\n",
    "print(f\"Loading model from {model_path}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, config, token_map = load_asr_model(model_path, config_path, device)\n",
    "\n",
    "print(f'model --> {model_path}')\n",
    "print(f'config --> {config_path}')\n",
    "phoneme_source = config.get('phoneme_maps_path', 'built from dataset')\n",
    "print(f'dictionary --> {phoneme_source}')\n",
    "\n",
    "dev_loader = build_dev_dataloader(config, device)\n",
    "print(f'Validation dataset size: {len(dev_loader.dataset)} samples')\n",
    "\n",
    "if ' ' not in token_map:\n",
    "    raise KeyError(\"The vocabulary does not contain the blank symbol ' '.\")\n",
    "BLANK_ID = token_map[' ']\n",
    "print(f'Blank token id: {BLANK_ID}')\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af333384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helpers for CTC best-path decoding and skip-merge diagnostics\n",
    "@torch.no_grad()\n",
    "def best_path_durations(logits, lens, blank_id):\n",
    "    ids_all, durs_all = [], []\n",
    "    path = logits.argmax(-1)\n",
    "    for b in range(path.size(0)):\n",
    "        prev = None\n",
    "        ids_b, durs_b = [], []\n",
    "        T = int(lens[b])\n",
    "        for t in range(T):\n",
    "            cur = int(path[b, t])\n",
    "            if cur == blank_id:\n",
    "                prev = cur\n",
    "                continue\n",
    "            if prev != cur:\n",
    "                ids_b.append(cur)\n",
    "                durs_b.append(1)\n",
    "            else:\n",
    "                durs_b[-1] += 1\n",
    "            prev = cur\n",
    "        ids_all.append(ids_b)\n",
    "        durs_all.append(durs_b)\n",
    "    return ids_all, durs_all\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def skip_merge_flags(model, dev_loader, blank_id, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    model.eval()\n",
    "    diffs = []\n",
    "    downsample_factor = 2 ** getattr(model, 'n_down', 1)\n",
    "\n",
    "    for batch in dev_loader:\n",
    "        texts, text_lens, mels, mel_lens = batch[:4]\n",
    "        mels = mels.to(device)\n",
    "        text_lens = text_lens.to(torch.long)\n",
    "        mel_lens = mel_lens.to(torch.long)\n",
    "\n",
    "        outputs = model(mels)\n",
    "        if isinstance(outputs, dict):\n",
    "            logits = outputs.get('logits_ctc')\n",
    "            if logits is None:\n",
    "                raise KeyError(\"Model output dict does not contain 'logits_ctc'.\")\n",
    "        elif isinstance(outputs, (tuple, list)):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs\n",
    "\n",
    "        logits = logits.detach().cpu()\n",
    "        logit_lens = torch.clamp(\n",
    "            mel_lens // downsample_factor, min=1, max=logits.size(1)\n",
    "        ).cpu()\n",
    "\n",
    "        ids, _ = best_path_durations(logits, logit_lens, blank_id)\n",
    "        tgt_lens = text_lens.cpu().tolist()\n",
    "\n",
    "        for collapsed, tlen in zip(ids, tgt_lens):\n",
    "            diffs.append(len(collapsed) - int(tlen))\n",
    "\n",
    "    if not diffs:\n",
    "        raise RuntimeError('No samples processed when computing skip-merge statistics.')\n",
    "\n",
    "    diffs = np.array(diffs, dtype=np.float32)\n",
    "    return {\n",
    "        'mean_len_diff': float(diffs.mean()),\n",
    "        'p10': float(np.percentile(diffs, 10)),\n",
    "        'p90': float(np.percentile(diffs, 90)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d36ea-50e8-442e-8de9-00c8f28e1515",
   "metadata": {},
   "source": [
    "### Skip-Merge Diagnostics\n",
    "- mean_len_diff near 0 with tight percentiles implies the model isn't wildly skipping or over-splitting phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a874b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the skip-merge flag diagnostic\n",
    "metrics = skip_merge_flags(model, dev_loader, blank_id=BLANK_ID, device=device)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cb133",
   "metadata": {},
   "source": [
    "## Self-Conditioned CTC Support\n",
    "\n",
    "This notebook has been updated to work with the optional self-conditioned CTC feature. To experiment with it, enable the feature in `Configs/config.yml` by setting:\n",
    "\n",
    "```yaml\n",
    "stabilization:\n",
    "  self_conditioned_ctc:\n",
    "    enabled: true\n",
    "    layers:\n",
    "      - index: 2\n",
    "      - index: 4\n",
    "    conditioning_strategy: add  # or concat\n",
    "    detach_conditioning: true\n",
    "loss_weights:\n",
    "  self_conditioned_ctc: 0.2\n",
    "```\n",
    "\n",
    "With the feature enabled the trainer will expose `self_conditioned_ctc_logits` and `self_conditioned_ctc_log_probs` in the model outputs so they can be visualised or scored inside the notebook just like the existing CTC signals.\n",
    "\n",
    "```yaml\n",
    "regularization:\n",
    "  entropy:\n",
    "    enabled: false\n",
    "    mode: minimize  # set to \"maximize\" to encourage smoother distributions\n",
    "    eps: 1.0e-6\n",
    "    targets:\n",
    "      ctc:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "      s2s:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def _resolve_mixed_precision_from_config(_cfg):\n",
    "    if 'cfg_get_nested' in globals():\n",
    "        return cfg_get_nested(_cfg, 'precision.mixed_precision', {})\n",
    "    precision_cfg = _cfg.get('precision', {}) if isinstance(_cfg, dict) else {}\n",
    "    return precision_cfg.get('mixed_precision', {}) if isinstance(precision_cfg, dict) else {}\n",
    "\n",
    "if 'config' in globals():\n",
    "    _mp_root_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_root_config = yaml.safe_load(_f)\n",
    "\n",
    "mixed_precision_cfg = _resolve_mixed_precision_from_config(_mp_root_config) or {}\n",
    "grad_scaler_cfg = mixed_precision_cfg.get('grad_scaler', {}) if isinstance(mixed_precision_cfg, dict) else {}\n",
    "print(f\"mixed precision enabled --> {bool(mixed_precision_cfg.get('enabled', False))}\")\n",
    "print(f\"mixed precision dtype --> {mixed_precision_cfg.get('dtype', 'float16')}\")\n",
    "print(f\"grad scaler enabled --> {bool(grad_scaler_cfg.get('enabled', True))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
