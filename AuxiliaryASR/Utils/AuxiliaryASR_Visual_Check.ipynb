{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7073371",
   "metadata": {},
   "source": [
    "## Multi-task auxiliary objectives update\n",
    "This notebook has been updated to work with the extended multi-task ASR head. You can enable or disable CTC, seq2seq, frame-level phoneme classifiers, speaker embeddings, and pronunciation error classifiers individually through `Configs/config.yml` under the `multi_task` section. Adjust the corresponding `loss_weights` entries when experimenting with these objectives.\n",
    "\n",
    "> **Entropy regularisation**: Use the new `regularization.entropy` section in `Configs/config.yml` to enable or disable per-head entropy penalties. You can independently attach the regulariser to the CTC and seq2seq objectives without breaking joint training.\n",
    "\n",
    "> **Lazy phoneme dictionaries**: Toggle `phoneme_dictionary.lazy_loading` and `phoneme_dictionary.shared_cache` in `Configs/config.yml` to control when the phoneme map is parsed and whether dataloader workers reuse the cached mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50453a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect and optionally warm the metadata cache for notebook experiments\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if not os.path.isdir('Configs'):\n",
    "    os.chdir('..')\n",
    "\n",
    "_nb_root_dir = os.getcwd()\n",
    "if _nb_root_dir not in sys.path:\n",
    "    sys.path.insert(0, _nb_root_dir)\n",
    "\n",
    "import yaml\n",
    "from train import prepare_data_list\n",
    "from utils import get_data_path_list\n",
    "\n",
    "with open('Configs/config.yml', 'r', encoding='utf-8') as _cfg_file:\n",
    "    _nb_root_config = yaml.safe_load(_cfg_file)\n",
    "\n",
    "_metadata_cache_cfg = (_nb_root_config.get('metadata_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"metadata cache enabled --> {bool(_metadata_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"metadata cache directory --> {_metadata_cache_cfg.get('directory', 'Data/cache')}\")\n",
    "_dataset_toggles = (_metadata_cache_cfg.get('datasets') or {}) if isinstance(_metadata_cache_cfg, dict) else {}\n",
    "_normalised_dataset_toggles = {str(k).lower(): bool(v) for k, v in _dataset_toggles.items()} if isinstance(_dataset_toggles, dict) else {}\n",
    "\n",
    "def _cache_enabled_for(name: str) -> bool:\n",
    "    if not bool(_metadata_cache_cfg.get('enabled', False)):\n",
    "        return False\n",
    "    if not _normalised_dataset_toggles:\n",
    "        return True\n",
    "    return bool(_normalised_dataset_toggles.get(name.lower(), True))\n",
    "\n",
    "(\n",
    "    _train_raw,\n",
    "    _val_raw,\n",
    "    _train_meta_path,\n",
    "    _val_meta_path,\n",
    ") = get_data_path_list(\n",
    "    _nb_root_config.get('train_data'),\n",
    "    _nb_root_config.get('val_data'),\n",
    "    return_paths=True,\n",
    ")\n",
    "\n",
    "if _cache_enabled_for('train'):\n",
    "    _train_entries, _train_durations = prepare_data_list(\n",
    "        _train_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_train_meta_path,\n",
    "        dataset_name='train',\n",
    "    )\n",
    "    print(f\"[cache] training entries available: {len(_train_entries)}\")\n",
    "\n",
    "if _cache_enabled_for('val'):\n",
    "    _val_entries, _val_durations = prepare_data_list(\n",
    "        _val_raw,\n",
    "        root_path='',\n",
    "        cache_config=_metadata_cache_cfg,\n",
    "        metadata_path=_val_meta_path,\n",
    "        dataset_name='val',\n",
    "    )\n",
    "    print(f\"[cache] validation entries available: {len(_val_entries)}\")\n",
    "\n",
    "_ood_metadata_path = _nb_root_config.get('ood_data')\n",
    "if _ood_metadata_path and _cache_enabled_for('ood'):\n",
    "    try:\n",
    "        with open(_ood_metadata_path, 'r', encoding='utf-8') as _ood_file:\n",
    "            _ood_raw = _ood_file.readlines()\n",
    "        _ood_entries, _ood_durations = prepare_data_list(\n",
    "            _ood_raw,\n",
    "            root_path='',\n",
    "            cache_config=_metadata_cache_cfg,\n",
    "            metadata_path=_ood_metadata_path,\n",
    "            dataset_name='ood',\n",
    "        )\n",
    "        print(f\"[cache] OOD entries available: {len(_ood_entries)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[cache] OOD metadata not found at {_ood_metadata_path!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b96dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect mel cache configuration for notebook experiments\n",
    "_mel_cache_cfg = (_nb_root_config.get('mel_cache') or {}) if isinstance(_nb_root_config, dict) else {}\n",
    "print(f\"mel cache enabled --> {bool(_mel_cache_cfg.get('enabled', False))}\")\n",
    "print(f\"mel cache directory --> {_mel_cache_cfg.get('directory', 'Data/mel_cache')}\")\n",
    "print(f\"mel cache dtype --> {_mel_cache_cfg.get('dtype', 'float32')}\")\n",
    "_mel_cache_datasets = (_mel_cache_cfg.get('datasets') or {}) if isinstance(_mel_cache_cfg, dict) else {}\n",
    "if isinstance(_mel_cache_datasets, dict) and _mel_cache_datasets:\n",
    "    for _name, _flag in sorted(_mel_cache_datasets.items()):\n",
    "        print(f\"[mel cache] {_name}: {bool(_flag)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ad80a",
   "metadata": {},
   "source": [
    "## Memory optimization settings\n",
    "Lazy creation of decoder masks can now be toggled through the new `memory_optimizations.lazy_masks` section in `Configs/config.yml`. When enabled (the default), the trainer skips preallocating the `future_mask` and `text_mask` tensors unless an experiment explicitly needs them. Set the corresponding flags to `false` to restore the previous eager allocation behaviour.\n",
    "\n",
    "Gradient checkpointing for the deeper encoder blocks is configurable through `memory_optimizations.gradient_checkpointing`. Setting `enabled: true` activates `torch.utils.checkpoint.checkpoint_sequential` on the selected layer range so activations are recomputed during backpropagation instead of stored. Tune `start_layer`/`end_layer` to target specific encoder depths, `chunk_size` to limit how many stages are bundled into a checkpoint, and `segments` when you need finer control over how each chunk is split. The helper flags `min_sequence_length` and `use_checkpoint_sequential` let you skip short utterances or fall back to the basic checkpoint API if required.\n",
    "\n",
    "### CTC/seq2seq head sharing\n",
    "Enable `multi_task.head_sharing.ctc_seq2seq.enabled` in `Configs/config.yml` to reuse the intermediate projection computed for the CTC logits when running the seq2seq decoder. With the feature turned on the encoder exposes the shared tensor via `model_outputs[\"ctc_seq2seq_shared_states\"]` and feeds an adapter into the decoder so both heads reuse the same computation.\n",
    "\n",
    "Set the flag back to `false` to restore the previous behaviour. The optional `detach_for_seq2seq` switch stops decoder gradients from flowing through the shared branch if you want to isolate the two objectives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c765c9-c6ea-4a1a-b653-22514b84e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available CUDA devices\n",
    "import torch\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            device_name = torch.cuda.get_device_name(i)\n",
    "            devices.append({\n",
    "                \"type\": \"CUDA\",\n",
    "                \"available\": True,\n",
    "                \"name\": device_name,\n",
    "                \"index\": i\n",
    "            })\n",
    "else:\n",
    "    devices.append({\"type\": \"CUDA\", \"available\": False, \"name\": \"N/A\"})\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87948fd8",
   "metadata": {},
   "source": [
    "## Augmentation configuration\n",
    "This notebook now honours the extended SpecAugment policies, waveform perturbations, mixup, and phoneme-level dropout toggles defined in `Configs/config.yml`. Adjust those settings in the config file before running these evaluation cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e5615-cae5-4e9e-a6f5-cf41aa100e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change folder into the root of the ASR project\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"Configs\"):\n",
    "    %cd ../\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee581d0-e7ae-4cb5-94ba-77aaef068317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages, define common functions\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "from models import ASRCNN\n",
    "from utils import select_logits_from_output\n",
    "from meldataset import build_dataloader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "from text_utils import TextCleaner\n",
    "import itertools\n",
    "from jiwer import wer\n",
    "import re\n",
    "from token_map import build_token_map_from_data\n",
    "\n",
    "def cfg_get_nested(cfg: dict, path, default=None, sep=\".\"):\n",
    "    \"\"\"\n",
    "    Get a nested value from a dict using a list of keys or a dot-separated string.\n",
    "\n",
    "    Examples:\n",
    "        cfg_get_nested(config, [\"model_params\", \"input_dim\"], 80)\n",
    "        cfg_get_nested(config, \"model_params.input_dim\", 80)\n",
    "        cfg_get_nested(config, \"top_key\", 80)\n",
    "    \"\"\"\n",
    "    if isinstance(path, str):\n",
    "        keys = path.split(sep)\n",
    "    else:\n",
    "        keys = path\n",
    "\n",
    "    cur = cfg\n",
    "    for k in keys:\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "        mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "        mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "        return mask\n",
    "\n",
    "def load_ASR_models(ASR_MODEL_PATH, ASR_MODEL_CONFIG):\n",
    "    def _load_model(model_config, model_path):\n",
    "        model = ASRCNN(**model_config)\n",
    "        params = torch.load(model_path, map_location='cpu', weights_only=False)['model']\n",
    "        try:\n",
    "            model.load_state_dict(params)\n",
    "        except Exception as e:\n",
    "            new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in params.items()}\n",
    "            model.load_state_dict(new_state_dict)\n",
    "        return model\n",
    "\n",
    "    with open(ASR_MODEL_CONFIG) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    token_src = config.get('phoneme_maps_path')\n",
    "    if not token_src:\n",
    "        token_map = build_token_map_from_data(config.get('train_data'), config.get('val_data'), config.get('ood_data'), apply_asr_tokenizer=True)\n",
    "    elif isinstance(token_src, dict):\n",
    "        token_map = token_src\n",
    "    else:\n",
    "        csv = pd.read_csv(token_src, header=None).values\n",
    "        token_map = {word: index for word, index in csv}\n",
    "\n",
    "    default_model_params = {\n",
    "        'input_dim': 80,\n",
    "        'hidden_dim': 256,\n",
    "        'n_token': len(token_map),\n",
    "        'token_embedding_dim': 512,\n",
    "        'n_layers': 5,\n",
    "        'location_kernel_size': 31\n",
    "    }\n",
    "    model_params = cfg_get_nested(config, 'model_params', default_model_params)\n",
    "    if isinstance(model_params, dict):\n",
    "        model_params = dict(model_params)\n",
    "    else:\n",
    "        model_params = dict(default_model_params)\n",
    "\n",
    "    model_params.setdefault('n_token', len(token_map))\n",
    "    model_params.setdefault('stabilization_config', cfg_get_nested(config, 'stabilization', {}))\n",
    "\n",
    "    multi_task_config = cfg_get_nested(config, 'multi_task', {}) or {}\n",
    "    model_params['multi_task_config'] = multi_task_config\n",
    "\n",
    "    asr_model = _load_model(model_params, ASR_MODEL_PATH)\n",
    "\n",
    "    return asr_model\n",
    "\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_dataset_params(config, base_overrides=None):\n",
    "    dataset_params = {\n",
    "        'dict_path': cfg_get_nested(config, 'phoneme_maps_path', 'Data/word_index_dict.txt'),\n",
    "        'sr': cfg_get_nested(config, 'preprocess_params.sr', cfg_get_nested(config, 'preprocess_parasm.sr', 24000)),\n",
    "        'spect_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.spect_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.spect_params', {'n_fft': 1024, 'win_length': 1024, 'hop_length': 300}),\n",
    "        ),\n",
    "        'mel_params': cfg_get_nested(\n",
    "            config,\n",
    "            'preprocess_params.mel_params',\n",
    "            cfg_get_nested(config, 'preprocess_parasm.mel_params', {'n_mels': 80}),\n",
    "        ),\n",
    "    }\n",
    "    dataset_params['mel_cache'] = cfg_get_nested(config, 'mel_cache', {}) or {}\n",
    "    dataset_params['phoneme_dictionary_config'] = cfg_get_nested(config, 'phoneme_dictionary', {}) or {}\n",
    "    dataset_overrides = cfg_get_nested(config, 'dataset_params', {})\n",
    "    if isinstance(dataset_overrides, dict):\n",
    "        for key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config'):\n",
    "            if key in dataset_overrides:\n",
    "                dataset_params[key] = dataset_overrides[key]\n",
    "        if 'spec_augment' in dataset_overrides:\n",
    "            dataset_params['spec_augment_params'] = dataset_overrides['spec_augment']\n",
    "        for key, value in dataset_overrides.items():\n",
    "            if key in ('dict_path', 'sr', 'spect_params', 'mel_params', 'phoneme_dictionary_config', 'spec_augment'):\n",
    "                continue\n",
    "            dataset_params[key] = value\n",
    "    if base_overrides:\n",
    "        dataset_params.update(base_overrides)\n",
    "    return dataset_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c1c49-cd60-4252-8cf3-10bfc2457bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = \"Checkpoint\"\n",
    "\n",
    "files = [f for f in os.listdir( checkpoint_dir + \"/\") if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "model_path = checkpoint_dir + \"/\" + sorted_files[-1]\n",
    "#model_path = \"Checkpoint/epoch_00080.pth\"\n",
    "\n",
    "config_path = 'Checkpoint/config.yml'\n",
    "\n",
    "config = yaml.safe_load(open(config_path))\n",
    "phoneme_map = config.get('phoneme_maps_path')\n",
    "if not phoneme_map:\n",
    "    phoneme_map = build_token_map_from_data(config.get('train_data'), config.get('val_data'), config.get('ood_data'), apply_asr_tokenizer=True)\n",
    "\n",
    "test_csv_path = config['val_data']\n",
    "\n",
    "def _dict_desc(obj):\n",
    "    return obj if isinstance(obj, str) else 'built from dataset'\n",
    "\n",
    "print( \"model --> \" + model_path )\n",
    "print( \"config --> \" + config_path)\n",
    "print( \"dictionary --> \" + _dict_desc(phoneme_map))\n",
    "print( \"test: --> \" + test_csv_path)\n",
    "\n",
    "model = load_ASR_models(model_path, config_path)\n",
    "model.eval()\n",
    "\n",
    "print( \"All OK ✓\")\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea1982-c0b1-4367-a454-c5659bb64e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleaner(phoneme_map)\n",
    "device = \"cpu\"\n",
    "\n",
    "with open(test_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "dataset_params = resolve_dataset_params(config)\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    path_list=[l[:-1].split('|') for l in test_lines],\n",
    "    validation=True,\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    device=device,\n",
    "    collate_config={\"return_wave\": False},\n",
    "    dataset_config=dataset_params,\n",
    "    dataset_name=\"val\",\n",
    ")\n",
    "\n",
    "if isinstance(phoneme_map, str):\n",
    "    csv = pd.read_csv(phoneme_map, header=None).values\n",
    "    wlist = {word: index for word, index in csv}\n",
    "else:\n",
    "    wlist = phoneme_map\n",
    "index2phoneme = {v: k for k, v in wlist.items()}\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "cleartexts = []\n",
    "\n",
    "model.eval()\n",
    "log_interval = 10\n",
    "total = len(test_lines)\n",
    "cntr = 0\n",
    "maxtestsize = 1\n",
    "#maxtestsize = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        cleartexts.append(test_lines[cntr])\n",
    "\n",
    "        texts, input_lengths, mels, output_lengths = batch  # from Collater\n",
    "\n",
    "        mels = mels.to(device)\n",
    "        output = model(mels)\n",
    "        logits = select_logits_from_output(output)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        print(f\"Batch {cntr} - Expected text: {test_lines[cntr].strip().split('|')[1]}\")\n",
    "        predicted_text = ''.join([text_cleaner.inverse_mapping.get(phoneme.item(), '<unk>') for phoneme in predicted_ids[0]])\n",
    "        print(f\"Batch {cntr} - Predicted text: {predicted_text}\")\n",
    "\n",
    "        for i in range(predicted_ids.size(0)):\n",
    "            pred_seq = predicted_ids[i][:output_lengths[i]//3]\n",
    "            ref_seq = texts[i][:input_lengths[i]]\n",
    "\n",
    "            pred_phonemes = [index2phoneme.get(p.item(), '') for p in pred_seq]\n",
    "            ref_phonemes = [index2phoneme.get(r.item(), '') for r in ref_seq]\n",
    "\n",
    "            predictions.append(pred_phonemes)\n",
    "            references.append(ref_phonemes)\n",
    "\n",
    "        cntr += 1\n",
    "        if (cntr)%log_interval == 0:\n",
    "            print(f\"{cntr} of {total} sentences tested\")\n",
    "\n",
    "        if maxtestsize > 0 and cntr >= maxtestsize:\n",
    "            print(f\"early stop reached at {maxtestsize} sentences\")\n",
    "            break\n",
    "\n",
    "print(\"Done - all sententes tested.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1ab20-dea7-4aca-ba50-9f61b5bf7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean extra quotes and join tokens into space-separated strings\n",
    "references_cleaned = [' '.join(token.strip('\"') for token in seq) for seq in references]\n",
    "predictions_cleaned = [' '.join(token.strip('\"') for token in seq) for seq in predictions]\n",
    "\n",
    "# Now use jiwer\n",
    "per = wer(references_cleaned, predictions_cleaned)\n",
    "print(f'Phoneme Error Rate: {per:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983e6be-d995-4254-8477-bd7f7b59e17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Find the best AUX model (with best PER) #\n",
    "###########################################\n",
    "\n",
    "#checkpoint_dir = \"Checkpoint-en-test\"\n",
    "\n",
    "files = [f for f in os.listdir( checkpoint_dir + \"/\") if f.startswith('epoch_') and f.endswith('.pth')]\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "model_path = checkpoint_dir + \"/\" + sorted_files[-1]\n",
    "#model_path = \"Checkpoint/epoch_00040.pth\"\n",
    "#config_path = \"Configs/config.yml\"\n",
    "\n",
    "config = yaml.safe_load(open(config_path))\n",
    "phoneme_map = config.get('phoneme_maps_path')\n",
    "if not phoneme_map:\n",
    "    phoneme_map = build_token_map_from_data(config.get('train_data'), config.get('val_data'), config.get('ood_data'), apply_asr_tokenizer=True)\n",
    "\n",
    "test_csv_path = config['val_data']\n",
    "\n",
    "def _dict_desc(obj):\n",
    "    return obj if isinstance(obj, str) else 'built from dataset'\n",
    "\n",
    "print( \"config --> \" + config_path )\n",
    "print( \"dictionary --> \" + _dict_desc(phoneme_map) )\n",
    "print( \"test --> \" + test_csv_path )\n",
    "\n",
    "text_cleaner = TextCleaner(phoneme_map)\n",
    "#text_cleaner = TextCleaner()\n",
    "device = \"cpu\"\n",
    "\n",
    "with open(test_csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "dataset_params = resolve_dataset_params(config)\n",
    "\n",
    "test_loader = build_dataloader(\n",
    "    #path_list=test_lines,\n",
    "    path_list=[l[:-1].split('|') for l in test_lines],\n",
    "    validation=True,\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    device=device,\n",
    "    collate_config={\"return_wave\": False},\n",
    "    dataset_config=dataset_params,\n",
    "    dataset_name=\"val\",\n",
    ")\n",
    "\n",
    "if isinstance(phoneme_map, str):\n",
    "    csv = pd.read_csv(phoneme_map, header=None).values\n",
    "    wlist = {word: index for word, index in csv}\n",
    "else:\n",
    "    wlist = phoneme_map\n",
    "index2phoneme = {v: k for k, v in wlist.items()}\n",
    "\n",
    "best_model = \"\"\n",
    "best_model_per = 100.0\n",
    "\n",
    "model_cntr = 1\n",
    "total_files = len(sorted_files)\n",
    "results = []\n",
    "#maxtestsize = 0 # will test the full validation set - may take a while and is usually not necessary\n",
    "maxtestsize = 25\n",
    "\n",
    "for aux_model_file in sorted_files:\n",
    "    model_path = checkpoint_dir + \"/\" + aux_model_file\n",
    "    model = load_ASR_models(model_path, config_path)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"[{model_cntr}/{total_files}] Now evaluating AUX model: {aux_model_file}\")\n",
    "    model_cntr += 1\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    first_ref = \"\"\n",
    "    first_pred = \"\"\n",
    "\n",
    "    test_iter = iter(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for sample_idx in range(len(test_loader)):\n",
    "            if maxtestsize > 0 and sample_idx >= maxtestsize:\n",
    "                break\n",
    "\n",
    "            batch = next(test_iter)\n",
    "            texts, input_lengths, mels, output_lengths = batch\n",
    "            mels = mels.to(device)\n",
    "            output = model(mels)\n",
    "            logits = select_logits_from_output(output)\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            for i in range(predicted_ids.size(0)):\n",
    "                pred_seq = predicted_ids[i][:output_lengths[i] // 3]\n",
    "                ref_seq = texts[i][:input_lengths[i]]\n",
    "\n",
    "                pred_phonemes = [index2phoneme.get(p.item(), '') for p in pred_seq]\n",
    "                ref_phonemes = [index2phoneme.get(r.item(), '') for r in ref_seq]\n",
    "\n",
    "                predictions.append(pred_phonemes)\n",
    "                references.append(ref_phonemes)\n",
    "\n",
    "                if first_ref == \"\":\n",
    "                    first_ref = ' '.join(ref_phonemes)\n",
    "                    first_pred = ' '.join(pred_phonemes)\n",
    "\n",
    "    references_cleaned = [' '.join(seq) for seq in references]\n",
    "    predictions_cleaned = [' '.join(seq) for seq in predictions]\n",
    "    per = wer(references_cleaned, predictions_cleaned)\n",
    "\n",
    "    print(f'Phoneme Error Rate: {per:.4f} {\"✓\" if per < best_model_per else \"✗\"}')\n",
    "    if per < best_model_per:\n",
    "        best_model_per = per\n",
    "        best_model = aux_model_file\n",
    "\n",
    "    results.append({\n",
    "        'model': aux_model_file,\n",
    "        'per': per,\n",
    "        'first_ref': first_ref,\n",
    "        'first_pred': first_pred\n",
    "    })\n",
    "\n",
    "results_sorted = sorted(results, key=lambda x: x['per'])\n",
    "\n",
    "print(\"===================\")\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"===================\")\n",
    "for res in results_sorted:\n",
    "    print(f\"Model: {res['model']}\")\n",
    "    print(f\"PER: {res['per']:.4f}\")\n",
    "    print(f\"Reference: {res['first_ref']}\")\n",
    "    print(f\"Prediction: {res['first_pred']}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "best = results_sorted[0]\n",
    "print(f\"✅ Best model: {best['model']} with PER = {best['per']:.4f}\")\n",
    "\n",
    "if 'config' in globals() and isinstance(config, dict):\n",
    "    _mp_base_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_base_config = yaml.safe_load(_f)\n",
    "\n",
    "memory_opts = _mp_base_config.get(\"memory_optimizations\", {}) if isinstance(_mp_base_config, dict) else {}\n",
    "lazy_mask_cfg = memory_opts.get(\"lazy_masks\", {}) if isinstance(memory_opts, dict) else {}\n",
    "lazy_enabled = bool(lazy_mask_cfg.get(\"enabled\", True))\n",
    "print(f\"lazy mask creation enabled --> {lazy_enabled}\")\n",
    "print(f\"skip future mask allocation --> {bool(lazy_mask_cfg.get('future_mask', True))}\")\n",
    "print(f\"skip text mask allocation --> {bool(lazy_mask_cfg.get('text_mask', True))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a9257",
   "metadata": {},
   "source": [
    "## Self-Conditioned CTC Support\n",
    "\n",
    "This notebook has been updated to work with the optional self-conditioned CTC feature. To experiment with it, enable the feature in `Configs/config.yml` by setting:\n",
    "\n",
    "```yaml\n",
    "stabilization:\n",
    "  self_conditioned_ctc:\n",
    "    enabled: true\n",
    "    layers:\n",
    "      - index: 2\n",
    "      - index: 4\n",
    "    conditioning_strategy: add  # or concat\n",
    "    detach_conditioning: true\n",
    "loss_weights:\n",
    "  self_conditioned_ctc: 0.2\n",
    "```\n",
    "\n",
    "With the feature enabled the trainer will expose `self_conditioned_ctc_logits` and `self_conditioned_ctc_log_probs` in the model outputs so they can be visualised or scored inside the notebook just like the existing CTC signals.\n",
    "\n",
    "```yaml\n",
    "regularization:\n",
    "  entropy:\n",
    "    enabled: false\n",
    "    mode: minimize  # set to \"maximize\" to encourage smoother distributions\n",
    "    eps: 1.0e-6\n",
    "    targets:\n",
    "      ctc:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "      s2s:\n",
    "        enabled: true\n",
    "        weight: 0.0\n",
    "        length_normalize: true\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785837e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def _resolve_mixed_precision_from_config(_cfg):\n",
    "    if 'cfg_get_nested' in globals():\n",
    "        return cfg_get_nested(_cfg, 'precision.mixed_precision', {})\n",
    "    precision_cfg = _cfg.get('precision', {}) if isinstance(_cfg, dict) else {}\n",
    "    return precision_cfg.get('mixed_precision', {}) if isinstance(precision_cfg, dict) else {}\n",
    "\n",
    "if 'config' in globals():\n",
    "    _mp_root_config = config\n",
    "else:\n",
    "    with open('Configs/config.yml') as _f:\n",
    "        _mp_root_config = yaml.safe_load(_f)\n",
    "\n",
    "mixed_precision_cfg = _resolve_mixed_precision_from_config(_mp_root_config) or {}\n",
    "grad_scaler_cfg = mixed_precision_cfg.get('grad_scaler', {}) if isinstance(mixed_precision_cfg, dict) else {}\n",
    "print(f\"mixed precision enabled --> {bool(mixed_precision_cfg.get('enabled', False))}\")\n",
    "print(f\"mixed precision dtype --> {mixed_precision_cfg.get('dtype', 'float16')}\")\n",
    "print(f\"grad scaler enabled --> {bool(grad_scaler_cfg.get('enabled', True))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
