{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LJSpeech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nm653VK4CG9F"
   },
   "source": [
    "### Install packages and download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gciBKMqCCLvT"
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "git clone https://github.com/yl4579/StyleTTS2.git\n",
    "cd StyleTTS2\n",
    "pip install SoundFile torchaudio munch torch pydub pyyaml librosa nltk matplotlib accelerate transformers phonemizer einops einops-exts tqdm typing-extensions git+https://github.com/resemble-ai/monotonic_align.git\n",
    "sudo apt-get install espeak-ng\n",
    "git-lfs clone https://huggingface.co/yl4579/StyleTTS2-LJSpeech\n",
    "mv StyleTTS2-LJSpeech/Models ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAA8lx-XCQnM"
   },
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0XRpbxSCSix"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"Modules\"):\n",
    "    %cd ../\n",
    "\n",
    "!pwd\n",
    "lang_id = 'ar'\n",
    "checkpoint_dir = \"Checkpoint_ar_new_aux_whisper_large\"\n",
    "#checkpoint_dir = \"Models/LJSpeech_cs_wavlm\"\n",
    "#checkpoint_dir = \"Models/LJSpeech_ar_en_whisper\"\n",
    "config_file_name = \"config.yml\"\n",
    "\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import noisereduce as nr\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max()).unsqueeze(0).expand(lengths.shape[0], -1).type_as(lengths)\n",
    "    mask = torch.gt(mask+1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "def compute_style(ref_dicts):\n",
    "    reference_embeddings = {}\n",
    "    for key, path in ref_dicts.items():\n",
    "        wave, sr = librosa.load(path, sr=24000)\n",
    "        audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "        if sr != 24000:\n",
    "            audio = librosa.resample(audio, sr, 24000)\n",
    "        mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ref = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        reference_embeddings[key] = (ref.squeeze(1), audio)\n",
    "\n",
    "    return reference_embeddings\n",
    "\n",
    "# load phonemizer\n",
    "import phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language=lang_id, preserve_punctuation=True, with_stress=True, words_mismatch='ignore')\n",
    "\n",
    "#config = yaml.safe_load(open( checkpoint_dir + \"/config.yml\" ))\n",
    "config = yaml.safe_load(open( checkpoint_dir + \"/\" + config_file_name ))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model = build_model(recursive_munch(config['model_params']), text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "files = [f for f in os.listdir( checkpoint_dir + \"/\") if f.startswith('epoch_2nd') and f.endswith('.pth')]\n",
    "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "model_path = checkpoint_dir + \"/\" + sorted_files[-1]\n",
    "print(\"Loading model\", model_path)\n",
    "params_whole = torch.load(model_path, map_location='cpu')\n",
    "params = params_whole['net']\n",
    "\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "\n",
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0), # empirical parameters\n",
    "    clamp=False\n",
    ")\n",
    "\n",
    "def inference(text, noise, diffusion_steps=5, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\"', '')\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = word_tokenize(ps[0])\n",
    "    ps = ' '.join(ps)\n",
    "\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(noise,\n",
    "              embedding=bert_dur[0].unsqueeze(0), num_steps=diffusion_steps,\n",
    "              embedding_scale=embedding_scale).squeeze(0)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_dur[-1] += 5\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "        out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
    "                                F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return out.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "def inference_100ms(\n",
    "        text: str,\n",
    "        noise,\n",
    "        *,\n",
    "        diffusion_steps: int = 5,\n",
    "        embedding_scale: float = 1.0,\n",
    "        sample_rate: int = 24000,\n",
    "        trim_ms: int = 100,\n",
    "        remove_pause: bool = True,\n",
    "):\n",
    "    \"\"\"Standard StyleTTS2 inference with built-in tail-trim & pause removal.\n",
    "\n",
    "    Differences from the original reference:\n",
    "      • *No* extra pause phonemes/punctuation when ``remove_pause`` is *True*.\n",
    "      • Last ``trim_ms`` ms of audio are discarded to remove trailing artifacts.\n",
    "      • Keeps the original behaviour of adding +5 frames to the final token’s\n",
    "        predicted duration.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) TEXT PRE-PROCESS\n",
    "    text = text.strip().replace('\"', '')\n",
    "    if remove_pause:\n",
    "        text = re.sub(r\"[.,;:!?]\", \"\", text)\n",
    "\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    phonemes = word_tokenize(ps[0])\n",
    "    if remove_pause:\n",
    "        phonemes = [p for p in phonemes if p.lower() not in (\"sp\", \"sil\", \"pau\")]\n",
    "    ps = \" \".join(phonemes)\n",
    "\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise,\n",
    "            embedding=bert_dur[0].unsqueeze(0),\n",
    "            num_steps=diffusion_steps,\n",
    "            embedding_scale=embedding_scale,\n",
    "        ).squeeze(0)\n",
    "\n",
    "        s, ref = s_pred[:, 128:], s_pred[:, :128]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = torch.sigmoid(model.predictor.duration_proj(x)).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        # preserve original tweak: lengthen final token a bit\n",
    "        pred_dur[-1] += 5\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            span = int(pred_dur[i])\n",
    "            pred_aln_trg[i, c_frame:c_frame + span] = 1\n",
    "            c_frame += span\n",
    "\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "        wav = model.decoder(\n",
    "            t_en @ pred_aln_trg.unsqueeze(0).to(device),\n",
    "            F0_pred,\n",
    "            N_pred,\n",
    "            ref.squeeze().unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    # 5) POST-PROCESS – trim tail\n",
    "    audio = wav.squeeze().cpu().numpy()\n",
    "    samples_trim = int(sample_rate * trim_ms / 1000)\n",
    "    if audio.shape[-1] > samples_trim:\n",
    "        audio = audio[:-samples_trim]\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def LFinference(text, s_prev, noise, alpha=0.7, diffusion_steps=5, embedding_scale=1):\n",
    "  text = text.strip()\n",
    "  text = text.replace('\"', '')\n",
    "  ps = global_phonemizer.phonemize([text])\n",
    "  ps = word_tokenize(ps[0])\n",
    "  ps = ' '.join(ps)\n",
    "\n",
    "  tokens = textclenaer(ps)\n",
    "  tokens.insert(0, 0)\n",
    "  tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "      text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "      t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "      bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "      d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "      s_pred = sampler(noise,\n",
    "            embedding=bert_dur[0].unsqueeze(0), num_steps=diffusion_steps,\n",
    "            embedding_scale=embedding_scale).squeeze(0)\n",
    "\n",
    "      if s_prev is not None:\n",
    "          # convex combination of previous and current style\n",
    "          s_pred = alpha * s_prev + (1 - alpha) * s_pred\n",
    "\n",
    "      s = s_pred[:, 128:]\n",
    "      ref = s_pred[:, :128]\n",
    "\n",
    "      d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "      x, _ = model.predictor.lstm(d)\n",
    "      duration = model.predictor.duration_proj(x)\n",
    "      duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "      pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "      pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "      c_frame = 0\n",
    "      for i in range(pred_aln_trg.size(0)):\n",
    "          pred_aln_trg[i, c_frame:c_frame + int(pred_dur[i].data)] = 1\n",
    "          c_frame += int(pred_dur[i].data)\n",
    "\n",
    "      # encode prosody\n",
    "      en = (d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device))\n",
    "      F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "      out = model.decoder((t_en @ pred_aln_trg.unsqueeze(0).to(device)),\n",
    "                              F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "  return out.squeeze().cpu().numpy(), s_pred\n",
    "\n",
    "\n",
    "def _trim_tail_if_voiceless(audio, sample_rate, trim_ms=100, voice_threshold=0.02):\n",
    "    samples_trim = int(sample_rate * trim_ms / 1000)\n",
    "    if samples_trim <= 0 or len(audio) <= samples_trim:\n",
    "        return audio\n",
    "    tail = audio[-samples_trim:]\n",
    "    energy = np.abs(tail)\n",
    "    # check if any voiced part (above threshold) is in last 100ms\n",
    "    if np.any(energy > voice_threshold):\n",
    "        return audio  # do NOT trim if voiced segment exists in tail\n",
    "    return audio[:-samples_trim]\n",
    "\n",
    "def _crossfade_concat(wavs, crossfade_ms=50, sample_rate=24000):\n",
    "    if not wavs:\n",
    "        return np.array([])\n",
    "    if len(wavs) == 1:\n",
    "        return wavs[0]\n",
    "\n",
    "    crossfade_samples = int(sample_rate * crossfade_ms / 1000)\n",
    "    result = wavs[0]\n",
    "\n",
    "    for w in wavs[1:]:\n",
    "        if len(result) > crossfade_samples and len(w) > crossfade_samples:\n",
    "            fade_out = np.linspace(1, 0, crossfade_samples)\n",
    "            fade_in = np.linspace(0, 1, crossfade_samples)\n",
    "            overlap = result[-crossfade_samples:] * fade_out + w[:crossfade_samples] * fade_in\n",
    "            result = np.concatenate([result[:-crossfade_samples], overlap, w[crossfade_samples:]])\n",
    "        else:\n",
    "            result = np.concatenate([result, w])\n",
    "\n",
    "    return result\n",
    "    \n",
    "def _append_silence(audio, sample_rate, silence_ms=200, energy_threshold=0.003):\n",
    "    silence_len = int(sample_rate * silence_ms / 1000)\n",
    "    tail = audio[-silence_len:]\n",
    "    if np.max(np.abs(tail)) < energy_threshold:\n",
    "        noise_segment = tail\n",
    "    else:\n",
    "        # Extract low-energy segments for noise reference\n",
    "        noise_segment = _estimate_noise(audio, sample_rate, segment_ms=silence_ms, energy_threshold=energy_threshold)\n",
    "        if noise_segment is None:\n",
    "            noise_segment = np.zeros(silence_len, dtype=audio.dtype)\n",
    "\n",
    "    # Repeat noise segment until desired length is reached\n",
    "    repeat_count = int(np.ceil(silence_len / len(noise_segment)))\n",
    "    extended_noise = np.tile(noise_segment, repeat_count)[:silence_len]\n",
    "    return np.concatenate([audio, extended_noise])\n",
    "\n",
    "def _estimate_noise(audio, sample_rate, segment_ms=200, energy_threshold=0.02):\n",
    "    segment_len = int(sample_rate * segment_ms / 1000)\n",
    "    noise_segments = []\n",
    "    for start in range(0, len(audio), segment_len):\n",
    "        end = start + segment_len\n",
    "        seg = audio[start:end]\n",
    "        if len(seg) < segment_len:\n",
    "            break\n",
    "        if np.max(np.abs(seg)) < energy_threshold:\n",
    "            noise_segments.append(seg)\n",
    "    if not noise_segments:\n",
    "        return None\n",
    "    return np.concatenate(noise_segments)\n",
    "\n",
    "def _denoise_audio(audio, noise_reference, reduction=0.6):\n",
    "    if noise_reference is None or len(noise_reference) == 0:\n",
    "        return audio\n",
    "    return nr.reduce_noise(y=audio, y_noise=noise_reference, sr=24000, prop_decrease=reduction)\n",
    "\n",
    "def LFinference_trim_100ms(\n",
    "        text: str,\n",
    "        s_prev=None,\n",
    "        noise=None,\n",
    "        *,\n",
    "        alpha: float = 0.7,\n",
    "        diffusion_steps: int = 5,\n",
    "        embedding_scale: float = 1.0,\n",
    "        sample_rate: int = 24000,\n",
    "        trim_ms: int = 100,\n",
    "        remove_pause: bool = True,\n",
    "        end_silence_ms: int = 200,\n",
    "        noise_reduction_threshold: float = 0.6,\n",
    "        noise_estimage_energy_threshold: float = 0.02,\n",
    "        append_silence_energy_threshold: float = 0.003\n",
    "):\n",
    "    text = text.strip().replace('\"', '')\n",
    "    #if remove_pause:\n",
    "        #text = re.sub(r\"[.,;:!?]\", \"\", text)\n",
    "\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    phonemes = word_tokenize(ps[0])\n",
    "    if remove_pause:\n",
    "        phonemes = [p for p in phonemes if p.lower() not in (\"sp\", \"sil\", \"pau\")]\n",
    "    ps = \" \".join(phonemes)\n",
    "\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)\n",
    "        text_mask = length_to_mask(input_lengths).to(tokens.device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise,\n",
    "            embedding=bert_dur[0].unsqueeze(0),\n",
    "            num_steps=diffusion_steps,\n",
    "            embedding_scale=embedding_scale,\n",
    "        ).squeeze(0)\n",
    "\n",
    "        if s_prev is not None:\n",
    "            s_pred = alpha * s_prev + (1.0 - alpha) * s_pred\n",
    "\n",
    "        s, ref = s_pred[:, 128:], s_pred[:, :128]\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = torch.sigmoid(model.predictor.duration_proj(x)).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            span = int(pred_dur[i])\n",
    "            pred_aln_trg[i, c:c + span] = 1\n",
    "            c += span\n",
    "\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "        wav = model.decoder(\n",
    "            t_en @ pred_aln_trg.unsqueeze(0).to(device),\n",
    "            F0_pred,\n",
    "            N_pred,\n",
    "            ref.squeeze().unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    audio = wav.squeeze().cpu().numpy()\n",
    "    noise_ref = _estimate_noise(audio, sample_rate, energy_threshold=noise_estimage_energy_threshold)\n",
    "    audio = _denoise_audio(audio, noise_ref, reduction=noise_reduction_threshold)\n",
    "    audio = _trim_tail_if_voiceless(audio, sample_rate, trim_ms)\n",
    "    audio = _append_silence(audio, sample_rate, end_silence_ms, energy_threshold=append_silence_energy_threshold)\n",
    "    return audio, s_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuCbS0gdArgJ"
   },
   "source": [
    "### Synthesize speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ud1Y-kbBPTw"
   },
   "outputs": [],
   "source": [
    "# @title Input Text { display-mode: \"form\" }\n",
    "# synthesize a text\n",
    "#text = \"StyleTTS 2 is a text-to-speech model that leverages style diffusion and adversarial training with large speech language models to achieve human-level text-to-speech synthesis.\" # @param {type:\"string\"}\n",
    "#text = \"Příběh strýčka Martina je jiná kniha, než na jaké jste zvyklí. A dost možná to ani není kniha pro vás. Proto také vychází v malém nákladu, jen pro úzký okruh lidí. Není to ani historická beletrie, ani fantazy, ani soudobá próza. Nejblíže má k iniciačnímu románu.\"\n",
    "#text = \"Tři roky psaná kniha, kterou není lehké zařadit. V prvé fázi deníček studentky, která hledá svého strýčka. V druhé fázi vidiny, stíny minulosti. Dechberoucí obrazy bitev, zkázy, rozcestí české a evropské minulosti. To je Příběh strýčka Martina.\"\n",
    "#text = \"Historická rovina knihy se pohybuje kolem roku 950, v době boje knížete Boleslava s německým králem Otou. Historický příběh vlastně začíná smrtí hlavního hrdiny, šéfšpicla a šedé eminence české země Martina z Wartberka a ztraceným zemským tributem, který měl konvoj dovést k říšskému králi.\"\n",
    "#text = \"Horečnou snahou zajistit zemi, která se najednou ocitla ve smrtelném ohrožení, protože kdo ví, co všechno se ztratilo s Wartberkovou smrtí.\"\n",
    "#text = \"Současný příběh je hledání Veroniky po strýčkovi, který se ztratil. Kam a jak, to vlastně nikdo neví. Až teď začíná Veronika nacházet spojnice mezi svými vizemi a sny se současností. A uvědomuje si, že Martin historický je tím Martinem současným, že najít jednoho znamená najít druhého. Kéž by to bylo tak jednoduché.\"\n",
    "#text = \"Jenže, darmo se neříká, že cesta je cíl. Platí to i v tomto případě - projít cestu je podstatné.\"\n",
    "#text = \"V knize se potkáte se dvěmi historickými bitvami, které dnes už vlastně upadly v zapomnění.\"\n",
    "#text = \"Bitvou u Lechu v roce 955, kde padl prakticky celý český vojenský sbor v boji proti Maďarům - zdarma se dozvíte, z čeho je jméno \"\"maďaři\"\" odvozeno - a s bitvou u Nového hradu v roce 950, kde - a o tom kroniky cudně mlčí - porazil kníže Boleslav krále Otu a učinil z Čech plnoprávného souseda a souputníka Říše.\"\n",
    "#text = \"Setkáte se tu s řadou historických postav. Vlastně všechny postavy v knize jsou postavy žijící, historické, které mají nějaký svůj předobraz. Výjimkou je Maxmilián ze Schweringenu, komoří Oty. Důvod? Toho skutečného nemám rád a tak jsem ho nechtěl ani zmiňovat. Snad mi prominete.\"\n",
    "#text = \"إذا كان هناك يوم عمل واحد فقط بين عطلتين رسميتين، يعتبر هذا اليوم جزءا من العطلة\"\n",
    "#text = \"اللي ما يعرف الصقر يشويه و من طول الغيبات جاب الغنايم\"\n",
    "#text = '\"Umíš to přeložit?\" Zeptal se mě.\"'\n",
    "#text = '\"Petra je latinsky skála, ale zároveň je to jméno - Petr. Je odvozené ze slova skála, že?\" Zeptal se Martin.'\n",
    "#text = 'A pak mi to došlo. Takže se nedá říct, jestli Ježíš mluví o Petrovi nebo o skále?'\n",
    "#text = 'اكتشف بعد فتره طويله مافي شي في الحياة غلط او كذب او ليس صحيح لأ هناك وجهات نظر مختلفه انت تؤمن بهذا الشي'\n",
    "#text = 'وتطرق الجانبان إلى تطورات الملف السوري، وأكدا أهمية التزام المجتمع الدولي بالمعاهدات والاتفاقيات الدولية ذات الصلة، كما شددا على دور المنظمة في ضمان التنفيذ الفعّال لاتفاقية حظر استخدام الأسلحة الكيميائية.'\n",
    "#text = 'وأشاد سعادة المدير العام لمنظمة حظر الأسلحة الكيميائية، خلال الاجتماع، بدور دولة قطر في تمثيل مصالح الجمهورية العربية السورية في المنظمة،  ودعمها للجهود الدولية الرامية إلى تحقيق الأمن والاستقرار على المستويين الإقليمي والدولي، والتزامها بمبادئ القانون الدولي'\n",
    "text = 'أَلِف با تا ثا جِيم حَا خَا دَال ذَال را زَاي سِين شِين صَاد ضَاد طَا ظَا عَين غَين فَا قَاف كَاف لَام مِيم نُون هَا وَاو يَا'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM2NjuM7B6sz"
   },
   "source": [
    "#### Basic synthesis (5 diffusion steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KILqC-V-Ay5e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.manual_seed(17484051992422920962)\n",
    "#print(\"seed\", torch.initial_seed())\n",
    "print(\"seed\", torch.seed())\n",
    "start = time.time()\n",
    "noise = torch.randn(1,1,256).to(device)\n",
    "wav = inference(text, noise, diffusion_steps=10, embedding_scale=1)\n",
    "rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "display(ipd.Audio(wav, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZk9o-EzCBVx"
   },
   "source": [
    "#### With higher diffusion steps (more diverse)\n",
    "Since the sampler is ancestral, the higher the stpes, the more diverse the samples are, with the cost of slower synthesis speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_OHtzMbB9gL"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "noise = torch.randn(1,1,256).to(device)\n",
    "wav = inference(text, noise, diffusion_steps=10, embedding_scale=1)\n",
    "rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "print(f\"RTF = {rtf:5f}\")\n",
    "import IPython.display as ipd\n",
    "display(ipd.Audio(wav, rate=24000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyDACd-0CaqL"
   },
   "source": [
    "### Speech expressiveness\n",
    "The following section recreates the samples shown in [Section 6](https://styletts2.github.io/#emo) of the demo page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRkS5VWxCck4"
   },
   "source": [
    "#### With embedding_scale=1\n",
    "This is the classifier-free guidance scale. The higher the scale, the more conditional the style is to the input text and hence more emotional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5g5RO-mCbZB"
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "texts['Happy'] = \"We are happy to invite you to join us on a journey to the past, where we will visit the most amazing monuments ever built by human hands.\"\n",
    "texts['Sad'] = \"I am sorry to say that we have suffered a severe setback in our efforts to restore prosperity and confidence.\"\n",
    "texts['Angry'] = \"The field of astronomy is a joke! Its theories are based on flawed observations and biased interpretations!\"\n",
    "texts['Surprised'] = \"I can't believe it! You mean to tell me that you have discovered a new species of bacteria in this pond?\"\n",
    "\n",
    "for k,v in texts.items():\n",
    "    noise = torch.randn(1,1,256).to(device)\n",
    "    wav = inference(v, noise, diffusion_steps=10, embedding_scale=1)\n",
    "    print(k + \": \")\n",
    "    display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4S8TXSpCgpA"
   },
   "source": [
    "#### With embedding_scale=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHHIdeNrCezC"
   },
   "outputs": [],
   "source": [
    "texts = {}\n",
    "texts['Happy'] = \"We are happy to invite you to join us on a journey to the past, where we will visit the most amazing monuments ever built by human hands.\"\n",
    "texts['Sad'] = \"I am sorry to say that we have suffered a severe setback in our efforts to restore prosperity and confidence.\"\n",
    "texts['Angry'] = \"The field of astronomy is a joke! Its theories are based on flawed observations and biased interpretations!\"\n",
    "texts['Surprised'] = \"I can't believe it! You mean to tell me that you have discovered a new species of bacteria in this pond?\"\n",
    "\n",
    "for k,v in texts.items():\n",
    "    noise = torch.randn(1,1,256).to(device)\n",
    "    wav = inference(v, noise, diffusion_steps=10, embedding_scale=2) # embedding_scale=2 for more pronounced emotion\n",
    "    print(k + \": \")\n",
    "    display(ipd.Audio(wav, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAh7Tov4CkuH"
   },
   "source": [
    "### Long-form generation\n",
    "This section includes basic implementation of Algorithm 1 in the paper for consistent longform audio generation. The example passage is taken from [Section 5](https://styletts2.github.io/#long) of the demo page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IJwUbgvACoDu"
   },
   "outputs": [],
   "source": [
    "passage = '''\n",
    "Příběh strýčka Martina je jiná kniha, než na jaké jste zvyklí.\n",
    "A dost možná to ani není kniha pro vás.\n",
    "Proto také vychází v malém nákladu, jen pro úzký okruh lidí.\n",
    "Není to ani historická beletrie, ani fantazy, ani soudobá próza.\n",
    "Nejblíže má k iniciačnímu románu.\n",
    "Kniha střídá neumělé vyprávění studenty a barvité sny, vize.\n",
    "Budete si stěžovat, že nesnášíte její současné části - nebo, že nemáte rádi ty historické.\n",
    "Bude vám chybět konec nebo přebývat jeho mnohoznačnost.\n",
    "Pak to zřejmě není kniha pro vás nebo pro tuto vaši životní etapu.\n",
    "Možná se začtete a až odtrhnete unavené oči o mnoho hodin později, budete svět vidět jinak, s pochybnostmi.\n",
    "Tři roky psaná kniha, kterou není lehké zařadit.\n",
    "V prvé fázi deníček studentky, která hledá svého strýčka.\n",
    "V druhé fázi vidiny, stíny minulosti.\n",
    "Dechberoucí obrazy bitev, zkázy, rozcestí české a evropské minulosti.\n",
    "To je Příběh strýčka Martina.\n",
    "Historická rovina knihy se pohybuje kolem roku 950, v době boje knížete Boleslava s německým králem Otou.\n",
    "Historický příběh vlastně začíná smrtí hlavního hrdiny, šéfšpicla a šedé eminence české země Martina z Wartberka a ztraceným zemským tributem, který měl konvoj dovést k říšskému králi.\n",
    "Horečnou snahou zajistit zemi, která se najednou ocitla ve smrtelném ohrožení, protože kdo ví, co všechno se ztratilo s Wartberkovou smrtí.\n",
    "Současný příběh je hledání Veroniky po strýčkovi, který se ztratil.\n",
    "Kam a jak, to vlastně nikdo neví.\n",
    "Až teď začíná Veronika nacházet spojnice mezi svými vizemi a sny se současností.\n",
    "A uvědomuje si, že Martin historický je tím Martinem současným, že najít jednoho znamená najít druhého.\n",
    "Kéž by to bylo tak jednoduché...\n",
    "Jenže, darmo se neříká, že cesta je cíl.\n",
    "Platí to i v tomto případě - projít cestu je podstatné.\n",
    "V knize se potkáte se dvěmi historickými bitvami, které dnes už vlastně upadly v zapomnění.\n",
    "Bitvou u Lechu v roce 955, kde padl prakticky celý český vojenský sbor v boji proti Maďarům. Zdarma se dozvíte, z čeho je jméno \"maďaři\" odvozeno.\n",
    "A s bitvou u Nového hradu v roce 950, kde, a o tom kroniky cudně mlčí, porazil kníže Boleslav krále Otu a učinil z Čech plnoprávného souseda a souputníka Říše.\n",
    "Setkáte se tu s řadou historických postav.\n",
    "Vlastně všechny postavy v knize jsou postavy žijící, historické, které mají nějaký svůj předobraz.\n",
    "Výjimkou je Maxmilián ze Schweringenu, komoří Oty.\n",
    "Důvod?\n",
    "Toho skutečného nemám rád a tak jsem ho nechtěl ani zmiňovat. Snad mi prominete...\n",
    "''' # @param {type:\"string\"}\n",
    "\n",
    "# passage = '''\n",
    "# Tu fotku jsem našla ve výpravné fotografické publikaci o Pražském hradu a hned, jak jsem ji viděla, jsem věděla, že tohle je to správné místo, skála.\n",
    "# Protože abych pravdu řekla, neočekávala jsem, že Martin by měl v oblibě skálu jakožto přírodní útvar.\n",
    "# V jeho přírodě je skála symbolem, náznakem.\n",
    "# A skála v podobě přesmyčky latinského petra je ideálním symbolem. Jsem si skoro jistá, že tím místem schůzky je skála v podobě svatého Petra.\n",
    "# Fotografie pochází z vnější zdi kaple svatého Kříže na druhém nádvoří Pražského hradu a pamatuju si, jak jsme kdysi, to mi bylo něco málo přes deset, kolem té sochy šli.\n",
    "# Martin mi ukázal ten nápis a přečetl ho: \"Tu es Petrus et super hanc petram aedificabo ecclesiam meam.\"\n",
    "# \"Umíš to přeložit?\" Zeptal se mě.\n",
    "# Šárka s námi nebyla. Ležela doma nemocná a vztekala se, že ji mamka nepustila.\n",
    "# Začala jsem slabikovat: \"Ty, jsi, Petr\" a pak už jsem si na ten verš z Bible vzpomněla a dořekla ho: \"Petr, Skála - a nad tou skálou vybuduji svoji církev.\"\n",
    "# \"Pamatuješ si to dobře, Vítězko, ale to tam není.\"\n",
    "# Přelétla jsem očima ten nápis, slovo po slově a s drzostí desetiletého vševěda řekla: \"Ale je!\"\n",
    "# \"Petra je latinsky skála, ale zároveň je to jméno - Petr. Je odvozené ze slova skála, že?\" Zeptal se Martin.\n",
    "# \"Ano, to je,\" zamyslela jsem se.\n",
    "# A pak mi to došlo: \"Takže se nedá říct, jestli Ježíš mluví o Petrovi nebo o skále?\"\n",
    "# \"To ne. Z latinského překladu se to opravdu říct nedá. Proto to do češtiny překládají různě, ne každý překladatel ten dvojsmysl akceptoval nebo znal originál.\"\n",
    "# \"Jak ten verš překládá kralická?\"\n",
    "# Zavrtěla jsem hlavou, že nevím. Nemám kralickou ráda. Radši mám ekumenický překlad, který je čtivější. Obzvlášť pro desetileté dítě.\n",
    "# '''\n",
    "passage = '''\n",
    "في التقرير الأخير عن تطورات المشروع، قيل إن الفريق سيقوم بـ تطرق شامل لكل الجوانب، لكن عند النطق ظهرت كلمة تطرق وكأنها تبدأ بحرف الطاء بدل التاء، وظهرت كلمة تطورات وكأنها تبدأ بحرف الطاء بدلًا من التاء. كذلك تكرر الأمر عند الحديث عن تقييم النتائج، حيث نطقها النظام تكيم بدلًا من تقييم، مما جعل حرف القاف يتحول إلى كاف، وأيضًا في عبارة \"هذه قوة إضافية\"، تحولت إلى \"كوة إضافية\". أما عند ذكر الصلة بين الأقسام، فقد قلب النظام الحرف فقرأها السلة بدلًا من الصلة، والعكس حصل في كلمة الأسلحة حيث ظهرت \"الأصلحة\" بدلًا من \"الأسلحة\". ومن المشكلات المتكررة أن كلمة منظمة لا تُقرأ كما ينبغي، إذ ينطقها TTS إما \"منظمه\" أو \"منظمات\"، فيختفي الفرق بين المفرد والجمع. وأخيرًا في الخطة التعليمية وردت عبارة \"هذا البرنامج مقسم إلى المستويين الأول والثاني\"، لكن المحرك لم ينطق كلمة المستويين بشكل واضح، بل جعلها أحيانًا \"المستويان\" وأحيانًا بشكل غير مفهوم. وعند تكرار هذه الكلمات مرة أخرى، نجد أن تطرق، تطورات، تقييم، قوة، الصلة، الأسلحة، منظمة، و المستويين جميعها تظهر مشكلات متكررة في النطق تجعل السامع يدرك أن النظام لا يميز بين الطاء والتاء، ولا بين القاف والكاف، ولا بين السين والصاد، كما يخطئ في التاء المربوطة ويعجز عن نطق المستويين بوضوح.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP-7i2QAC0JT"
   },
   "outputs": [],
   "source": [
    "#torch.manual_seed(2)\n",
    "#torch.manual_seed(9)\n",
    "#torch.manual_seed(12773624985526461855)\n",
    "torch.manual_seed(17779876142042974804)\n",
    "print(\"seed\", torch.initial_seed())\n",
    "#print(\"seed\", torch.seed())\n",
    "sentences = passage.split('.') # simple split by comma\n",
    "wavs = []\n",
    "s_prev = None\n",
    "for text in sentences:\n",
    "    if text.strip() == \"\": continue\n",
    "    text += '. «' # add it back\n",
    "    text = text.replace(\"(\", \", \").replace(\")\", \", \")\n",
    "    noise = torch.randn(1,1,256).to(device)\n",
    "    #wav = inference_100ms(text, noise, diffusion_steps=5, embedding_scale=1.2)\n",
    "    wav, s_prev = LFinference(text, s_prev, noise, alpha=0.7, diffusion_steps=10, embedding_scale=1.5)\n",
    "    #wav, s_prev = LFinference_trim_100ms(text, s_prev, noise, alpha=0.7, diffusion_steps=10, embedding_scale=1.5)\n",
    "    #wav, s_prev = LFinference_trim_100ms(text, s_prev, noise, alpha=0.4, diffusion_steps=10, embedding_scale=1.2, trim_ms=350, end_silence_ms=1000, noise_reduction_threshold=0.5, noise_estimage_energy_threshold=0.015, append_silence_energy_threshold=0.0015)\n",
    "    wavs.append(wav)\n",
    "display(ipd.Audio(np.concatenate(wavs), rate=24000, normalize=True))\n",
    "#display(ipd.Audio(_crossfade_concat(wavs, crossfade_ms=1100, sample_rate=24000), rate=24000, normalize=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1x2mx2VnkYNFVlD+DFzmy",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
